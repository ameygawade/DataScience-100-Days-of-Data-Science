{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e24c73e",
   "metadata": {},
   "source": [
    "# Day 33: Clustering Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03a2ecf",
   "metadata": {},
   "source": [
    "Welcome to Day 33! Unlike supervised learning, where you have a clear target variable to measure your model against, evaluating unsupervised clustering models is more nuanced. Since there's no \"correct\" answer, we use metrics that assess the quality and compactness of the clusters themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b8ce94",
   "metadata": {},
   "source": [
    "## Topics Covered:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43c692a",
   "metadata": {},
   "source": [
    "- The Challenge of Evaluating Unsupervised Models\n",
    "\n",
    "- Common Clustering Metrics\n",
    "    - Intertia\n",
    "\n",
    "    - Silhouette Score\n",
    "\n",
    "    - Davies-Bouldin Index\n",
    "\n",
    "    - Calinski-Harabasz Index\n",
    "\n",
    "- How to Interpret These Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2cb6ea",
   "metadata": {},
   "source": [
    "## The Challenge of Unsupervised Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77036434",
   "metadata": {},
   "source": [
    "In supervised learning, we have an answer key (the labels). We can easily calculate how many predictions were correct using metrics like accuracy or an F1-Score. \n",
    "\n",
    "In unsupervised learning, however, we don't have labels. The model is creating its own structure, and we need to find a way to measure how \"good\" that structure is without an answer key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70e3f47",
   "metadata": {},
   "source": [
    "So, we use **internal evaluation metrics** to assess the compactness and separation of clusters:\n",
    "- **Compactness**: Points within a cluster should be close to each other.\n",
    "- **Separation**: Clusters should be well-separated from one another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7249185b",
   "metadata": {},
   "source": [
    "## Internal Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c295e4b",
   "metadata": {},
   "source": [
    "### Inertia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511c431f",
   "metadata": {},
   "source": [
    "Inertia is sum of squared distances of samples to their closest cluster center. It measures how compact the clusters are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415ffedb",
   "metadata": {},
   "source": [
    "### Silhouette Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4b21ed",
   "metadata": {},
   "source": [
    "Measures **how similar a point is to its own cluster vs. other clusters**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ebc609",
   "metadata": {},
   "source": [
    "\n",
    "- Ranges from **-1 to 1**:\n",
    "  - +1 ‚Üí well-clustered\n",
    "  - 0 ‚Üí on the boundary\n",
    "  - -1 ‚Üí wrongly clustered\n",
    "\n",
    "### Formula:\n",
    "\n",
    "### üìê Silhouette Score Formula\n",
    "\n",
    "For a point \\( i \\):\n",
    "\n",
    "- a(i): average distance to points in the **same** cluster  \n",
    "- b(i): lowest average distance to points in **another** cluster  \n",
    "\n",
    "$$ Silhouette(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a84891e",
   "metadata": {},
   "source": [
    "### Davies-Bouldin Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf8b95d",
   "metadata": {},
   "source": [
    "The Davies-Bouldin Index measures the average similarity between each cluster and its most similar cluster. Similarity is a ratio of within-cluster distance to between-cluster distance. A lower Davies-Bouldin score indicates a better clustering. A perfect score is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7445e4b0",
   "metadata": {},
   "source": [
    "$$DB = \\frac{1}{k} \\sum_{i=1}^{k} \\max_{j \\neq i} \\left(\\frac{\\sigma_i + \\sigma_j}{d(c_i, c_j)}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b71a3f",
   "metadata": {},
   "source": [
    "### Calinski-Harabasz Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519ba24c",
   "metadata": {},
   "source": [
    "Also known as the Variance Ratio Criterion, this index is a ratio of the between-cluster variance to the within-cluster variance. A higher score means the clusters are better defined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fb8253",
   "metadata": {},
   "source": [
    "$$CH = \\frac{SS_B / (k-1)}{SS_W / (N-k)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e285eb",
   "metadata": {},
   "source": [
    "## External Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000a63eb",
   "metadata": {},
   "source": [
    "### Adjusted Rand Index (ARI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5675f876",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
