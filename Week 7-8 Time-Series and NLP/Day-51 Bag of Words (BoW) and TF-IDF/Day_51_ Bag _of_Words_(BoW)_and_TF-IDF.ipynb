{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db12ba98",
   "metadata": {},
   "source": [
    "# Day-51 Bag of Words (BOW) and TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b2ee01",
   "metadata": {},
   "source": [
    "Yesterday, we learned how to clean text by tokenizing and stemming. Today, we tackle the most crucial step in classical NLP: Feature Extraction. We need to convert that clean list of words into a numerical format (a vector) that machine learning algorithms can actually process. We'll focus on the two foundational methods: Bag of Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571235e6",
   "metadata": {},
   "source": [
    "## Topic Covered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a551167",
   "metadata": {},
   "source": [
    "- Bag of Words (BoW) and Count Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25abc2e",
   "metadata": {},
   "source": [
    "## Bag of Words (BoW) and Count Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6d30d2",
   "metadata": {},
   "source": [
    "The Bag of Words (BoW) model is a simple way of representing text data where the order of words doesn't matter (it's just a \"bag\"). Count Vectorization is the process used to implement BoW."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13883204",
   "metadata": {},
   "source": [
    "- `How it Works`: \n",
    "    - It creates a vocabulary (a list of every unique word across all documents) and then represents each document as a vector where each element is the count of how often that vocabulary word appears in the document.\n",
    "\n",
    "- `Analogy`: \n",
    "    - Shopping List. A bag of groceries might contain 3 apples, 2 bananas, and 1 milk. You care about the counts, not the order you put them in the bag.\n",
    "\n",
    "- `Example (2 Documents)`:\n",
    "\n",
    "    - Doc 1: \"The quick brown fox\"\n",
    "\n",
    "    - Doc 2: \"The quick fox ran\"\n",
    "\n",
    "    - Vocabulary: ['the', 'quick', 'brown', 'fox', 'ran']\n",
    "\n",
    "        - Vector for Doc 1: [1, 1, 1, 1, 0]\n",
    "\n",
    "        - Vector for Doc 2: [1, 1, 0, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a2b56e",
   "metadata": {},
   "source": [
    "## Sparse Matrix: Managing Massive Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786305e3",
   "metadata": {},
   "source": [
    "In real-world NLP, the vocabulary size can reach tens of thousands of words. If a document only contains 50 words, its vector will have thousands of zeros.\n",
    "\n",
    "- Sparse Matrix: A specialized data structure used to store these BoW or TF-IDF vectors efficiently. Instead of storing every zero, it only records the position and value of the non-zero elements.\n",
    "\n",
    "- `Why it Matters`: Storing the entire matrix (which is mostly zeros) wastes enormous amounts of memory and computational power. Using a sparse matrix is essential for scaling NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dd5b7c",
   "metadata": {},
   "source": [
    "## TF-IDF: Term Weighting and Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b820e7ae",
   "metadata": {},
   "source": [
    "Term Frequency-Inverse Document Frequency (TF-IDF) is an advanced version of BoW that assigns a numerical weight to each word, reflecting its importance in a document relative to the entire collection of documents (the corpus)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304890c8",
   "metadata": {},
   "source": [
    "It is calculated by multiplying two components:\n",
    "\n",
    "1. **Term Frequency (TF)**: How often a word appears in a specific document. (Similar to BoW)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f62796",
   "metadata": {},
   "source": [
    "$ TF(t,d)= \\frac{Total number of terms in document d}{Count of term t in document d} $\n",
    "​\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dbbd44",
   "metadata": {},
   "source": [
    "2. **Inverse Document Frequency (IDF)**: A measure of how rare a word is across all documents. Words like \"the\" have a low IDF score (they appear in many documents), while unique technical words have a high IDF score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451b727e",
   "metadata": {},
   "source": [
    "$ IDF(t)=log( \\frac{Number of documents containing term t}{Total number of documents}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13faf9c",
   "metadata": {},
   "source": [
    "- `Final Score`: $ TF-IDF=TF×IDF $\n",
    "\n",
    "- `Analogy`: The Expert Witness. A common word like \"is\" might appear frequently (high TF), but since it appears in every document (low IDF), its TF-IDF weight is low. A unique word like \"photosynthesis\" might appear only once (low TF), but since it appears in only one document (high IDF), its TF-IDF weight is high. The high-scoring word is the \"expert\" that defines the document's topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efbe0fb",
   "metadata": {},
   "source": [
    "## Code Example: BoW and TF-IDF with Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72405f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Count Vectorization (BoW) ---\n",
      "Vocabulary: ['analysis' 'and' 'coding' 'data' 'enjoyable' 'exciting' 'for' 'fun'\n",
      " 'great' 'in' 'is' 'key' 'language' 'learning' 'love' 'machine' 'python'\n",
      " 'science' 'to']\n",
      "\n",
      "BoW Matrix (Counts):\n",
      "[[0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0]\n",
      " [0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0]\n",
      " [0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0]\n",
      " [1 0 0 2 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1]\n",
      " [0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0]\n",
      " [0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0]]\n",
      "\n",
      "--- TF-IDF Vectorization ---\n",
      "TF-IDF Matrix (Weights):\n",
      "[[0.    0.364 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.425 0.503 0.503 0.425 0.    0.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.482 0.    0.482 0.    0.286 0.\n",
      "  0.482 0.333 0.    0.    0.333 0.    0.   ]\n",
      " [0.    0.34  0.47  0.47  0.    0.    0.    0.    0.    0.    0.    0.\n",
      "  0.    0.    0.47  0.    0.    0.47  0.   ]\n",
      " [0.386 0.    0.    0.633 0.    0.    0.    0.    0.    0.    0.229 0.386\n",
      "  0.    0.    0.    0.    0.    0.316 0.386]\n",
      " [0.    0.316 0.    0.    0.    0.532 0.    0.437 0.    0.    0.316 0.\n",
      "  0.    0.369 0.    0.437 0.    0.    0.   ]\n",
      " [0.    0.279 0.385 0.    0.47  0.    0.    0.385 0.    0.47  0.279 0.\n",
      "  0.    0.    0.    0.    0.325 0.    0.   ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# 1. Input Corpus (Assume pre-processed and cleaned text)\n",
    "corpus = [\n",
    "    'i love machine learning and python',\n",
    "    'python is a great language for learning',\n",
    "    'i love data science and coding',\n",
    "    'data analysis is key to data science',\n",
    "    'machine learning is fun and exciting',\n",
    "    'coding in python is enjoyable and fun'\n",
    "\n",
    "]\n",
    "\n",
    "# --- A. Bag of Words (BoW) using CountVectorizer ---\n",
    "print(\"--- Count Vectorization (BoW) ---\")\n",
    "# 1. Initialize the CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# 2. Fit the vectorizer to learn the vocabulary and transform the corpus\n",
    "bow_matrix = count_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# 3. Get the vocabulary (features/column names)\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "print(f\"Vocabulary: {feature_names}\")\n",
    "\n",
    "# 4. Convert the sparse matrix to a dense NumPy array for viewing\n",
    "print(\"\\nBoW Matrix (Counts):\")\n",
    "print(bow_matrix.toarray())\n",
    "# Output (Rows=Documents, Columns=Vocabulary):\n",
    "# Row 1 (Doc 1): [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
    "\n",
    "# --- B. TF-IDF Vectorization ---\n",
    "print(\"\\n--- TF-IDF Vectorization ---\")\n",
    "# 1. Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# 2. Fit and transform the corpus\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# 3. Get feature names (same as BoW, but weights are different)\n",
    "# feature_names_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# 4. Convert the sparse matrix to a dense NumPy array for viewing (shows the weights)\n",
    "print(\"TF-IDF Matrix (Weights):\")\n",
    "print(tfidf_matrix.toarray().round(3))\n",
    "# Notice the weights are floats, reflecting the importance of words like 'data' (high weight) \n",
    "# versus 'i' or 'love' (low weight because they appear frequently)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85322d13",
   "metadata": {},
   "source": [
    "## Summary of Day 51"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7efb9b",
   "metadata": {},
   "source": [
    "Today, you learned how to convert messy text into clean numerical features. Count Vectorization creates the Bag of Words model based on raw frequency. TF-IDF refines this by applying term weighting, assigning higher scores to words that are rare across the corpus but frequent within a specific document. Both methods produce a sparse matrix representation, which is key for efficient machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a665910c",
   "metadata": {},
   "source": [
    "## What's Next (Day 52)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47890f46",
   "metadata": {},
   "source": [
    "BoW and TF-IDF are great, but they treat every word as an independent dimension, ignoring meaning and context. Tomorrow, on Day 52, we will move to the next generation of NLP features: Word Embeddings. You'll learn how models like Word2Vec and GloVe capture the semantic relationships between words, which is fundamental to modern deep learning NLP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613dd6e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42726fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
