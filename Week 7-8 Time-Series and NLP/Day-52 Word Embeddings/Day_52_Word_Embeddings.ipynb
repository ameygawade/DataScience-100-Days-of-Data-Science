{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8349ed40",
   "metadata": {},
   "source": [
    "# Day-52: Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbb99b6",
   "metadata": {},
   "source": [
    "Yesterday, we took a crucial step by converting words into count-based vectors (BoW and TF-IDF). While effective, these methods treat words as isolated, independent features, failing to capture the rich semantic relationships between them.\n",
    "\n",
    "Today, we delve into the core of modern NLP: Word Embeddings. This technique transforms words into dense numerical vectors that capture meaning and context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d60ff2b",
   "metadata": {},
   "source": [
    "## Topic Covered:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa8fa1a",
   "metadata": {},
   "source": [
    "## Word Embeddings: Meaning in Motion "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d39498a",
   "metadata": {},
   "source": [
    "Word Embeddings are dense, low-dimensional vector representations of words. The core idea is that words that appear in similar contexts should have similar vector representations.\n",
    "\n",
    "- `How it Works`: These vectors (typically 50 to 300 dimensions) are learned by training a neural network on a massive corpus of text (like Wikipedia or the entire web).\n",
    "\n",
    "- `Analogy`: Geographical Map. Imagine every word is a city. BoW just lists cities alphabetically. Word Embeddings place them on a map: words with similar meanings (e.g., \"king,\" \"queen\") are mapped close together, and the direction between related words (e.g., the vector from \"man\" to \"woman\") is consistent.\n",
    "\n",
    "- `Key Feature`: Vector Arithmetic. The most famous result of embeddings is their ability to perform algebraic operations that capture semantic relationships:\n",
    "\n",
    "$$ king − man + woman ≈ queen $$\n",
    "\n",
    "​\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e59705",
   "metadata": {},
   "source": [
    "## Word2Vec: Predicting Contex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da45bb74",
   "metadata": {},
   "source": [
    "Word2Vec is one of the earliest and most influential methods for generating word embeddings. It introduced two main architectures:\n",
    "\n",
    "1. Continuous Bag of Words (CBOW): Predicts the current word based on its surrounding context words. (Input: context, Output: target word).\n",
    "\n",
    "    - `Example`: Predict \"fox\" from context \"The quick [ ] ran.\"\n",
    "\n",
    "2. Skip-gram: Predicts the context words given the current word. (Input: target word, Output: context).\n",
    "\n",
    "    - `Example`: Predict \"The,\" \"quick,\" \"ran\" from target \"fox.\"\n",
    "\n",
    "Skip-gram generally performs better on large datasets, as its objective of predicting the surrounding words allows it to capture a wider, more subtle range of contextual information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d43f55",
   "metadata": {},
   "source": [
    "## GloVe: Global Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875384db",
   "metadata": {},
   "source": [
    "GloVe (Global Vectors for Word Representation) is another popular embedding method that combines the principles of local context windows (like Word2Vec) with global matrix factorization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1119b405",
   "metadata": {},
   "source": [
    "- `How it Works`: Instead of a complex predictive model, GloVe focuses on building a matrix of Word-Word Co-occurrence. It then uses factorization to generate vectors such that the ratio of their components reflects the frequency of words appearing together.\n",
    "\n",
    "- `Advantage`: GloVe often converges faster than Word2Vec and performs well even on smaller training corpora because it leverages global statistics about word relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99811ea2",
   "metadata": {},
   "source": [
    "## Code Example: Loading and Using Pre-trained GloVe Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1c66e2",
   "metadata": {},
   "source": [
    "Since training your own Word2Vec or GloVe model is computationally expensive, we always use pre-trained models (trained on billions of words) and load them via libraries like Gensim or through a machine learning framework.\n",
    "\n",
    "The following example loads a small pre-trained GloVe model to demonstrate the concept of vector similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11abd8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp311-cp311-win_amd64.whl.metadata (8.2 kB)\n",
      "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.13.1-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.3.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
      "  Downloading wrapt-1.17.3-cp311-cp311-win_amd64.whl.metadata (6.5 kB)\n",
      "Downloading gensim-4.3.3-cp311-cp311-win_amd64.whl (24.0 MB)\n",
      "   ---------------------------------------- 0.0/24.0 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 10.2/24.0 MB 49.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 21.0/24.0 MB 51.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.0/24.0 MB 47.5 MB/s  0:00:00\n",
      "Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "Downloading scipy-1.13.1-cp311-cp311-win_amd64.whl (46.2 MB)\n",
      "   ---------------------------------------- 0.0/46.2 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 11.5/46.2 MB 60.3 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 23.3/46.2 MB 54.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 34.6/46.2 MB 54.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  45.6/46.2 MB 53.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 46.2/46.2 MB 50.7 MB/s  0:00:00\n",
      "Downloading smart_open-7.3.1-py3-none-any.whl (61 kB)\n",
      "Downloading wrapt-1.17.3-cp311-cp311-win_amd64.whl (38 kB)\n",
      "Installing collected packages: wrapt, numpy, smart-open, scipy, gensim\n",
      "\n",
      "  Attempting uninstall: numpy\n",
      "\n",
      "    Found existing installation: numpy 2.2.6\n",
      "\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "    Uninstalling numpy-2.2.6:\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "      Successfully uninstalled numpy-2.2.6\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   -------- ------------------------------- 1/5 [numpy]\n",
      "   ---------------- ----------------------- 2/5 [smart-open]\n",
      "   ---------------- ----------------------- 2/5 [smart-open]\n",
      "  Attempting uninstall: scipy\n",
      "   ---------------- ----------------------- 2/5 [smart-open]\n",
      "    Found existing installation: scipy 1.16.2\n",
      "   ---------------- ----------------------- 2/5 [smart-open]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "    Uninstalling scipy-1.16.2:\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "      Successfully uninstalled scipy-1.16.2\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   ------------------------ --------------- 3/5 [scipy]\n",
      "   -------------------------------- ------- 4/5 [gensim]\n",
      "   -------------------------------- ------- 4/5 [gensim]\n",
      "   -------------------------------- ------- 4/5 [gensim]\n",
      "   -------------------------------- ------- 4/5 [gensim]\n",
      "   -------------------------------- ------- 4/5 [gensim]\n",
      "   -------------------------------- ------- 4/5 [gensim]\n",
      "   -------------------------------- ------- 4/5 [gensim]\n",
      "   -------------------------------- ------- 4/5 [gensim]\n",
      "   -------------------------------- ------- 4/5 [gensim]\n",
      "   -------------------------------- ------- 4/5 [gensim]\n",
      "   -------------------------------- ------- 4/5 [gensim]\n",
      "   -------------------------------- ------- 4/5 [gensim]\n",
      "   -------------------------------- ------- 4/5 [gensim]\n",
      "   -------------------------------- ------- 4/5 [gensim]\n",
      "   ---------------------------------------- 5/5 [gensim]\n",
      "\n",
      "Successfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1 smart-open-7.3.1 wrapt-1.17.3\n"
     ]
    }
   ],
   "source": [
    "! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f23d96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Arithmetic: (king - man + woman) result:\n",
      "[0.4 0.7 0.1]\n",
      "\n",
      "Similarity (King vs. Queen): 0.9449\n",
      "Similarity (King vs. Apple): 0.5588\n",
      "\n",
      "Note: Similarity scores range from -1 (opposite) to 1 (identical).\n",
      "Similarity(apple vs. fruit): 0.9831\n",
      "Similarity(apple vs. phone): 0.2289\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Gensim is the standard library for loading/working with Word2Vec and GloVe\n",
    "from gensim.models import KeyedVectors \n",
    "\n",
    "# --- NOTE: Loading large embedding files requires the file to be present ---\n",
    "# For demonstration, we will assume a small file or simulate the key vectors:\n",
    "\n",
    "# Simulated GloVe vector dictionary (Words -> 3-dimensional vector)\n",
    "word_vectors_dict = {\n",
    "    'king': np.array([0.5, 0.4, 0.3]),\n",
    "    'queen': np.array([0.4, 0.6, 0.2]),\n",
    "    'man': np.array([0.7, 0.2, 0.6]),\n",
    "    'woman': np.array([0.6, 0.5, 0.4]),\n",
    "    'apple': np.array([0.1, 0.1, 0.9]), # Unrelated word\n",
    "    'fruit': np.array([0.2, 0.2, 0.8]), # Related to apple\n",
    "    'phone': np.array([0.9, 0.1, 0.1])  # Unrelated word\n",
    "}\n",
    "\n",
    "# 1. Perform Vector Arithmetic (Illustrating the core feature)\n",
    "vec_king = word_vectors_dict['king']\n",
    "vec_man = word_vectors_dict['man']\n",
    "vec_woman = word_vectors_dict['woman']\n",
    "\n",
    "# Operation: king - man + woman\n",
    "result_vector = vec_king - vec_man + vec_woman\n",
    "\n",
    "print(\"Vector Arithmetic: (king - man + woman) result:\")\n",
    "print(np.round(result_vector, 2))\n",
    "# The resulting vector [0.4 0.7 0.1] should be numerically close to the 'queen' vector [0.4 0.6 0.2]\n",
    "\n",
    "# 2. Check Similarity (Cosine Similarity)\n",
    "def cosine_similarity(v1, v2):\n",
    "    \"\"\"Calculates the cosine similarity between two vectors.\"\"\"\n",
    "    dot_product = np.dot(v1, v2)\n",
    "    norm_v1 = np.linalg.norm(v1)\n",
    "    norm_v2 = np.linalg.norm(v2)\n",
    "    return dot_product / (norm_v1 * norm_v2)\n",
    "\n",
    "# Compare similar words\n",
    "sim_king_queen = cosine_similarity(word_vectors_dict['king'], word_vectors_dict['queen'])\n",
    "# Compare dissimilar words\n",
    "sim_king_apple = cosine_similarity(word_vectors_dict['king'], word_vectors_dict['apple'])\n",
    "# Compare related words\n",
    "sim_apple_fruit = cosine_similarity(word_vectors_dict['apple'], word_vectors_dict['fruit'])\n",
    "# compare unrelated words\n",
    "sim_apple_phone = cosine_similarity(word_vectors_dict['apple'], word_vectors_dict['phone'])\n",
    "\n",
    "# Display results\n",
    "\n",
    "print(f\"\\nSimilarity (King vs. Queen): {sim_king_queen:.4f}\")\n",
    "print(f\"Similarity (King vs. Apple): {sim_king_apple:.4f}\")\n",
    "print(\"\\nNote: Similarity scores range from -1 (opposite) to 1 (identical).\")\n",
    "print(f\"Similarity(apple vs. fruit): {sim_apple_fruit:.4f}\")\n",
    "print(f\"Similarity(apple vs. phone): {sim_apple_phone:.4f}\")\n",
    "# Output: King/Queen should be much higher (closer to 1.0) than King/Apple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d5ba46",
   "metadata": {},
   "source": [
    "## Summary of Day 52"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bfbfc2",
   "metadata": {},
   "source": [
    "Today, you moved beyond simple word counting to embrace Word Embeddings. You learned that Word2Vec and GloVe generate dense vectors that encode the semantic meaning of words, enabling powerful features like vector arithmetic. This ability to understand context is what separates modern NLP from classical approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf0618a",
   "metadata": {},
   "source": [
    "## What's Next (Day 53)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cdd28a",
   "metadata": {},
   "source": [
    "We've focused on words, but what about the structure of a sentence? Tomorrow, on Day 53, we'll dive into two foundational structural analysis tools: Named Entity Recognition (NER) and Part-of-Speech (POS) Tagging. You'll learn how to use libraries like spaCy to automatically tag grammar and identify key entities (people, places, organizations) in text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
