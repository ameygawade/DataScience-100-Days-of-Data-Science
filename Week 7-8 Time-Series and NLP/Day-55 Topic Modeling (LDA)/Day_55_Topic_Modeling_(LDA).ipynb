{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27441541",
   "metadata": {},
   "source": [
    "# Day-55: Topic Modeling (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a4065b",
   "metadata": {},
   "source": [
    "We've covered what text is, what it means, and how it feels. \n",
    "\n",
    "Today, we tackle the big picture: Topic Modeling. This is a powerful machine learning technique that allows us to scan massive collections of documents (a corpus) and automatically discover the abstract topics that run through them. We'll focus on the most famous algorithm for this task: Latent Dirichlet Allocation (LDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fc6bb9",
   "metadata": {},
   "source": [
    "## Topic Covered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024cf39c",
   "metadata": {},
   "source": [
    "Latent Dirichlet Allocation, Document-Topic Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca73ce5c",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA): Uncovering Hidden Themes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732206af",
   "metadata": {},
   "source": [
    "LDA is a generative statistical model that assumes every document is a mixture of various topics, and every topic is, in turn, a collection of words that frequently occur together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dae763",
   "metadata": {},
   "source": [
    "- Generative Assumption: LDA works backward, assuming a process where documents are created by:\n",
    "\n",
    "    1. Randomly choosing a topic (e.g., \"Space Exploration\").\n",
    "\n",
    "    2. Randomly choosing a word from that topic (e.g., \"rocket,\" \"Mars,\" \"launch\").\n",
    "\n",
    "    3. Repeating this process until the document is complete.\n",
    "\n",
    "- `The Goal`: LDA reverses this, determining the most likely set of topics and their associated words that could have generated the observed corpus.\n",
    "\n",
    "- `Analogy`: The Cook's Recipe Book. Imagine you have a large library of recipes. LDA doesn't read the titles; it looks at the ingredients:\n",
    "\n",
    "    - If a \"topic\" frequently uses \"flour,\" \"sugar,\" and \"butter,\" LDA labels it the 'Baking' Topic.\n",
    "\n",
    "    - If a \"topic\" frequently uses \"tomato,\" \"pasta,\" and \"basil,\" LDA labels it the 'Italian Food' Topic.\n",
    "\n",
    "    - A single recipe (document) might be 80% 'Italian Food' and 20% 'Baking' (for the bread sticks)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35c3d24",
   "metadata": {},
   "source": [
    "## Document-Topic Distributions: The Mixture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16f909a",
   "metadata": {},
   "source": [
    "The output of an LDA model provides two key distributions that summarize the corpus:\n",
    "\n",
    "1. Topic-Word Distribution: This shows the probability of a specific word appearing in a specific topic. This is how we define and interpret the topics.\n",
    "\n",
    "    - Example: Topic 1: {(dog,0.15),(cat,0.10),(vet,0.05),…}\n",
    "\n",
    "2. Document-Topic Distribution: This shows the proportional mixture of topics within each document. This tells you what each document is about.\n",
    "\n",
    "    - Example: Document A: Topic 1 (Pets):85%, Topic 2 (Finance):10%, Topic 3 (Travel):5%.\n",
    "\n",
    "By analyzing the Document-Topic Distribution, you can cluster, filter, or categorize documents based on their content without ever manually reading and labeling them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade07e0c",
   "metadata": {},
   "source": [
    "## Code Example: Topic Modeling with Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694c9360",
   "metadata": {},
   "source": [
    "We use the Gensim library, which is the industry standard for implementing LDA. This requires cleaning and feature extraction (BoW) from our previous days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac6f716",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip uninstall gensim numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccea8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install numpy==1.23.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ce2d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install gensim numpy nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2701dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TOPIC-WORD DISTRIBUTIONS ---\n",
      "Topic #1: 0.034*\"prepare\" + 0.034*\"long\" + 0.034*\"mars\" + 0.034*\"cosmonauts\" + 0.034*\"moon\"\n",
      "Topic #2: 0.061*\"learning\" + 0.043*\"machine\" + 0.026*\"technical\" + 0.026*\"prices\" + 0.026*\"using\"\n",
      "\n",
      "--- DOCUMENT-TOPIC DISTRIBUTION (Doc 1) ---\n",
      "Document 1 is about: [(0, 0.043867763), (1, 0.95613223)]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# --- 0. Data Preparation (Simulation) ---\n",
    "documents = [\n",
    "    \"Machine learning models predict stock prices using technical analysis and financial reports.\",\n",
    "    \"The new rocket launch from NASA successfully put a satellite into orbit around the Earth.\",\n",
    "    \"Neural networks and deep learning are crucial for complex machine learning tasks like image recognition.\",\n",
    "    \"Astronauts and cosmonauts prepare for long duration space missions to the Moon and Mars.\",\n",
    "    \"A massive portfolio diversification strategy involves stocks, bonds, and real estate investments.\"\n",
    "]\n",
    "\n",
    "# Simple Preprocessing (Tokenization, Lowercasing, Stopword Removal)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "processed_docs = []\n",
    "for doc in documents:\n",
    "    # Tokenize and remove stopwords/punctuation\n",
    "    tokens = [token.lower() for token in word_tokenize(doc) if token.isalpha() and token.lower() not in stop_words]\n",
    "    processed_docs.append(tokens)\n",
    "\n",
    "# --- 1. Feature Extraction (Dictionary and Corpus) ---\n",
    "# Gensim uses its own dictionary and corpus format (BoW)\n",
    "\n",
    "# Create a dictionary (Mapping of word -> unique ID)\n",
    "dictionary = corpora.Dictionary(processed_docs)\n",
    "\n",
    "# Create a DTM (Document-Term Matrix / Bag-of-Words Corpus)\n",
    "# Each tuple is (word_id, count)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "# --- 2. LDA Model Training ---\n",
    "# We tell the model to find a specific number of topics (e.g., 2)\n",
    "num_topics = 2\n",
    "lda_model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=num_topics,\n",
    "    random_state=42,\n",
    "    passes=10 # Number of training passes\n",
    ")\n",
    "\n",
    "# --- 3. Interpretation (Topic-Word Distribution) ---\n",
    "print(\"--- TOPIC-WORD DISTRIBUTIONS ---\")\n",
    "for idx, topic in lda_model.print_topics(num_words=5):\n",
    "    print(f\"Topic #{idx + 1}: {topic}\")\n",
    "\n",
    "# Expected output shows clear separation:\n",
    "# Topic 1: likely 'Finance/ML' words (e.g., 'stock', 'investments', 'learning')\n",
    "# Topic 2: likely 'Space' words (e.g., 'rocket', 'NASA', 'Mars')\n",
    "\n",
    "# --- 4. Document-Topic Distribution ---\n",
    "# Get the topic distribution for the first document\n",
    "doc_topic_distribution = lda_model.get_document_topics(corpus[0])\n",
    "print(f\"\\n--- DOCUMENT-TOPIC DISTRIBUTION (Doc 1) ---\")\n",
    "print(f\"Document 1 is about: {doc_topic_distribution}\")\n",
    "# Example: [(0, 0.95), (1, 0.05)] -> 95% Topic 1, 5% Topic 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e767ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
