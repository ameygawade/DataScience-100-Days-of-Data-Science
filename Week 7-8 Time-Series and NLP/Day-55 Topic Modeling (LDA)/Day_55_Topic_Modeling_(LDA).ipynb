{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27441541",
   "metadata": {},
   "source": [
    "# Day-55: Topic Modeling (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a4065b",
   "metadata": {},
   "source": [
    "We've covered what text is, what it means, and how it feels. \n",
    "\n",
    "Today, we tackle the big picture: Topic Modeling. This is a powerful machine learning technique that allows us to scan massive collections of documents (a corpus) and automatically discover the abstract topics that run through them. We'll focus on the most famous algorithm for this task: Latent Dirichlet Allocation (LDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fc6bb9",
   "metadata": {},
   "source": [
    "## Topic Covered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024cf39c",
   "metadata": {},
   "source": [
    "Latent Dirichlet Allocation, Document-Topic Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca73ce5c",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA): Uncovering Hidden Themes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732206af",
   "metadata": {},
   "source": [
    "LDA is a generative statistical model that assumes every document is a mixture of various topics, and every topic is, in turn, a collection of words that frequently occur together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dae763",
   "metadata": {},
   "source": [
    "- Generative Assumption: LDA works backward, assuming a process where documents are created by:\n",
    "\n",
    "    1. Randomly choosing a topic (e.g., \"Space Exploration\").\n",
    "\n",
    "    2. Randomly choosing a word from that topic (e.g., \"rocket,\" \"Mars,\" \"launch\").\n",
    "\n",
    "    3. Repeating this process until the document is complete.\n",
    "\n",
    "- `The Goal`: LDA reverses this, determining the most likely set of topics and their associated words that could have generated the observed corpus.\n",
    "\n",
    "- `Analogy`: The Cook's Recipe Book. Imagine you have a large library of recipes. LDA doesn't read the titles; it looks at the ingredients:\n",
    "\n",
    "    - If a \"topic\" frequently uses \"flour,\" \"sugar,\" and \"butter,\" LDA labels it the 'Baking' Topic.\n",
    "\n",
    "    - If a \"topic\" frequently uses \"tomato,\" \"pasta,\" and \"basil,\" LDA labels it the 'Italian Food' Topic.\n",
    "\n",
    "    - A single recipe (document) might be 80% 'Italian Food' and 20% 'Baking' (for the bread sticks)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35c3d24",
   "metadata": {},
   "source": [
    "## Document-Topic Distributions: The Mixture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16f909a",
   "metadata": {},
   "source": [
    "The output of an LDA model provides two key distributions that summarize the corpus:\n",
    "\n",
    "1. Topic-Word Distribution: This shows the probability of a specific word appearing in a specific topic. This is how we define and interpret the topics.\n",
    "\n",
    "    - Example: Topic 1: {(dog,0.15),(cat,0.10),(vet,0.05),…}\n",
    "\n",
    "2. Document-Topic Distribution: This shows the proportional mixture of topics within each document. This tells you what each document is about.\n",
    "\n",
    "    - Example: Document A: Topic 1 (Pets):85%, Topic 2 (Finance):10%, Topic 3 (Travel):5%.\n",
    "\n",
    "By analyzing the Document-Topic Distribution, you can cluster, filter, or categorize documents based on their content without ever manually reading and labeling them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade07e0c",
   "metadata": {},
   "source": [
    "## Code Example: Topic Modeling with Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694c9360",
   "metadata": {},
   "source": [
    "We use the Gensim library, which is the industry standard for implementing LDA. This requires cleaning and feature extraction (BoW) from our previous days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac6f716",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip uninstall gensim numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ccea8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.23.5\n",
      "  Using cached numpy-1.23.5-cp311-cp311-win_amd64.whl.metadata (2.3 kB)\n",
      "Using cached numpy-1.23.5-cp311-cp311-win_amd64.whl (14.6 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "Successfully installed numpy-1.23.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "numba 0.61.2 requires numpy<2.3,>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "! pip install numpy==1.23.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8ce2d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (1.23.5)\n",
      "Requirement already satisfied: nltk in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from gensim) (7.3.1)\n",
      "Requirement already satisfied: click in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from nltk) (2025.9.18)\n",
      "Requirement already satisfied: tqdm in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "! pip install gensim numpy nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2701dd3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m corpora\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mldamodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LdaModel\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stopwords\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amey9\\Documents\\GitHub\\DataScience-100-Days-of-Data-Science\\.venv\\Lib\\site-packages\\gensim\\__init__.py:11\u001b[39m\n\u001b[32m      7\u001b[39m __version__ = \u001b[33m'\u001b[39m\u001b[33m4.3.3\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[32m     14\u001b[39m logger = logging.getLogger(\u001b[33m'\u001b[39m\u001b[33mgensim\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger.handlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amey9\\Documents\\GitHub\\DataScience-100-Days-of-Data-Science\\.venv\\Lib\\site-packages\\gensim\\corpora\\__init__.py:6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexedcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmmcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbleicorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amey9\\Documents\\GitHub\\DataScience-100-Days-of-Data-Science\\.venv\\Lib\\site-packages\\gensim\\corpora\\indexedcorpus.py:14\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[32m     16\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIndexedCorpus\u001b[39;00m(interfaces.CorpusABC):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amey9\\Documents\\GitHub\\DataScience-100-Days-of-Data-Science\\.venv\\Lib\\site-packages\\gensim\\interfaces.py:19\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[33;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[32m      8\u001b[39m \n\u001b[32m      9\u001b[39m \u001b[33;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m \n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[32m     22\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mCorpusABC\u001b[39;00m(utils.SaveLoad):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amey9\\Documents\\GitHub\\DataScience-100-Days-of-Data-Science\\.venv\\Lib\\site-packages\\gensim\\matutils.py:1034\u001b[39m\n\u001b[32m   1029\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m1.\u001b[39m - \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mlen\u001b[39m(set1 & set2)) / \u001b[38;5;28mfloat\u001b[39m(union_cardinality)\n\u001b[32m   1032\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1033\u001b[39m     \u001b[38;5;66;03m# try to load fast, cythonized code if possible\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1034\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_matutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logsumexp, mean_absolute_difference, dirichlet_expectation\n\u001b[32m   1036\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlogsumexp\u001b[39m(x):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amey9\\Documents\\GitHub\\DataScience-100-Days-of-Data-Science\\.venv\\Lib\\site-packages\\gensim\\_matutils.pyx:1\u001b[39m, in \u001b[36minit gensim._matutils\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# --- 0. Data Preparation (Simulation) ---\n",
    "documents = [\n",
    "    \"Machine learning models predict stock prices using technical analysis and financial reports.\",\n",
    "    \"The new rocket launch from NASA successfully put a satellite into orbit around the Earth.\",\n",
    "    \"Neural networks and deep learning are crucial for complex machine learning tasks like image recognition.\",\n",
    "    \"Astronauts and cosmonauts prepare for long duration space missions to the Moon and Mars.\",\n",
    "    \"A massive portfolio diversification strategy involves stocks, bonds, and real estate investments.\"\n",
    "]\n",
    "\n",
    "# Simple Preprocessing (Tokenization, Lowercasing, Stopword Removal)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "processed_docs = []\n",
    "for doc in documents:\n",
    "    # Tokenize and remove stopwords/punctuation\n",
    "    tokens = [token.lower() for token in word_tokenize(doc) if token.isalpha() and token.lower() not in stop_words]\n",
    "    processed_docs.append(tokens)\n",
    "\n",
    "# --- 1. Feature Extraction (Dictionary and Corpus) ---\n",
    "# Gensim uses its own dictionary and corpus format (BoW)\n",
    "\n",
    "# Create a dictionary (Mapping of word -> unique ID)\n",
    "dictionary = corpora.Dictionary(processed_docs)\n",
    "\n",
    "# Create a DTM (Document-Term Matrix / Bag-of-Words Corpus)\n",
    "# Each tuple is (word_id, count)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "# --- 2. LDA Model Training ---\n",
    "# We tell the model to find a specific number of topics (e.g., 2)\n",
    "num_topics = 2\n",
    "lda_model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=num_topics,\n",
    "    random_state=42,\n",
    "    passes=10 # Number of training passes\n",
    ")\n",
    "\n",
    "# --- 3. Interpretation (Topic-Word Distribution) ---\n",
    "print(\"--- TOPIC-WORD DISTRIBUTIONS ---\")\n",
    "for idx, topic in lda_model.print_topics(num_words=5):\n",
    "    print(f\"Topic #{idx + 1}: {topic}\")\n",
    "\n",
    "# Expected output shows clear separation:\n",
    "# Topic 1: likely 'Finance/ML' words (e.g., 'stock', 'investments', 'learning')\n",
    "# Topic 2: likely 'Space' words (e.g., 'rocket', 'NASA', 'Mars')\n",
    "\n",
    "# --- 4. Document-Topic Distribution ---\n",
    "# Get the topic distribution for the first document\n",
    "doc_topic_distribution = lda_model.get_document_topics(corpus[0])\n",
    "print(f\"\\n--- DOCUMENT-TOPIC DISTRIBUTION (Doc 1) ---\")\n",
    "print(f\"Document 1 is about: {doc_topic_distribution}\")\n",
    "# Example: [(0, 0.95), (1, 0.05)] -> 95% Topic 1, 5% Topic 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e767ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
