{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f55fedb",
   "metadata": {},
   "source": [
    "# Day-50:Introduction to NLP + Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1d9cc4",
   "metadata": {},
   "source": [
    "We're shifting gears completely today. After weeks of numbers, time series, and metrics, we're diving into the messy, fascinating world of text with Natural Language Processing (NLP). Today, we focus on the most fundamental skill: Text Preprocessing—turning raw human language into structured data that machines can understand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a13eafa",
   "metadata": {},
   "source": [
    "## Topic Covered:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21730c25",
   "metadata": {},
   "source": [
    "- Tokenization, \n",
    "- Lowercasing, \n",
    "- Stemming, \n",
    "- Lemmatization, \n",
    "- Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7a6cda",
   "metadata": {},
   "source": [
    "## Tokenization: Breaking it Down"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43ac67b",
   "metadata": {},
   "source": [
    "Tokenization is the process of splitting text into smaller, meaningful units called tokens. These tokens are typically words, but can also be sentences, phrases, or symbols. \n",
    "It's the first step in any NLP pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6822b95e",
   "metadata": {},
   "source": [
    "- `Analogy`: The Assembly Line. Before you can analyze a car, you must break it down into its core components (engine, tire, door). Tokens are the components of text.\n",
    "\n",
    "- `Example`:\n",
    "    - Raw Text: \"The fox ran quickly to the box.\"\n",
    "\n",
    "    - Tokens: ['The', 'fox', 'ran', 'quickly', 'to', 'the', 'box', '.'] (Note: Punctuation is often a token!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c69b6ba",
   "metadata": {},
   "source": [
    "## Lowercasing: Standardization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e4c719",
   "metadata": {},
   "source": [
    "Lowercasing converts all tokens to a uniform case (usually lowercase). This ensures that the model treats \"Apple\" (the company) and \"apple\" (the fruit) as the same word, which is usually necessary for basic tasks like counting word frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f39c5d6",
   "metadata": {},
   "source": [
    "- `Analogy`: Standardizing Measurements. You wouldn't measure half of your data in meters and the other half in feet. Lowercasing standardizes the linguistic measurement.\n",
    "\n",
    "- `Example`: ['The', 'Fox'] → ['the', 'fox']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59731e5",
   "metadata": {},
   "source": [
    "## Stopwords Removal: Trimming the Fat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee59f01e",
   "metadata": {},
   "source": [
    "Stopwords are common words that appear frequently but typically add little meaning to the content or sentiment of a sentence (e.g., 'the', 'a', 'is', 'for'). Removing them reduces the vocabulary size and noise, speeding up training and often improving model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e59f5e2",
   "metadata": {},
   "source": [
    "- `Analogy`: Filtering Spam. Stopwords are like the junk mail in your mailbox—they're volume, but not value. You filter them out to focus on the important content.\n",
    "\n",
    "- `Example`:\n",
    "\n",
    "    - Tokens: ['the', 'fox', 'ran', 'quickly', 'to', 'the', 'box']\n",
    "\n",
    "    - Stopwords Removed: ['fox', 'ran', 'quickly', 'box']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192b45e8",
   "metadata": {},
   "source": [
    "## Stemming: The Blunt Instrument "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073ba48b",
   "metadata": {},
   "source": [
    "Stemming is a simple, heuristic process of chopping off the ends of words to get to a common root or stem. It's fast but often produces roots that are not actual dictionary words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b568921e",
   "metadata": {},
   "source": [
    "- `Analogy`: Cutting a Tree Branch. You just cut the branch close to the trunk. The result is rough, but you still know where the word comes from.\n",
    "\n",
    "- `Example`:\n",
    "    - running, runs, ran → run\n",
    "\n",
    "    - finalization, finalized → final (correct)\n",
    "\n",
    "    - universal, university → univers (incorrect, over-stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e078451",
   "metadata": {},
   "source": [
    "## Lemmatization: The Refined Tool "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4208800",
   "metadata": {},
   "source": [
    "Lemmatization is a more sophisticated process that uses a vocabulary and morphological analysis (context) to convert a word back to its dictionary form, or lemma. The output is always a valid word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30bd148",
   "metadata": {},
   "source": [
    "- `Analogy`: The Dictionary Lookup. It finds the exact entry in the dictionary. It’s slower but more accurate than stemming.\n",
    "\n",
    "- `Example`:\n",
    "    - running, runs, ran → run\n",
    "    - better → good (Lemmatization understands that 'better' is the comparative form of 'good'.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1ab02a",
   "metadata": {},
   "source": [
    "# Code Example: Text Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "470f0056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: click in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2025.9.18-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 26.6 MB/s  0:00:00\n",
      "Downloading regex-2025.9.18-cp311-cp311-win_amd64.whl (276 kB)\n",
      "Installing collected packages: regex, nltk\n",
      "\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   ---------------------------------------- 2/2 [nltk]\n",
      "\n",
      "Successfully installed nltk-3.9.2 regex-2025.9.18\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0522af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amey9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\amey9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\amey9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\amey9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure necessary NLTK data files are downloaded\n",
    "# You only need to run these lines once\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c36aab8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercased: the foxes were running quickly across the fields, but it was raining heavily.\n",
      "Tokens: ['the', 'foxes', 'were', 'running', 'quickly', 'across', 'the', 'fields', ',', 'but', 'it', 'was', 'raining', 'heavily', '.']\n",
      "\n",
      "Filtered Tokens: ['foxes', 'running', 'quickly', 'across', 'fields', 'raining', 'heavily']\n",
      "\n",
      "Stemmed Tokens: ['fox', 'run', 'quickli', 'across', 'field', 'rain', 'heavili']\n",
      "Lemmatized Tokens: ['fox', 'run', 'quickly', 'across', 'field', 'rain', 'heavily']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Sample text\n",
    "text = \"The foxes were running quickly across the fields, but it was raining heavily.\"\n",
    "\n",
    "# 1. Lowercasing\n",
    "text_lower = text.lower()\n",
    "print(f\"Lowercased: {text_lower}\")\n",
    "# Output: the foxes were running quickly across the fields, but it was raining heavily.\n",
    "\n",
    "# 2. Tokenization\n",
    "tokens = word_tokenize(text_lower)\n",
    "print(f\"Tokens: {tokens}\\n\")\n",
    "# Output: ['the', 'foxes', 'were', 'running', 'quickly', 'across', 'the', 'fields', ',', 'but', 'it', 'was', 'raining', 'heavily', '.']\n",
    "\n",
    "# 3. Stopwords Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens_filtered = [word for word in tokens if word not in stop_words and word.isalpha()] # .isalpha() removes punctuation\n",
    "print(f\"Filtered Tokens: {tokens_filtered}\\n\")\n",
    "# Output: ['foxes', 'running', 'quickly', 'across', 'fields', 'raining', 'heavily']\n",
    "\n",
    "# 4. Stemming (Rough Reduction)\n",
    "stemmer = PorterStemmer()\n",
    "tokens_stemmed = [stemmer.stem(word) for word in tokens_filtered]\n",
    "print(f\"Stemmed Tokens: {tokens_stemmed}\")\n",
    "# Output: ['fox', 'run', 'quickli', 'across', 'field', 'rain', 'heavili'] # Notice 'quickli' and 'heavili' are not real words\n",
    "\n",
    "# 5. Lemmatization (Refined Reduction)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Need to supply the Part-of-Speech (pos='v' for verb is common)\n",
    "tokens_lemmatized = [lemmatizer.lemmatize(word, pos='v') for word in tokens_filtered]\n",
    "print(f\"Lemmatized Tokens: {tokens_lemmatized}\")\n",
    "# Output: ['fox', 'run', 'quickly', 'across', 'field', 'rain', 'heavily'] # Cleaner results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8fe09a",
   "metadata": {},
   "source": [
    "## Summary of Day 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c107c8",
   "metadata": {},
   "source": [
    "Today, you learned that NLP starts with cleaning! You mastered the essential preprocessing pipeline: Tokenization to break text apart, Lowercasing for standardization, Stopword Removal for efficiency, and Stemming vs. Lemmatization for reducing words to their base forms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1317705b",
   "metadata": {},
   "source": [
    "## What's Next (Day 51)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776d03ec",
   "metadata": {},
   "source": [
    "Now that you can clean text, how do you turn those lists of words into numbers a machine can process? Tomorrow, on Day 51, we'll dive into the two foundational techniques for feature extraction in NLP: Bag of Words (BoW) and TF-IDF. You'll learn how to create count vectors and use term weighting to build a numerical matrix from your text!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
