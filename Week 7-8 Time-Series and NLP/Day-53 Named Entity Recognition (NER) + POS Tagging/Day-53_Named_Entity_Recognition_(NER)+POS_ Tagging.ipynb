{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "871f55f0",
   "metadata": {},
   "source": [
    "# Day-53: Named Entity Recongnition (NER) + POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4682ed",
   "metadata": {},
   "source": [
    "We've mastered turning words into meaning-rich vectors. Now, we shift our focus from individual words to sentence structure and information extraction. Today, we're diving into the essential structural analysis tools of NLP: Part-of-Speech (POS) Tagging and Named Entity Recognition (NER), primarily using the industry-leading library, spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aaa28c",
   "metadata": {},
   "source": [
    "## Topic Covered:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54487aa",
   "metadata": {},
   "source": [
    "## Part-of-Speech (POS) Tagging: The Grammar Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507357ec",
   "metadata": {},
   "source": [
    "POS Tagging is the process of marking up a word in a text as corresponding to a particular part of speech, based on both its definition and its context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e412cf",
   "metadata": {},
   "source": [
    "- `How it Works`: Every word is labeled with its grammatical role: noun, verb, adjective, adverb, pronoun, preposition, etc. This is crucial for understanding relationships between words and for more advanced techniques like lemmatization (which needs the POS tag to correctly find the base form).\n",
    "\n",
    "- `Analogy`: Grammar School. It's like having a grammar teacher go through your sentence and label every word's function.\n",
    "\n",
    "- `Example`:\n",
    "\n",
    "    - `Sentence`: \"Apple stock soared yesterday.\"\n",
    "\n",
    "        - `POS Tags`:\n",
    "\n",
    "            \"Apple\" → Noun (NNP - Proper Noun)\n",
    "\n",
    "            \"stock\" → Noun (NN - Noun, singular)\n",
    "\n",
    "            \"soared\" → Verb (VBD - Verb, past tense)\n",
    "\n",
    "            \"yesterday\" → Noun (NN - Noun, singular)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dada3790",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER): Extracting Key Information "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d01432a",
   "metadata": {},
   "source": [
    "NER is the process of automatically locating and classifying key elements in text into pre-defined categories such as person names, organizations, locations, quantities, monetary values, and dates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532d637d",
   "metadata": {},
   "source": [
    "- `How it Works`: NER models use context to identify spans of text that correspond to \"named entities.\" This is essential for building knowledge graphs, automating customer support, and summarizing large documents.\n",
    "\n",
    "- `Analogy` : The Highlighter. It's like automatically highlighting the most important facts (names, dates, places) in a long document.\n",
    "\n",
    "- `Example (Common spaCy Entity Types)`:\n",
    "\n",
    "        ORG: Organization (e.g., Google, Tesla)\n",
    "\n",
    "        PERSON: People (e.g., Elon Musk, Taylor Swift)\n",
    "\n",
    "        GPE: Geopolitical Entity (e.g., Paris, Germany)\n",
    "\n",
    "        DATE: Absolute or relative dates/periods (e.g., 2025, yesterday)\n",
    "\n",
    "- `Example NER`:\n",
    "\n",
    "    - `Sentence`: \"Tim Cook announced a new iPhone at Apple Park last Tuesday.\"\n",
    "\n",
    "    - `Entities`:\n",
    "\n",
    "        \"Tim Cook\" → PERSON\n",
    "\n",
    "        \"Apple Park\" → FAC (Facility)\n",
    "\n",
    "        \"last Tuesday\" → DATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b78551",
   "metadata": {},
   "source": [
    "## The Power of spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12735a1c",
   "metadata": {},
   "source": [
    "spaCy is a highly efficient and production-ready Python library built for advanced NLP tasks. Unlike NLTK (which is more academic), spaCy is designed for speed and scale, making it the industry standard for NER and POS tagging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29c3f0f",
   "metadata": {},
   "source": [
    "## Code Example: spaCy for POS Tagging and NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e518af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.7-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.13-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp311-cp311-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.10-cp311-cp311-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.6-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.19.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from spacy) (2.32.4)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.11.10-py3-none-any.whl.metadata (68 kB)\n",
      "Collecting jinja2 (from spacy)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from spacy) (25.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.33.2-cp311-cp311-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.7.14)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.0-cp311-cp311-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Using cached numpy-2.3.3-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.22.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.3.1-cp311-cp311-win_amd64.whl.metadata (10 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->spacy)\n",
      "  Downloading markupsafe-3.0.3-cp311-cp311-win_amd64.whl.metadata (2.8 kB)\n",
      "Downloading spacy-3.8.7-cp311-cp311-win_amd64.whl (14.9 MB)\n",
      "   ---------------------------------------- 0.0/14.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/14.9 MB ? eta -:--:--\n",
      "   ----------------------- ---------------- 8.9/14.9 MB 55.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.9/14.9 MB 49.4 MB/s  0:00:00\n",
      "Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp311-cp311-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.13-cp311-cp311-win_amd64.whl (24 kB)\n",
      "Downloading preshed-3.0.10-cp311-cp311-win_amd64.whl (117 kB)\n",
      "Downloading pydantic-2.11.10-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp311-cp311-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 54.5 MB/s  0:00:00\n",
      "Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp311-cp311-win_amd64.whl (632 kB)\n",
      "   ---------------------------------------- 0.0/632.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 632.6/632.6 kB 23.2 MB/s  0:00:00\n",
      "Downloading thinc-8.3.6-cp311-cp311-win_amd64.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.8/1.8 MB 32.2 MB/s  0:00:00\n",
      "Downloading blis-1.3.0-cp311-cp311-win_amd64.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 6.2/6.2 MB 54.3 MB/s  0:00:00\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading numpy-2.3.3-cp311-cp311-win_amd64.whl (13.1 MB)\n",
      "   ---------------------------------------- 0.0/13.1 MB ? eta -:--:--\n",
      "   -------------------------- ------------- 8.7/13.1 MB 44.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.1/13.1 MB 43.2 MB/s  0:00:00\n",
      "Downloading typer-0.19.2-py3-none-any.whl (46 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading cloudpathlib-0.22.0-py3-none-any.whl (61 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 5.4/5.4 MB 47.0 MB/s  0:00:00\n",
      "Downloading marisa_trie-1.3.1-cp311-cp311-win_amd64.whl (143 kB)\n",
      "Downloading rich-14.1.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading markupsafe-3.0.3-cp311-cp311-win_amd64.whl (15 kB)\n",
      "Installing collected packages: cymem, wasabi, typing-inspection, spacy-loggers, spacy-legacy, shellingham, pydantic-core, numpy, murmurhash, mdurl, MarkupSafe, marisa-trie, cloudpathlib, catalogue, annotated-types, srsly, pydantic, preshed, markdown-it-py, language-data, jinja2, blis, rich, langcodes, confection, typer, thinc, weasel, spacy\n",
      "\n",
      "   - --------------------------------------  1/29 [wasabi]\n",
      "   ---- -----------------------------------  3/29 [spacy-loggers]\n",
      "   ----- ----------------------------------  4/29 [spacy-legacy]\n",
      "   ----- ----------------------------------  4/29 [spacy-legacy]\n",
      "   -------- -------------------------------  6/29 [pydantic-core]\n",
      "  Attempting uninstall: numpy\n",
      "   -------- -------------------------------  6/29 [pydantic-core]\n",
      "    Found existing installation: numpy 1.26.4\n",
      "   -------- -------------------------------  6/29 [pydantic-core]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "    Uninstalling numpy-1.26.4:\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   --------- ------------------------------  7/29 [numpy]\n",
      "   ----------- ----------------------------  8/29 [murmurhash]\n",
      "   --------------- ------------------------ 11/29 [marisa-trie]\n",
      "   ---------------- ----------------------- 12/29 [cloudpathlib]\n",
      "   ---------------- ----------------------- 12/29 [cloudpathlib]\n",
      "   ----------------- ---------------------- 13/29 [catalogue]\n",
      "   -------------------- ------------------- 15/29 [srsly]\n",
      "   -------------------- ------------------- 15/29 [srsly]\n",
      "   -------------------- ------------------- 15/29 [srsly]\n",
      "   -------------------- ------------------- 15/29 [srsly]\n",
      "   -------------------- ------------------- 15/29 [srsly]\n",
      "   -------------------- ------------------- 15/29 [srsly]\n",
      "   -------------------- ------------------- 15/29 [srsly]\n",
      "   -------------------- ------------------- 15/29 [srsly]\n",
      "   -------------------- ------------------- 15/29 [srsly]\n",
      "   -------------------- ------------------- 15/29 [srsly]\n",
      "   -------------------- ------------------- 15/29 [srsly]\n",
      "   ---------------------- ----------------- 16/29 [pydantic]\n",
      "   ---------------------- ----------------- 16/29 [pydantic]\n",
      "   ---------------------- ----------------- 16/29 [pydantic]\n",
      "   ---------------------- ----------------- 16/29 [pydantic]\n",
      "   ---------------------- ----------------- 16/29 [pydantic]\n",
      "   ---------------------- ----------------- 16/29 [pydantic]\n",
      "   ---------------------- ----------------- 16/29 [pydantic]\n",
      "   ---------------------- ----------------- 16/29 [pydantic]\n",
      "   ---------------------- ----------------- 16/29 [pydantic]\n",
      "   ---------------------- ----------------- 16/29 [pydantic]\n",
      "   ---------------------- ----------------- 16/29 [pydantic]\n",
      "   ---------------------- ----------------- 16/29 [pydantic]\n",
      "   ---------------------- ----------------- 16/29 [pydantic]\n",
      "   ---------------------- ----------------- 16/29 [pydantic]\n",
      "   ---------------------- ----------------- 16/29 [pydantic]\n",
      "   ------------------------ --------------- 18/29 [markdown-it-py]\n",
      "   ------------------------ --------------- 18/29 [markdown-it-py]\n",
      "   ------------------------ --------------- 18/29 [markdown-it-py]\n",
      "   ------------------------ --------------- 18/29 [markdown-it-py]\n",
      "   ------------------------ --------------- 18/29 [markdown-it-py]\n",
      "   ------------------------ --------------- 18/29 [markdown-it-py]\n",
      "   ------------------------ --------------- 18/29 [markdown-it-py]\n",
      "   ------------------------ --------------- 18/29 [markdown-it-py]\n",
      "   -------------------------- ------------- 19/29 [language-data]\n",
      "   -------------------------- ------------- 19/29 [language-data]\n",
      "   -------------------------- ------------- 19/29 [language-data]\n",
      "   -------------------------- ------------- 19/29 [language-data]\n",
      "   -------------------------- ------------- 19/29 [language-data]\n",
      "   -------------------------- ------------- 19/29 [language-data]\n",
      "   -------------------------- ------------- 19/29 [language-data]\n",
      "   -------------------------- ------------- 19/29 [language-data]\n",
      "   --------------------------- ------------ 20/29 [jinja2]\n",
      "   --------------------------- ------------ 20/29 [jinja2]\n",
      "   --------------------------- ------------ 20/29 [jinja2]\n",
      "   ---------------------------- ----------- 21/29 [blis]\n",
      "   ------------------------------ --------- 22/29 [rich]\n",
      "   ------------------------------ --------- 22/29 [rich]\n",
      "   ------------------------------ --------- 22/29 [rich]\n",
      "   ------------------------------ --------- 22/29 [rich]\n",
      "   ------------------------------ --------- 22/29 [rich]\n",
      "   ------------------------------ --------- 22/29 [rich]\n",
      "   ------------------------------ --------- 22/29 [rich]\n",
      "   ------------------------------ --------- 22/29 [rich]\n",
      "   ------------------------------ --------- 22/29 [rich]\n",
      "   ------------------------------- -------- 23/29 [langcodes]\n",
      "   ------------------------------- -------- 23/29 [langcodes]\n",
      "   --------------------------------- ------ 24/29 [confection]\n",
      "   ---------------------------------- ----- 25/29 [typer]\n",
      "   ---------------------------------- ----- 25/29 [typer]\n",
      "   ----------------------------------- ---- 26/29 [thinc]\n",
      "   ----------------------------------- ---- 26/29 [thinc]\n",
      "   ----------------------------------- ---- 26/29 [thinc]\n",
      "   ----------------------------------- ---- 26/29 [thinc]\n",
      "   ----------------------------------- ---- 26/29 [thinc]\n",
      "   ----------------------------------- ---- 26/29 [thinc]\n",
      "   ----------------------------------- ---- 26/29 [thinc]\n",
      "   ----------------------------------- ---- 26/29 [thinc]\n",
      "   ----------------------------------- ---- 26/29 [thinc]\n",
      "   ----------------------------------- ---- 26/29 [thinc]\n",
      "   ----------------------------------- ---- 26/29 [thinc]\n",
      "   ----------------------------------- ---- 26/29 [thinc]\n",
      "   ----------------------------------- ---- 26/29 [thinc]\n",
      "   ----------------------------------- ---- 26/29 [thinc]\n",
      "   ----------------------------------- ---- 26/29 [thinc]\n",
      "   ------------------------------------- -- 27/29 [weasel]\n",
      "   ------------------------------------- -- 27/29 [weasel]\n",
      "   ------------------------------------- -- 27/29 [weasel]\n",
      "   ------------------------------------- -- 27/29 [weasel]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   -------------------------------------- - 28/29 [spacy]\n",
      "   ---------------------------------------- 29/29 [spacy]\n",
      "\n",
      "Successfully installed MarkupSafe-3.0.3 annotated-types-0.7.0 blis-1.3.0 catalogue-2.0.10 cloudpathlib-0.22.0 confection-0.1.5 cymem-2.0.11 jinja2-3.1.6 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.3.1 markdown-it-py-4.0.0 mdurl-0.1.2 murmurhash-1.0.13 numpy-2.3.3 preshed-3.0.10 pydantic-2.11.10 pydantic-core-2.33.2 rich-14.1.0 shellingham-1.5.4 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.6 typer-0.19.2 typing-inspection-0.4.2 wasabi-1.1.3 weasel-0.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.3.3 which is incompatible.\n",
      "numba 0.61.2 requires numpy<2.3,>=1.24, but you have numpy 2.3.3 which is incompatible.\n",
      "scipy 1.13.1 requires numpy<2.3,>=1.22.4, but you have numpy 2.3.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "! pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e063eb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------  12.6/12.8 MB 65.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 57.3 MB/s  0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "539a1f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- POS TAGGING ---\n",
      "Google     | PROPN    | NNP  \n",
      "announced  | VERB     | VBD  \n",
      "that       | SCONJ    | IN   \n",
      "CEO        | PROPN    | NNP  \n",
      "Sundar     | PROPN    | NNP  \n",
      "Pichai     | PROPN    | NNP  \n",
      "visited    | VERB     | VBD  \n",
      "London     | PROPN    | NNP  \n",
      "on         | ADP      | IN   \n",
      "Monday     | PROPN    | NNP  \n",
      "to         | PART     | TO   \n",
      "discuss    | VERB     | VB   \n",
      "a          | DET      | DT   \n",
      "$          | SYM      | $    \n",
      "1          | NUM      | CD   \n",
      "billion    | NUM      | CD   \n",
      "investment | NOUN     | NN   \n",
      ".          | PUNCT    | .    \n",
      "\n",
      "--- NAMED ENTITY RECOGNITION (NER) ---\n",
      "Entity: Google               | Type: ORG        | Explanation: Companies, agencies, institutions, etc.\n",
      "Entity: Sundar Pichai        | Type: PERSON     | Explanation: People, including fictional\n",
      "Entity: London               | Type: GPE        | Explanation: Countries, cities, states\n",
      "Entity: Monday               | Type: DATE       | Explanation: Absolute or relative dates or periods\n",
      "Entity: $1 billion           | Type: MONEY      | Explanation: Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the small English language model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"Downloading spaCy model. Please run 'python -m spacy download en_core_web_sm' once.\")\n",
    "    # Fallback to loading it after assuming download is handled outside notebook\n",
    "\n",
    "# Sample text for analysis\n",
    "text = \"Google announced that CEO Sundar Pichai visited London on Monday to discuss a $1 billion investment.\"\n",
    "\n",
    "# Process the text with the spaCy pipeline\n",
    "doc = nlp(text)\n",
    "\n",
    "# --- 1. PART-OF-SPEECH (POS) TAGGING ---\n",
    "print(\"--- POS TAGGING ---\")\n",
    "for token in doc:\n",
    "    # token.text: The original word\n",
    "    # token.pos_: The simple POS tag (e.g., NOUN, VERB)\n",
    "    # token.tag_: The detailed POS tag (e.g., NNP, VBD)\n",
    "    print(f\"{token.text:<10} | {token.pos_:<8} | {token.tag_:<5}\")\n",
    "\n",
    "# --- 2. NAMED ENTITY RECOGNITION (NER) ---\n",
    "print(\"\\n--- NAMED ENTITY RECOGNITION (NER) ---\")\n",
    "for ent in doc.ents:\n",
    "    # ent.text: The actual entity text\n",
    "    # ent.label_: The type of entity\n",
    "    print(f\"Entity: {ent.text:<20} | Type: {ent.label_:<10} | Explanation: {spacy.explain(ent.label_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a045cb7",
   "metadata": {},
   "source": [
    "## Summary of Day 53"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdd697d",
   "metadata": {},
   "source": [
    "Today, you learned how to structurally analyze text using spaCy. POS Tagging determines the grammatical function of every word, while NER identifies and classifies critical real-world entities like people, organizations, and monetary values. This structural understanding is vital for advanced tasks like automated content classification and question-answering systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2a87cf",
   "metadata": {},
   "source": [
    "## What's Next (Day 54)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364f727c",
   "metadata": {},
   "source": [
    "Now that you can extract both semantic meaning (Word2Vec) and structural information (NER), the next logical step is to analyze the feeling behind the words! Tomorrow, on Day 54, we'll dive into Sentiment Analysis, learning how to use libraries like VADER and build simple logistic regression models to detect the polarity (positive/negative/neutral) of text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9603fa98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
