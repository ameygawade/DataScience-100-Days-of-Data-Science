{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7236247",
   "metadata": {},
   "source": [
    "# Day-56: NLP Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df11ad36",
   "metadata": {},
   "source": [
    "Today, on Day 56, we'll put these pieces together in a practical NLP Project focusing on common business use cases like review mining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45756a1",
   "metadata": {},
   "source": [
    "A major commercial application of NLP is Review Mining—analyzing customer feedback (reviews, tweets, surveys) to extract actionable insights. This project ties together your preprocessing, feature engineering, and classification skills to build a functional system for understanding what customers like and dislike."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763c2b55",
   "metadata": {},
   "source": [
    "## Topics Covered:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5042b01d",
   "metadata": {},
   "source": [
    "Classification, Keyword Extraction, Review Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61714931",
   "metadata": {},
   "source": [
    "## Review Mining: The Business Problem "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a9df90",
   "metadata": {},
   "source": [
    "Review Mining is the process of extracting meaningful information, patterns, and insights from text-based customer feedback. It enables businesses to quickly:\n",
    "\n",
    "1. Quantify Sentiment: Determine the percentage of positive vs. negative feedback.\n",
    "\n",
    "2. Identify Pain Points: Automatically flag recurring negative themes (e.g., \"slow delivery,\" \"poor quality\").\n",
    "\n",
    "3. Track Trends: Monitor changes in sentiment over time or after a product update.\n",
    "\n",
    "- `Analogy`: The Customer Service Supervisor. Instead of reading 10,000 emails, the system provides a dashboard that shows \"52% of reviews mention Topic X, and 90% of those are negative.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81feb42",
   "metadata": {},
   "source": [
    "## Sentiment Classification: The End Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ec1554",
   "metadata": {},
   "source": [
    "We use the labeled Sentiment column to train a supervised model that can predict the polarity of future, unseen comments.\n",
    "\n",
    "- `Algorithm`: We stick with Logistic Regression (Day 54) for its simplicity and interpretability, paired with TF-IDF features (Day 51).\n",
    "\n",
    "- **Why not VADER?**\n",
    "    - While VADER is fast (Day 54), a trained Logistic Regression model learns the specific nuances and slang of the YouTube comment environment (like using emojis or non-standard language), which makes it a more accurate domain-specific classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a93a35",
   "metadata": {},
   "source": [
    "## Keyword Extraction: Diagnosing the Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37db140",
   "metadata": {},
   "source": [
    "Once we identify a group of negative comments, we need to know why they are negative. We use TF-IDF scores to identify words that are highly specific to that group of comments.\n",
    "\n",
    "- `Process`: Filter all comments labeled 'Negative' (or those predicted as negative by the model) and then re-run the TF-IDF calculation on just that subset. Words with the highest weight represent the core keywords of the complaint (e.g., \"broken,\" \"slow,\" \"spam\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "928f26e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabulate in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: kagglehub in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: packaging in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from kagglehub) (25.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from kagglehub) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from kagglehub) (2.32.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from kagglehub[pandas-datasets]) (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from pandas->kagglehub[pandas-datasets]) (1.23.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from pandas->kagglehub[pandas-datasets]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from pandas->kagglehub[pandas-datasets]) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from pandas->kagglehub[pandas-datasets]) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->kagglehub[pandas-datasets]) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from requests->kagglehub) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from requests->kagglehub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from requests->kagglehub) (2025.7.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tqdm->kagglehub) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "! pip install tabulate kagglehub kagglehub[pandas-datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6113e33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\amey9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\amey9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\amey9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# Ensure necessary NLTK downloads are complete (run these once if needed)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0e593982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\amey9\\.cache\\kagglehub\\datasets\\atifaliak\\youtube-comments-dataset\\versions\\1\n",
      "First 5 records:                                              Comment Sentiment\n",
      "0  lets not forget that apple pay in 2014 require...   neutral\n",
      "1  here in nz 50 of retailers don’t even have con...  negative\n",
      "2  i will forever acknowledge this channel with t...  positive\n",
      "3  whenever i go to a place that doesn’t take app...  negative\n",
      "4  apple pay is so convenient secure and easy to ...  positive\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18408 entries, 0 to 18407\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Comment    18364 non-null  object\n",
      " 1   Sentiment  18408 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 287.8+ KB\n",
      "None\n",
      "Missing values in each column:\n",
      "Comment      44\n",
      "Sentiment     0\n",
      "dtype: int64\n",
      "After dropping missing values, dataset shape: (18364, 2)\n",
      "Label Mapping: ['negative' 'neutral' 'positive']\n",
      "Label Distribution:\n",
      "| Sentiment   |   count |\n",
      "|:------------|--------:|\n",
      "| positive    |   11402 |\n",
      "| neutral     |    4625 |\n",
      "| negative    |    2337 |\n",
      "\n",
      "--- 1. Preprocessing Complete ---\n",
      "Original (Example): lets not forget that apple pay in 2014 required a brand new iphone in order to use it a significant portion of apples user base wasnt able to use it even if they wanted to as each successive iphone incorporated the technology and older iphones were replaced the number of people who could use the technology increased\n",
      "Cleaned (Example): let forget apple pay required brand iphone order use significant portion apple user base wasnt able use wanted successive iphone incorporated technology older iphones replaced number use technology increased\n",
      "\n",
      "--- 2. SENTIMENT CLASSIFICATION REPORT ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.69      0.22      0.33       679\n",
      "     neutral       0.58      0.48      0.53      1375\n",
      "    positive       0.76      0.92      0.83      3456\n",
      "\n",
      "    accuracy                           0.72      5510\n",
      "   macro avg       0.68      0.54      0.56      5510\n",
      "weighted avg       0.71      0.72      0.69      5510\n",
      "\n",
      "\n",
      "--- 3. KEYWORD EXTRACTION BY SENTIMENT (Top 5) ---\n",
      "Using TF-IDF with Bigrams (1,2) to capture meaningful phrases.\n",
      "| Sentiment   | Keyword 1   | Keyword 2   | Keyword 3   | Keyword 4   | Keyword 5   |\n",
      "|-------------|-------------|-------------|-------------|-------------|-------------|\n",
      "| negative    | bad         | life        | he          | something   | doesnt      |\n",
      "| neutral     | data        | cube        | learning    | question    | conciertos  |\n",
      "| positive    | love        | video       | amazing     | keep        | song        |\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import kagglehub\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Install dependencies as needed:\n",
    "# pip install kagglehub[pandas-datasets]\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "\n",
    "# Download latest version\n",
    "file_path = kagglehub.dataset_download(\"atifaliak/youtube-comments-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", file_path)\n",
    "# Load the latest version\n",
    "df = pd.read_csv(file_path + \"\\YoutubeCommentsDataSet.csv\")\n",
    "print(\"First 5 records:\", df.head())\n",
    "\n",
    "# Basic info\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n",
    "print(\"Missing values in each column:\")\n",
    "print(df.isnull().sum()) # Check for missing values\n",
    "# Drop rows with missing values in 'Comment' or 'Sentiment'\n",
    "df.dropna(subset=['Comment', 'Sentiment'], inplace=True)\n",
    "print(\"After dropping missing values, dataset shape:\", df.shape)\n",
    "\n",
    "\n",
    "# Convert Sentiment to numerical labels\n",
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['Sentiment'])\n",
    "# Determine the mapping for readability\n",
    "label_map = dict(zip(df['label'].unique(), df['Sentiment'].unique()))\n",
    "print(f\"Label Mapping: {le.classes_}\") \n",
    "print(f\"Label Distribution:\\n{df['Sentiment'].value_counts().to_markdown()}\")\n",
    "\n",
    "\n",
    "# --- 1. Preprocessing Pipeline (Day 50) ---\n",
    "\n",
    "# Define aggressive custom stop words based on initial output (like, people, dont, etc.)\n",
    "CUSTOM_NOISE_WORDS = {\n",
    "    'like', 'people', 'dont', 'im', 'get', 'one', 'time', 'video', 'would', 'really', 'u', 'could', 'say', 'me', \n",
    "    'know', 'even', 'make', 'year', 'need', 'never', 'much', 'also', 'go', 'see', 'think', 'want', 'way', 'good', \n",
    "    'great', 'still', 'got', 'cant', 'us', 'look', 'back', 'thing', 'things', 'lot', 'lots', 'another', 'new', \n",
    "    'first', 'last', 'two', 'three', 'years', 'day', 'days', 'los', 'always', 'game', 'games', 'tan', 'anh', \n",
    "    'feel', 'man', 'says', 'said', 'sayin', 'guy', 'hermosa', 'mucho', 'mas', 'muy', 'bien', 'buenas', 'didnt', \n",
    "    'take', 'going', 'mejores', 'actually', 'better', 'best', 'work', 'works', 'working', 'watched', 'watching', \n",
    "    'watch', 'seen', 'seein', 'thank', 'thanks', 'thankyou', 'thankx', 'plz', 'plzz', 'plzzz', 'someone', 'he', \n",
    "    'right', 'many', 'she', 'they', 'we', 'ou'}\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Cleans and tokenizes text using lemmatization and aggressive stopword removal.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text) # Remove punctuation/special chars\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Combined stop words for English, French, and custom noise words\n",
    "    try:\n",
    "        combined_stop_words = set(stopwords.words('english') + stopwords.words('french'))\n",
    "    except LookupError:\n",
    "        # Fallback if French stop words are not downloaded\n",
    "        combined_stop_words = set(stopwords.words('english'))\n",
    "        print(\"Warning: Could not load French stopwords.\")\n",
    "\n",
    "    final_stop_words = combined_stop_words.union(CUSTOM_NOISE_WORDS)\n",
    "\n",
    "    # Filter stopwords, lemmatize, AND filter tokens by length (length > 2)\n",
    "    cleaned_tokens = [\n",
    "        lemmatizer.lemmatize(w) \n",
    "        for w in tokens \n",
    "        if w not in final_stop_words and len(w) > 2 # Filter out short noise tokens\n",
    "    ]\n",
    "    return \" \".join(cleaned_tokens)\n",
    "\n",
    "# Apply the preprocessing pipeline\n",
    "df['Clean_Comment'] = df['Comment'].apply(preprocess_text)\n",
    "print(\"\\n--- 1. Preprocessing Complete ---\")\n",
    "print(f\"Original (Example): {df['Comment'].iloc[0]}\")\n",
    "print(f\"Cleaned (Example): {df['Clean_Comment'].iloc[0]}\\n\")\n",
    "\n",
    "\n",
    "# --- 2. Sentiment Classification (Day 51 & Day 54) ---\n",
    "# Feature Extraction (TF-IDF)\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X = tfidf_vectorizer.fit_transform(df['Clean_Comment'])\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Training Logistic Regression Model\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation\n",
    "y_pred = lr_model.predict(X_test)\n",
    "print(\"--- 2. SENTIMENT CLASSIFICATION REPORT ---\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_,zero_division=0))\n",
    "\n",
    "\n",
    "# --- 3. Keyword Extraction by Sentiment (Review Mining) ---\n",
    "print(\"\\n--- 3. KEYWORD EXTRACTION BY SENTIMENT (Top 5) ---\")\n",
    "print(\"Using TF-IDF with Bigrams (1,2) to capture meaningful phrases.\")\n",
    "\n",
    "# List to store results for a clean table\n",
    "keyword_results = []\n",
    "\n",
    "for sentiment_class in le.classes_:\n",
    "    # Filter the original DataFrame for this specific sentiment class\n",
    "    filtered_df = df[df['Sentiment'] == sentiment_class]\n",
    "\n",
    "    if not filtered_df.empty:\n",
    "        # Key Improvement: Use ngram_range=(1, 2) to capture bigrams (e.g., 'apple pay', 'dont like')\n",
    "        sub_tfidf = TfidfVectorizer(ngram_range=(1, 2))\n",
    "        sub_X = sub_tfidf.fit_transform(filtered_df['Clean_Comment'])\n",
    "        \n",
    "        # Sum the TF-IDF weights across all documents in this class\n",
    "        feature_names = sub_tfidf.get_feature_names_out()\n",
    "        total_weights = sub_X.sum(axis=0).A1\n",
    "        keyword_scores = pd.Series(total_weights, index=feature_names)\n",
    "        \n",
    "        # Display the top 5 keywords\n",
    "        top_keywords = keyword_scores.nlargest(5)\n",
    "        \n",
    "        # Store results\n",
    "        keyword_results.append([sentiment_class] + list(top_keywords.index))\n",
    "\n",
    "# Print final keyword table\n",
    "headers = [\"Sentiment\"] + [f\"Keyword {i+1}\" for i in range(5)]\n",
    "print(tabulate(keyword_results, headers=headers, tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85701dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
