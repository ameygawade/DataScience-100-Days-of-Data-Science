{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "333d7b70",
   "metadata": {},
   "source": [
    "# Day-67: Transfer Learning with Pretrained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8376faf2",
   "metadata": {},
   "source": [
    "Welcome to Day 67! We are diving into the most powerful, time-saving, and industry-standard technique in computer vision: Transfer Learning! Let's smash this killer concept, guys!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5b0ad6",
   "metadata": {},
   "source": [
    "Imagine you’re training a CNN from scratch — it’s like teaching a baby to recognize animals from zero. That takes time, tons of images, and lots of computation!\n",
    "\n",
    "But what if I told you there’s a shortcut?\n",
    "You can take a model that’s already seen millions of images — like ResNet or MobileNet — and just adapt it to your task. That’s transfer learning — the secret weapon behind 90% of modern computer vision success stories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42802916",
   "metadata": {},
   "source": [
    "## Topics Covered\n",
    "\n",
    "- What is Transfer Learning?\n",
    "\n",
    "- Feature Extraction vs Fine-Tuning\n",
    "\n",
    "- Pretrained Models: ResNet & MobileNet\n",
    "\n",
    "- Practical Analogy\n",
    "\n",
    "- Hands-on Code Example (Keras/TensorFlow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faa5100",
   "metadata": {},
   "source": [
    "## What is Transfer Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc5b728",
   "metadata": {},
   "source": [
    "Transfer Learning is the idea of reusing a model that was already trained on a massive, general dataset (like the 14 million images in ImageNet) for a smaller, specific task (like classifying hot dogs vs. not hot dogs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bcf112",
   "metadata": {},
   "source": [
    "- `Analogy`: Learning to Drive a Truck. \n",
    "    - If you already know how to drive a car (trained on general road rules and mechanics), you don't start from scratch when you learn to drive a truck. \n",
    "    - You already have the foundational skills (steering, braking, mirrors). You just need to fine-tune those skills for a bigger vehicle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e25224",
   "metadata": {},
   "source": [
    "## Feature Extraction vs Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d075dbd0",
   "metadata": {},
   "source": [
    "### Method 1: Feature Extraction (The Quick Way)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac84f83",
   "metadata": {},
   "source": [
    "This is the fastest method, perfect when your new dataset is small and very similar to the original ImageNet data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1acb71",
   "metadata": {},
   "source": [
    "`Process`: \n",
    "1. We take the entire pre-trained CNN and freeze all the Convolutional layers. \n",
    "2. We only train the final FFNN layers (the Dense layers) that we add on top."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f63ed3",
   "metadata": {},
   "source": [
    "`Analogy`:\n",
    "It’s like using a trained chef’s knife skills but asking them to learn only your recipe — no need to relearn how to chop onions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82db075",
   "metadata": {},
   "source": [
    "`Benefit`: \n",
    "1. Since you're only training a tiny fraction of the parameters, it trains incredibly fast and prevents overfitting on small datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0090b362",
   "metadata": {},
   "source": [
    "### Method 2: Fine-Tuning (The Best Performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e5829d",
   "metadata": {},
   "source": [
    "This method is used when your new dataset is large but still slightly different from ImageNet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10adc577",
   "metadata": {},
   "source": [
    "`Process`: \n",
    "1. We unfreeze the last few Convolutional blocks of the pre-trained model. \n",
    "2. We then train the entire model (both the new Dense layers and the un-frozen Conv layers) using an extremely low learning rate (e.g., $10^{−5}$ )."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62c8f19",
   "metadata": {},
   "source": [
    "`Analogy`: Tweaking the Engine. You let the model slightly adjust the final, high-level features it learned (like dog noses or car doors) to better suit the specific objects in your new dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62182f50",
   "metadata": {},
   "source": [
    "`Benefit`: This provides the best performance but takes longer to train than simple Feature Extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cec81f",
   "metadata": {},
   "source": [
    "## Choosing the Right Pre-trained Model (ResNet vs. MobileNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd2c7cc",
   "metadata": {},
   "source": [
    "When selecting a model, we look at performance versus size:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46e9134",
   "metadata": {},
   "source": [
    "| Model | Focus | Key Innovation | When to Use |\n",
    "| :------ | :-------- | :------------------------------------------------------------ | :------------------------------------------------------------- |\n",
    "| **ResNet (e.g., ResNet50)** | Performance | Uses **Skip Connections (Residual Blocks)** to allow training of 100+ layers without vanishing gradients. | Best accuracy; use when you have sufficient **GPU power**. |\n",
    "| **MobileNet (e.g., MobileNetV2)** | Efficiency | Uses **Depthwise Separable Convolutions** to drastically reduce computation and model size. | Best for **mobile devices**, **web apps**, and **quick training** where speed matters. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7421b1d2",
   "metadata": {},
   "source": [
    "## Code Example : Feature Extraction & Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec11a72a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'data/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Data generator\u001b[39;00m\n\u001b[32m     29\u001b[39m train_datagen = ImageDataGenerator(rescale=\u001b[32m1.\u001b[39m/\u001b[32m255\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m train_gen = \u001b[43mtrain_datagen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflow_from_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata/train\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m model.fit(train_gen, epochs=\u001b[32m5\u001b[39m)\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Step 5: Fine-tuning (unfreeze top few layers)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amey9\\Documents\\GitHub\\DataScience-100-Days-of-Data-Science\\.venv\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:1138\u001b[39m, in \u001b[36mImageDataGenerator.flow_from_directory\u001b[39m\u001b[34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[39m\n\u001b[32m   1120\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mflow_from_directory\u001b[39m(\n\u001b[32m   1121\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1122\u001b[39m     directory,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1136\u001b[39m     keep_aspect_ratio=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1137\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDirectoryIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1140\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m        \u001b[49m\u001b[43msave_to_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_to_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m        \u001b[49m\u001b[43msave_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m        \u001b[49m\u001b[43msave_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_links\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubset\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amey9\\Documents\\GitHub\\DataScience-100-Days-of-Data-Science\\.venv\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:453\u001b[39m, in \u001b[36mDirectoryIterator.__init__\u001b[39m\u001b[34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[39m\n\u001b[32m    451\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[32m    452\u001b[39m     classes = []\n\u001b[32m--> \u001b[39m\u001b[32m453\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[32m    454\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m os.path.isdir(os.path.join(directory, subdir)):\n\u001b[32m    455\u001b[39m             classes.append(subdir)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 3] The system cannot find the path specified: 'data/train'"
     ]
    }
   ],
   "source": [
    "# Day 67 - Transfer Learning using ResNet50 and MobileNetV2\n",
    "from tensorflow.keras.applications import ResNet50, MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Step 1: Load pretrained model (without top layers)\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "\n",
    "# Step 2: Freeze base layers (Feature Extraction)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Step 3: Add custom layers\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Step 4: Compile and Train\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Data generator\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_gen = train_datagen.flow_from_directory('data/train', target_size=(224,224), batch_size=32)\n",
    "\n",
    "model.fit(train_gen, epochs=5)\n",
    "\n",
    "# Step 5: Fine-tuning (unfreeze top few layers)\n",
    "for layer in base_model.layers[-10:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_gen, epochs=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211effb8",
   "metadata": {},
   "source": [
    "## Summary of Day 67"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833fdc46",
   "metadata": {},
   "source": [
    "Smashing job today, guys! We've unlocked the secret weapon of modern Deep Learning: Transfer Learning!\n",
    "\n",
    "We learned the two core methods:\n",
    "\n",
    "1. Feature Extraction: Freeze the base, train only the new top Dense layer. Fastest method.\n",
    "\n",
    "2. Fine-Tuning: Unfreeze the top Conv layers and train them with a tiny learning rate. Highest accuracy method.\n",
    "\n",
    "We also know why models like ResNet and MobileNet are the industry standard for this task!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f91691b",
   "metadata": {},
   "source": [
    "## What's Next (Day 68)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f58a9d",
   "metadata": {},
   "source": [
    "We now have the architecture (CNN) and the method (Transfer Learning). But what if your dataset is still too small, even for Feature Extraction?\n",
    "\n",
    "Tomorrow, on Day 68, we will learn the final technique to fight overfitting and massively increase the diversity of your data without collecting a single new image: Data Augmentation! We'll cover Flipping, Cropping, Rotation, and Noise Injection!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d55fc86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0047e10b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f91db1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
