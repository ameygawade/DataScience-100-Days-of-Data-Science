{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c938b9a9",
   "metadata": {},
   "source": [
    "# Day-68: CNNs in Practice with TensorFlow/Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177fee7a",
   "metadata": {},
   "source": [
    "We've made massive progress this week! We mastered the CNN building blocks (Day 65) and unlocked the power of Transfer Learning (Day 67) using models like MobileNet!\n",
    "\n",
    "But here's the thing, I'm telling you: even with transfer learning, if you have a small dataset, your model will still overfit. It'll memorize the training images but fail miserably on unseen data.\n",
    "\n",
    "So, how do we get more data without spending a cent?\n",
    "\n",
    "Welcome to Day 68! Today, we master the dark art of artificially increasing our dataset size and making our data machine-ready: Data Augmentation & Image Preprocessing! Let's dive in, guys!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d85480",
   "metadata": {},
   "source": [
    "## Topics Covered:\n",
    "- Image Preprocessing: Normalization\n",
    "\n",
    "- Data Augmentation: Flipping, Rotation, Cropping, and Zooming\n",
    "\n",
    "- ImageDataGenerator in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c6e9be",
   "metadata": {},
   "source": [
    "## Image Preprocessing: Normalization (The First Step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36641df",
   "metadata": {},
   "source": [
    "Before any image hits the neural network, it needs to be processed.\n",
    "\n",
    "- `The Problem`: \n",
    "    - Raw pixel values range from 0 to 255. If you feed these large numbers into the network, the loss function values will be massive, causing huge and unstable gradient updates.\n",
    "\n",
    "- `The Solution`: \n",
    "    - Normalization (Rescaling). We scale every pixel value by dividing it by 255.\n",
    "\n",
    "        $ New Value=Raw Value/255 $\n",
    "\n",
    "- `Result`: \n",
    "    - All pixel inputs now fall into the nice, stable range of 0 to 1. \n",
    "    - This ensures the network trains faster and more stably."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5fd3cf",
   "metadata": {},
   "source": [
    "## Data Augmentation: The Virtual Photographer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad78ddf0",
   "metadata": {},
   "source": [
    "This is the technique we use to fight overfitting by artificially expanding our dataset.\n",
    "\n",
    "- `Analogy`: **The Student vs. The World.** \n",
    "    - Imagine a student memorizes a single picture of a tiger facing left. If you show them a picture of the same tiger facing right, they might fail! Data Augmentation forces the model to learn the essential features of the tiger, regardless of its position, angle, or lighting. We are showing the model that a tiger is a tiger, even if it's mirrored or slightly rotated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cdc70e",
   "metadata": {},
   "source": [
    "| **Technique** | **Description**                                                              | **Effect**                                                                          | **Keras Parameter**                       |\n",
    "| :------------ | :--------------------------------------------------------------------------- | :---------------------------------------------------------------------------------- | :---------------------------------------- |\n",
    "| **Flipping**  | Randomly flips the image horizontally (or vertically, depending on context). | Massive jump in generalization for symmetric objects (e.g., animals).               | `horizontal_flip=True`                    |\n",
    "| **Rotation**  | Rotates the image by a small, random degree (e.g., 10 or 20 degrees).        | Makes the model robust to slight camera tilts.                                      | `rotation_range=20`                       |\n",
    "| **Shifting**  | Shifts the image left, right, up, or down.                                   | Ensures the model learns the object's features even if it's not perfectly centered. | `width_shift_range`, `height_shift_range` |\n",
    "| **Zooming**   | Randomly zooms in or out of the image.                                       | Handles the object being close-up or far away in the frame.                         | `zoom_range`                              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a2a725",
   "metadata": {},
   "source": [
    "##  Noise Injection (Advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81cdf61",
   "metadata": {},
   "source": [
    "For high-level performance, sometimes we add random noise or slightly change the brightness/contrast to simulate real-world conditions (like a poor-quality photo). This further prevents the model from memorizing pixel-level details and forces it to focus on higher-level features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d7272a",
   "metadata": {},
   "source": [
    "- `Analogy`:\n",
    "    - Adding random noise is like making your model wear “foggy glasses” — if it still recognizes the object, it’s truly learned the concept!\n",
    "    - Noise injection improves robustness against lighting or sensor variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d29df06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (3.10.7)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.16.2-cp311-cp311-win_amd64.whl (38.7 MB)\n",
      "     --------------------------------------- 38.7/38.7 MB 46.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from matplotlib) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Installing collected packages: scipy\n",
      "Successfully installed scipy-1.16.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "! pip install matplotlib scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a5174e",
   "metadata": {},
   "source": [
    "## Code Example: Implementing Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c6fe0e",
   "metadata": {},
   "source": [
    "In TensorFlow/Keras, we use the ImageDataGenerator for both normalization and augmentation. I'm telling you, this is a beautiful one-line solution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6af20685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"cnn_fashion_idg\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"cnn_fashion_idg\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">94,858</span> (370.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m94,858\u001b[0m (370.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">94,410</span> (368.79 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m94,410\u001b[0m (368.79 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amey9\\Documents\\GitHub\\DataScience-100-Days-of-Data-Science\\.venv\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 58ms/step - accuracy: 0.7673 - loss: 0.6691 - val_accuracy: 0.1853 - val_loss: 4.9853\n",
      "Epoch 2/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 64ms/step - accuracy: 0.8357 - loss: 0.4583 - val_accuracy: 0.7620 - val_loss: 0.7053\n",
      "Epoch 3/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 63ms/step - accuracy: 0.8530 - loss: 0.4084 - val_accuracy: 0.7612 - val_loss: 0.6692\n",
      "Epoch 4/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 63ms/step - accuracy: 0.8625 - loss: 0.3786 - val_accuracy: 0.8190 - val_loss: 0.5099\n",
      "Epoch 5/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 63ms/step - accuracy: 0.8715 - loss: 0.3571 - val_accuracy: 0.8328 - val_loss: 0.4838\n",
      "Epoch 6/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 63ms/step - accuracy: 0.8721 - loss: 0.3494 - val_accuracy: 0.8400 - val_loss: 0.4494\n",
      "Epoch 7/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 63ms/step - accuracy: 0.8785 - loss: 0.3336 - val_accuracy: 0.8688 - val_loss: 0.3594\n",
      "Epoch 8/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 64ms/step - accuracy: 0.8797 - loss: 0.3294 - val_accuracy: 0.8938 - val_loss: 0.2869\n",
      "Epoch 9/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 64ms/step - accuracy: 0.8838 - loss: 0.3180 - val_accuracy: 0.8347 - val_loss: 0.4641\n",
      "Epoch 10/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 65ms/step - accuracy: 0.8851 - loss: 0.3135 - val_accuracy: 0.8708 - val_loss: 0.3648\n",
      "Test Accuracy: 0.8544 | Test Loss: 0.4112\n",
      "Preds (first 10):  [9 2 1 1 6 1 2 6 5 7]\n",
      "True  (first 10):  [9 2 1 1 6 1 4 6 5 7]\n"
     ]
    }
   ],
   "source": [
    "# Day 68 — Data Augmentation & Preprocessing with ImageDataGenerator (Fashion-MNIST)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1) Load Fashion-MNIST\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# 2) Add channel dim (grayscale -> (H, W, 1))\n",
    "x_train = x_train[..., None].astype(\"float32\")\n",
    "x_test  = x_test[...,  None].astype(\"float32\")\n",
    "\n",
    "# 3) Train/Val split\n",
    "val_frac = 0.1\n",
    "n_val = int(len(x_train) * val_frac)\n",
    "x_val, y_val = x_train[:n_val], y_train[:n_val]\n",
    "x_train, y_train = x_train[n_val:], y_train[n_val:]\n",
    "\n",
    "# 4) ImageDataGenerator — Augment only TRAIN, just rescale for VAL/TEST\n",
    "train_gen = ImageDataGenerator(\n",
    "    rescale=1./255,          # Normalization\n",
    "    horizontal_flip=True,    # Flipping\n",
    "    rotation_range=12,       # Rotation (~±12°)\n",
    "    width_shift_range=0.1,   # Shifting (width)\n",
    "    height_shift_range=0.1,  # Shifting (height)\n",
    "    zoom_range=0.1           # Zooming (in/out)\n",
    "    # you can also try: shear_range=0.05, brightness_range=[0.8, 1.2]\n",
    ")\n",
    "\n",
    "val_gen = ImageDataGenerator(rescale=1./255)\n",
    "test_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# 5) Create generators from numpy arrays\n",
    "train_flow = train_gen.flow(x_train, y_train, batch_size=batch_size, shuffle=True)\n",
    "val_flow   = val_gen.flow(x_val, y_val, batch_size=batch_size, shuffle=False)\n",
    "test_flow  = test_gen.flow(x_test, y_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 6) (Optional) Visualize a few augmented samples\n",
    "bx, by = next(train_flow)\n",
    "plt.figure(figsize=(10,2))\n",
    "for i in range(5):\n",
    "    plt.subplot(1,5,i+1)\n",
    "    plt.imshow(bx[i].squeeze(), cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "plt.suptitle(\"Augmented samples (Fashion-MNIST)\")\n",
    "plt.show()\n",
    "\n",
    "# 7) Build a compact CNN\n",
    "def build_model():\n",
    "    inputs = keras.Input(shape=(28,28,1))\n",
    "    x = layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "\n",
    "    x = layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "\n",
    "    x = layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "    return keras.Model(inputs, outputs, name=\"cnn_fashion_idg\")\n",
    "\n",
    "model = build_model()\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-3),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "# 8) Train (generators yield augmented batches on-the-fly)\n",
    "epochs = 10\n",
    "steps_per_epoch = len(train_flow)\n",
    "validation_steps = len(val_flow)\n",
    "\n",
    "history = model.fit(\n",
    "    train_flow,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_flow,\n",
    "    validation_steps=validation_steps,\n",
    "    epochs=epochs,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 9) Evaluate\n",
    "test_loss, test_acc = model.evaluate(test_flow, verbose=0)\n",
    "print(f\"Test Accuracy: {test_acc:.4f} | Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# 10) Quick predictions preview\n",
    "preds = model.predict(test_flow, verbose=0)\n",
    "pred_labels = preds.argmax(axis=1)\n",
    "print(\"Preds (first 10): \", pred_labels[:10])\n",
    "print(\"True  (first 10): \", y_test[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742ff8e8",
   "metadata": {},
   "source": [
    "## Summary of Day 68"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd31410f",
   "metadata": {},
   "source": [
    "Smashing job today, guys! You've learned the final piece of the puzzle to build robust CNNs:\n",
    "\n",
    "1. Normalization: Scaling pixels from 0-255 to 0-1 for stable training.\n",
    "\n",
    "2. Data Augmentation: Using techniques like Flipping, Rotation, and Zooming to artificially create diverse data, which is the best defense against overfitting on small datasets.\n",
    "\n",
    "3. Keras Tool: The ImageDataGenerator handles all of this automatically for us!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b66b2e0",
   "metadata": {},
   "source": [
    "## What's Next (Day 69)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e82e2a6",
   "metadata": {},
   "source": [
    "We've covered the CNN architecture, the Transfer Learning strategy, and the Data Augmentation method.\n",
    "\n",
    "Tomorrow, on Day 69, we stop talking and start doing! We will launch into our first major, hands-on CNN project using the Fashion-MNIST dataset! You'll write, train, and evaluate a CNN from scratch, applying all the concepts we've learned this week!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
