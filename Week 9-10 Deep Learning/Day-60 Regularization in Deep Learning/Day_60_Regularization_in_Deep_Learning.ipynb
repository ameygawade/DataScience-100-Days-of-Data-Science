{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a45e153",
   "metadata": {},
   "source": [
    "\n",
    "# Day-60: Regularization In Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cfad17",
   "metadata": {},
   "source": [
    "In Deep Learning, Overfitting is the biggest enemy. It happens when the network's capacity is too high, allowing it to memorize the noise and anomalies in the training data, rather than learning the underlying, general patterns. A heavily overfitted model will show near 100% accuracy on the training set but terrible performance on a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ac7a6f",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafe1417",
   "metadata": {},
   "source": [
    "Regularization refers to techniques designed to explicitly reduce the generalization error (i.e., prevent overfitting) without significantly increasing the training error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f5dbfd",
   "metadata": {},
   "source": [
    "- `analogy`:\n",
    "    - Think of your neural network like a student who memorizes answers instead of understanding concepts.\n",
    "They ace the practice tests (training data) but fail real exams (test data).\n",
    "\n",
    "    - Regularization techniques help the student learn the pattern, not the exact answers — that’s generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bf48be",
   "metadata": {},
   "source": [
    "##  Dropout (Preventing Co-Adaptation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cf7ad3",
   "metadata": {},
   "source": [
    "During training only, a certain percentage (e.g., 50%) of the neurons in a hidden layer are randomly shut off (their output is set to zero) for that single forward and backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef284bb",
   "metadata": {},
   "source": [
    "- `Analogy`:\n",
    "    - Imagine you’re training a football team. If you always train with the same players, others never learn.\n",
    "    - So occasionally, you ask some players to sit out. This forces others to adapt.\n",
    "\n",
    "Similarly, Dropout randomly turns off some neurons during training.It prevents co-dependency among neurons and improves generalization.\n",
    "\n",
    "During inference, all neurons are active, but each one’s output is scaled appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1942987d",
   "metadata": {},
   "source": [
    "## L2 Regularization (Weight Decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a892864",
   "metadata": {},
   "source": [
    "This is a classic machine learning technique applied directly to the loss function during backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159955f5",
   "metadata": {},
   "source": [
    "$$ \\text{Total Loss} = \\text{Original Loss} + \\lambda \\sum w^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a44a40",
   "metadata": {},
   "source": [
    "where $\\lambda$ controls how strong the penalty is.\n",
    "\n",
    "This helps prevent the model from relying too heavily on specific neurons — making it more stable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1987cd06",
   "metadata": {},
   "source": [
    "- `Analogy`:\n",
    "    - Imagine you’re packing for a trip. If your bag (weights) is too heavy, it slows you down.\n",
    "    - L2 regularization adds a small penalty for carrying large weights — it encourages the model to keep them small and balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a50597",
   "metadata": {},
   "source": [
    "## Batch Normalization (BN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ed8e88",
   "metadata": {},
   "source": [
    "Batch Normalization is a critical technique not just for regularization, but also for dramatically speeding up training and making it more stable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d31b1c7",
   "metadata": {},
   "source": [
    "The Problem: As weights are constantly updated during training, the inputs to subsequent layers are constantly changing. This is called Internal Covariate Shift, and it forces the later layers to constantly adapt, slowing down learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e44d22",
   "metadata": {},
   "source": [
    "Batch Normalization normalizes the inputs of each layer, ensuring faster and more stable training.\n",
    "It also acts as a mild regularizer, reducing the need for dropout in some architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce22f630",
   "metadata": {},
   "source": [
    "Mechanism: The BN layer is placed before the activation function. For every mini-batch of data, it normalizes the weighted sum (z) such that the output has a mean of 0 and a standard deviation of 1.\n",
    "$$ \\hat{z} = \\frac{z - \\mu_{\\text{batch}}}{\\sigma_{\\text{batch}}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c86f34",
   "metadata": {},
   "source": [
    "- `Analogy`: Consistent Ingredients\n",
    "    - If you're making a complex recipe, you want your ingredients (the inputs to the next layer) to be consistent. \n",
    "    - Batch Normalization ensures that every batch of data entering a layer is standardized, so the layer always sees the data in the same, easy-to-learn distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396385bc",
   "metadata": {},
   "source": [
    "Benefits:\n",
    "\n",
    "- Allows for much higher learning rates.\n",
    "\n",
    "- Acts as a subtle form of regularization, slightly reducing the need for Dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53d4ce9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60c7483b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (6.32.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (2.32.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (4.14.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (1.17.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (1.75.1)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (3.11.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (2.3.3)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.7.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.9)\n",
      "Requirement already satisfied: pillow in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (11.3.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from keras>=3.10.0->tensorflow) (14.1.0)\n",
      "Requirement already satisfied: namex in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3356f50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amey9\\Documents\\GitHub\\DataScience-100-Days-of-Data-Science\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9221 - loss: 0.8528 - val_accuracy: 0.9591 - val_loss: 0.5595\n",
      "Epoch 2/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9564 - loss: 0.4758 - val_accuracy: 0.9701 - val_loss: 0.3692\n",
      "Epoch 3/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9621 - loss: 0.3504 - val_accuracy: 0.9705 - val_loss: 0.2997\n",
      "Epoch 4/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9642 - loss: 0.2955 - val_accuracy: 0.9708 - val_loss: 0.2680\n",
      "Epoch 5/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9650 - loss: 0.2748 - val_accuracy: 0.9683 - val_loss: 0.2500\n",
      "Epoch 6/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9657 - loss: 0.2621 - val_accuracy: 0.9640 - val_loss: 0.2630\n",
      "Epoch 7/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9669 - loss: 0.2560 - val_accuracy: 0.9719 - val_loss: 0.2316\n",
      "Epoch 8/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9660 - loss: 0.2513 - val_accuracy: 0.9735 - val_loss: 0.2294\n",
      "Epoch 9/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9662 - loss: 0.2464 - val_accuracy: 0.9742 - val_loss: 0.2207\n",
      "Epoch 10/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9666 - loss: 0.2415 - val_accuracy: 0.9658 - val_loss: 0.2407\n"
     ]
    }
   ],
   "source": [
    "# Day 60: Regularization in Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load Data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train.reshape(-1, 784) / 255.0, x_test.reshape(-1, 784) / 255.0\n",
    "y_train, y_test = to_categorical(y_train), to_categorical(y_test)\n",
    "\n",
    "\n",
    "# Build Model\n",
    "model = Sequential([\n",
    "    Dense(512, activation='relu', kernel_regularizer=l2(0.001), input_shape=(784,)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=128,\n",
    "                    validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5b8de1",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "- l2(0.001) → adds weight decay penalty\n",
    "\n",
    "- Dropout(0.3) → randomly drops 30% neurons\n",
    "\n",
    "- BatchNormalization() → stabilizes learning and speeds up convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9684b4e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
