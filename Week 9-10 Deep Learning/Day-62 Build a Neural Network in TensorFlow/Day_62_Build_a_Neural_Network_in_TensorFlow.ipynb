{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ff69621",
   "metadata": {},
   "source": [
    "# Day-62: Build a Neural Network in TensorFlow/Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cfe10e",
   "metadata": {},
   "source": [
    "Yesterday we built a simple Feedforward Neural Network using Keras.\n",
    "\n",
    "Today, we’ll take it a step further — we’ll learn how to build, train, validate, and evaluate a complete Neural Network end-to-end using TensorFlow/Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16654283",
   "metadata": {},
   "source": [
    "A successful Deep Learning pipeline involves more than just stacking layers. It requires a disciplined training process to ensure the model generalizes well to new data. This is achieved by strategically splitting the data and monitoring key metrics throughout the training epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826e2866",
   "metadata": {},
   "source": [
    "## The Five Stages of a Deep Learning Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da0c67d",
   "metadata": {},
   "source": [
    "### 1. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669961ad",
   "metadata": {},
   "source": [
    "This is the foundation! Garbage in, garbage out, guys! The neural network is very sensitive to the quality and scale of the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceef03c",
   "metadata": {},
   "source": [
    "- **Cleaning**: Removing noise, handling missing values, and aggressive stopword filtering (like we did in our NLP project!).\n",
    "\n",
    "- **Feature Engineering** : Sometimes we create new features (e.g., combining age and income into a risk score).\n",
    "\n",
    "- **Scaling and Normalization** : This is crucial. We use StandardScaler to ensure all numerical features have a mean of 0 and standard deviation of 1. This prevents large-value features (like high salaries) from overpowering features with small values (like age) in the weight updates during training.\n",
    "\n",
    "- **Splitting** : Separating the data into the three sacred sets: Training, Validation, and Test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0636a0d",
   "metadata": {},
   "source": [
    "### 2. Model Building (Sequential API)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c069ac6",
   "metadata": {},
   "source": [
    "This is where we define the blueprint of our neural network, usually using Keras because of its simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9d732e",
   "metadata": {},
   "source": [
    "- **Architecture** : We use the ** Sequential API ** to stack layers linearly, like building blocks. We start with the input layer (which takes our features) and end with the output layer (which gives the prediction).\n",
    "\n",
    "- **Layers** : We primarily use Dense layers for an $FFNN$. We decide on the number of neurons in each hidden layer.\n",
    "\n",
    "- **Activation Functions** : We define the non-linearity: typically $ReLU$ for all hidden layers and $Sigmoid$ (for binary classification) or $Softmax$ (for multi-class classification) for the final output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c85e07",
   "metadata": {},
   "source": [
    "### 3. Training & Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a12338",
   "metadata": {},
   "source": [
    "This is the engine room! This is where you call `model.fit()`, and the **Forward** and **Backpropagation** algorithms run for multiple **epochs**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d29c54d",
   "metadata": {},
   "source": [
    "- **Training**: The model adjusts its internal weights $(w)$ based on the Training Set to minimize the **Loss Function** (e.g., `binary_crossentropy`).\n",
    "\n",
    "- **Validation (Your Selection!)**:\n",
    "    - The biggest challenge in training is Overfitting—when the model memorizes the training data but fails on unseen data. We solve this by introducing the Validation Set.\n",
    "    - The model is monitored on the Validation Set after every epoch. We check the **Validation Loss**. If the validation loss starts to increase while the training loss still decreases, it means the model is memorizing the training data. This is when we deploy Early Stopping to halt the process at the model's \"sweet spot.\"\n",
    "\n",
    "    - `Analogy`:\n",
    "        - **Training Set**: The student studies the textbook. The model adjusts its weights based on this data.\n",
    "        - **Validation Set**: The Strict Auditor who gives surprise quizzes every few hours. The model is never allowed to directly learn from this data. It is only used to monitor the model's performance on unseen examples during training.\n",
    "    \n",
    "    1. **The Role of the Validation Set**\n",
    "        -  **Monitoring Overfitting**:\n",
    "            - We monitor the Loss Curve for both the training set and the validation set.\n",
    "                1. **Ideal** : Both training loss and validation loss decrease together.\n",
    "                2. **Overfitting Detection**: If the Training Loss continues to decrease, but the Validation Loss starts to increase, the model is starting to memorize the training noise. This is the time to stop!\n",
    "    \n",
    "    2. **Early Stopping**\n",
    "        - Since we can detect the onset of **Overfitting** using the **Validation Set**, we can implement a callback function called **Early Stopping**.\n",
    "        - **Definition**: A regularization technique that stops the training process if the validation loss fails to improve for a pre-defined number of epochs (called `patience`).\n",
    "        - **Why it's a Big Deal**: It saves computational time and prevents the model from degrading its performance on generalized data. It ensures we capture the model at its \"sweet spot\" of performance.\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae2fea2",
   "metadata": {},
   "source": [
    "### 4. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11904c1f",
   "metadata": {},
   "source": [
    "Once training is complete, the weights are frozen. Now, we perform the final, unbiased audit.\n",
    "\n",
    "- **The Test Set**: We use the completely untouched Test Set—data the model has never seen—to run `model.evaluate()`.\n",
    "    - **Final Evaluation (The Test Set)**\n",
    "        - Once training is complete, the model's performance must be measured on the completely untouched, final **Test Set**.\n",
    "        - **The Final Exam**: \n",
    "            - The Test Set is the ultimate, final exam for the model. It gives us the unbiased measure of how the model will perform in the real world.\n",
    "        - **Function**: \n",
    "            - We use `model.evaluate(X_test, y_test)` in Keras to calculate the final loss and metrics (like accuracy). This value is what you should report to your stakeholders!\n",
    "\n",
    "- **Metrics**: We calculate the final, true performance metrics (like Accuracy, Precision, Recall, and the F1 score). This is the number you report to your team or client because it represents the model's real-world generalization capability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc741ab",
   "metadata": {},
   "source": [
    "### 5. Model Saving & Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbbef6d",
   "metadata": {},
   "source": [
    "All that hard work can't disappear! A model isn't production-ready until it can be saved and deployed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ceefcf",
   "metadata": {},
   "source": [
    "- **Saving**: We use `model.save('my_production_model.h5')` to serialize the entire state of the network, including the architecture, the trained weights, and the optimizer settings.\n",
    "\n",
    "- **Loading**: Later, a deployed API can use `tf.keras.models.load_model('my_production_model.h5')` to load the model instantly and start generating predictions in real-time, without having to retrain. This is a big deal for efficiency!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e45ad96",
   "metadata": {},
   "source": [
    "### 1. Data Loading & Preparation\n",
    "- We use the Breast Cancer Wisconsin dataset from `sklearn.datasets`.\n",
    "- We split the data into training (70%), validation (15%), and test (15%) sets using `train_test_split`.\n",
    "- We standardize the features using `StandardScaler` to ensure all features contribute equally to the learning process.\n",
    "\n",
    "### 2. Model Building (Sequential API)\n",
    "This is where we define the blueprint of our neural network, usually using Keras because of its simplicity. Keras (which runs on top of TensorFlow) is the high-level API that makes building this\n",
    "architecture incredibly simple. We use the Sequential model, which allows us to stack layers one after the other, like building blocks. --- IGNORE ---\n",
    "- **Architecture** : We use the ** Sequential API ** to stack layers linearly, like building blocks. We start with the input layer (which takes our features) and end with the output layer (which gives the prediction).\n",
    "- **Layers** : We primarily use Dense layers for an $FFNN$. We decide on the number of neurons in each hidden layer.\n",
    "- **Activation Functions** : We define the non-linearity: typically $ReLU$ for all hidden layers and $Sigmoid$ (for binary classification) or $Softmax$ (for multi-class classification) for the final output layer.\n",
    "### 3. Training & Validation\n",
    "This is the engine room! This is where you call `model.fit()`, and the **Forward** and **Backpropagation** algorithms run for multiple **epochs**.\n",
    "- We use the Adam optimizer, which adapts the learning rate during training for faster convergence.\n",
    "- We use binary cross-entropy loss since this is a binary classification problem.\n",
    "- We monitor performance on the validation set to tune hyperparameters and prevent overfitting.\n",
    "- We can visualize training and validation accuracy/loss over epochs to check for overfitting or underfitting.\n",
    "- **Regularization** : We can add techniques like Dropout or L2 regularization to prevent overfitting.\n",
    "### 4. Model Evaluation\n",
    "- After training, we evaluate the model on the unseen test set using `model.evaluate()`. This gives us an unbiased estimate of how well our model generalizes to new data.\n",
    "- We print the test accuracy to see how well our model performs.\n",
    "### 5. Model Saving & Loading\n",
    "- **Saving**: We use `model.save('my_production_model.h5')` to serialize the entire state of the network, including the architecture, the trained weights, and the optimizer settings.\n",
    "- **Loading**: Later, a deployed API can use `tf.keras.models.load_model('my_production_model.h5')` to load the model instantly and start generating predictions in real-time, without having to retrain. This is a big deal for efficiency!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfbe2c1",
   "metadata": {},
   "source": [
    "## Code Example: Binary classification on the Breast Cancer Wisconsin dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde6a09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amey9\\Documents\\GitHub\\DataScience-100-Days-of-Data-Science\\.venv\\Lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays_v1.py:37: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 2.3.3)\n",
      "  from scipy.sparse import issparse  # pylint: disable=g-import-not-at-top\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amey9\\Documents\\GitHub\\DataScience-100-Days-of-Data-Science\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.3618 - loss: 0.8692 - val_accuracy: 0.4235 - val_loss: 0.7176\n",
      "Epoch 2/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4899 - loss: 0.6867 - val_accuracy: 0.6941 - val_loss: 0.5957\n",
      "Epoch 3/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7638 - loss: 0.5825 - val_accuracy: 0.8000 - val_loss: 0.5231\n",
      "Epoch 4/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8442 - loss: 0.5178 - val_accuracy: 0.8706 - val_loss: 0.4684\n",
      "Epoch 5/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9020 - loss: 0.4633 - val_accuracy: 0.8824 - val_loss: 0.4208\n",
      "Epoch 6/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9171 - loss: 0.4112 - val_accuracy: 0.9176 - val_loss: 0.3759\n",
      "Epoch 7/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9296 - loss: 0.3630 - val_accuracy: 0.9176 - val_loss: 0.3338\n",
      "Epoch 8/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9397 - loss: 0.3195 - val_accuracy: 0.9294 - val_loss: 0.2954\n",
      "Epoch 9/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9548 - loss: 0.2793 - val_accuracy: 0.9294 - val_loss: 0.2628\n",
      "Epoch 10/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9548 - loss: 0.2462 - val_accuracy: 0.9294 - val_loss: 0.2340\n",
      "Epoch 11/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9548 - loss: 0.2178 - val_accuracy: 0.9412 - val_loss: 0.2089\n",
      "Epoch 12/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9573 - loss: 0.1944 - val_accuracy: 0.9529 - val_loss: 0.1880\n",
      "Epoch 13/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9623 - loss: 0.1754 - val_accuracy: 0.9529 - val_loss: 0.1702\n",
      "Epoch 14/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9623 - loss: 0.1595 - val_accuracy: 0.9529 - val_loss: 0.1554\n",
      "Epoch 15/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9623 - loss: 0.1461 - val_accuracy: 0.9529 - val_loss: 0.1433\n",
      "Epoch 16/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9648 - loss: 0.1349 - val_accuracy: 0.9529 - val_loss: 0.1332\n",
      "Epoch 17/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9724 - loss: 0.1250 - val_accuracy: 0.9529 - val_loss: 0.1248\n",
      "Epoch 18/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9749 - loss: 0.1160 - val_accuracy: 0.9647 - val_loss: 0.1179\n",
      "Epoch 19/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9799 - loss: 0.1086 - val_accuracy: 0.9765 - val_loss: 0.1115\n",
      "Epoch 20/20\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9849 - loss: 0.1019 - val_accuracy: 0.9765 - val_loss: 0.1064\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0842 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔁 Model reloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Day 62 - End-to-End Neural Network in TensorFlow/Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Load and prepare data\n",
    "# project: Binary classification on the Breast Cancer Wisconsin dataset\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Standardize features. Fit on training, transform on all.\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into training, validation, and test sets (70% train, 15% val, 15% test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# 2. Build model\n",
    "model = Sequential([\n",
    "    Dense(16, activation='relu', input_shape=(X.shape[1],)),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# 3. Compile model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 4. Train model with validation\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_val, y_val), batch_size=32)\n",
    "\n",
    "# 5. Evaluate model on test set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"✅ Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# 6. Save model\n",
    "model.save(\"breast_cancer_nn_model.h5\")\n",
    "\n",
    "# Load model later\n",
    "loaded_model = tf.keras.models.load_model(\"breast_cancer_nn_model.h5\")\n",
    "print(\"🔁 Model reloaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf479aca",
   "metadata": {},
   "source": [
    "## Summary of Day 62"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56f031a",
   "metadata": {},
   "source": [
    "\n",
    "- Today, we leveled up from simply building a model to running a professional, end-to-end Deep Learning pipeline.\n",
    "\n",
    "- You learned that the secret to generalization is monitoring:\n",
    "\n",
    "    - **Validation Set**: Used to monitor performance during training.\n",
    "\n",
    "    - **Loss Curves**: Training loss must decrease, but if Validation Loss starts to increase, it’s time to stop (Overfitting).\n",
    "\n",
    "    - **Early Stopping**: The technique used to automatically stop training at the optimal point.\n",
    "\n",
    "    - **Test Set**: The final, completely unbiased metric used to report the model's real-world accuracy.\n",
    "\n",
    "We are now officially ready to tackle a full Deep Learning project!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33c1d3c",
   "metadata": {},
   "source": [
    "## What's Next (Day 63)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef0c3c9",
   "metadata": {},
   "source": [
    "\n",
    "I'm telling you, it’s time for the capstone! Tomorrow, on Day 63, we start our Deep Learning Project where you apply everything from Day 57 through Day 62.\n",
    "\n",
    "You have three classic, amazing project options:\n",
    "\n",
    "- Predict Housing Prices: A classic Regression problem.\n",
    "\n",
    "- Classify Digits (MNIST): A foundational Multi-Class Classification project using images.\n",
    "\n",
    "- Detect Fraud: A critical Binary Classification project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
