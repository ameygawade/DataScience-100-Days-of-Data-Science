{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cc30e18",
   "metadata": {},
   "source": [
    "# Day-19: Model Interpretability tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6faa9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e5e227e",
   "metadata": {},
   "source": [
    "## Topics Covered:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8717ce5a",
   "metadata": {},
   "source": [
    "- What is Model Interpretability\n",
    "- How it works\n",
    "- Benefits of Model Interpretability\n",
    "- Types of Model Interpretability\n",
    "- When and Where to Use These Tools\n",
    "- Comparison & Visualization\n",
    "- Example+ code snippet\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dec5c93",
   "metadata": {},
   "source": [
    "## What is Model Interpretability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e067791c",
   "metadata": {},
   "source": [
    "- Model interpretability is the degree to which a human can understand the reasons behind a machine learning model's predictions. \n",
    "- Instead of simply getting a result, interpretability allows us to understand the \"why\" behind a model's decision. \n",
    "- It's about opening up the \"black box\" of complex models to gain insights into how they work.Especially when using black-box models like Random Forests, XGBoost, or Neural Networks, interpretation tools are critical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8043101",
   "metadata": {},
   "source": [
    "- Intrinsic Interpretability: \n",
    "    - Some models, like Decision Trees or Linear Regression, are inherently transparent because their structure directly explains the prediction. \n",
    "    - For instance, in linear regression, the coefficient for each feature shows its impact on the target variable.\n",
    "\n",
    "- Post-hoc Interpretability: \n",
    "    - This applies to models that are not easily interpretable, such as Random Forests or Neural Networks. \n",
    "    - Post-hoc methods are applied after the model has been trained to provide explanations for its decisions. \n",
    "    - The tools we've discussed (SHAP, LIME, Permutation Importance) are all post-hoc methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febc0280",
   "metadata": {},
   "source": [
    "## How it Works?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bf92be",
   "metadata": {},
   "source": [
    "Model interpretability tools analyze a trained model to reveal which features are most influential and how they affect the model's output. They do this by either looking at the model's internal structure (for simple models) or by probing the model's behavior with new data points (for complex models)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ec7d7",
   "metadata": {},
   "source": [
    "Intrinsic Interpretability: Some models, like Decision Trees or Linear Regression, are inherently transparent because their structure directly explains the prediction. For instance, in linear regression, the coefficient for each feature shows its impact on the target variable.\n",
    "\n",
    "Post-hoc Interpretability: This applies to models that are not easily interpretable, such as Random Forests or Neural Networks. Post-hoc methods are applied after the model has been trained to provide explanations for its decisions. The tools we've discussed (SHAP, LIME, Permutation Importance) are all post-hoc methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb570c4",
   "metadata": {},
   "source": [
    "## Benefits of Model Interpretability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905ef0fd",
   "metadata": {},
   "source": [
    "- Trust & Adoption: \n",
    "    - By explaining a model's behavior, we build trust with stakeholders, customers, and regulators. \n",
    "    - This is crucial for models used in high-stakes fields like finance and medicine.\n",
    "\n",
    "- Debugging & Auditing: \n",
    "    - Interpretability helps us debug models by identifying hidden biases or errors.\n",
    "    - If a model makes a bad prediction, we can use these tools to understand which features led to that error and fix the underlying issue in the data or model.\n",
    "\n",
    "- Scientific Discovery: \n",
    "    - In research, a model's insights can lead to new scientific discoveries. \n",
    "    - For example, a model's feature importance could highlight a gene or a protein that was previously unknown to be related to a disease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67723b8",
   "metadata": {},
   "source": [
    "## Types of Model Interpretablity ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0fdec4",
   "metadata": {},
   "source": [
    "Model interpretability can be categorized based on the scope of the explanation.\n",
    "\n",
    "    - Global Interpretability: \n",
    "        - This provides an understanding of the model's overall behavior. It tells us which features are most important on average for the model's predictions across the entire dataset. Permutation Importance and SHAP's summary plots are examples of global interpretability.\n",
    "\n",
    "    - Local Interpretability: \n",
    "        - This focuses on explaining a single, specific prediction. It helps us answer questions like, \"Why did the model classify this specific person as high-risk?\" LIME and SHAP's force plots are prime examples of local interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f79119",
   "metadata": {},
   "source": [
    "## Permutation Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf945961",
   "metadata": {},
   "source": [
    "Permutation Importance is a model-agnostic and global method for calculating feature importance. It works by measuring the decrease in a model's performance when a single feature's values are randomly shuffled (permuted). If a feature is important, shuffling its values will significantly reduce the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae27703",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "result = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42)\n",
    "importances = result.importances_mean\n",
    "\n",
    "# Visualize\n",
    "plt.barh(X_test.columns, importances)\n",
    "plt.title(\"Permutation Feature Importance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1abbe8f",
   "metadata": {},
   "source": [
    "- How it works:\n",
    "\n",
    "1. Train a model on your data and record a baseline performance score on a validation set (e.g., accuracy, R-squared).\n",
    "\n",
    "2. For a single feature, randomly shuffle its values in the validation set.\n",
    "\n",
    "3. Re-run the model's predictions on the shuffled data and calculate the performance score again.\n",
    "\n",
    "4. The difference between the baseline score and the new score indicates the importance of that feature. A larger drop means the feature is more important.\n",
    "\n",
    "5. Repeat this process for all features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a8e510",
   "metadata": {},
   "source": [
    "## LIME (Local Interpretable Model-Agnostic Explanations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1da12ef",
   "metadata": {},
   "source": [
    "LIME is a method that explains an individual prediction by approximating a complex model locally with a simple, interpretable one. It focuses on a single instance and creates a new, small dataset around it. This is particularly useful for explaining \"black box\" models, whereas we could easily understand the logic of an inherently interpretable model like a Decision Tree (Day 13) just by looking at its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44f4c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "explainer = LimeTabularExplainer(training_data=np.array(X_train),\n",
    "                                 feature_names=X_train.columns,\n",
    "                                 class_names=['No Churn', 'Churn'],\n",
    "                                 mode='classification')\n",
    "\n",
    "# Choose a row to explain\n",
    "exp = explainer.explain_instance(X_test.iloc[1].values, model.predict_proba, num_features=5)\n",
    "exp.show_in_notebook()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c1b8d0",
   "metadata": {},
   "source": [
    "- How it works:\n",
    "\n",
    "1. Select a single data point you want to explain.\n",
    "\n",
    "2. Perturb this data point multiple times to create a new, local dataset.\n",
    "\n",
    "3. Use the complex model to make predictions on this new dataset.\n",
    "\n",
    "4. Train a simple, interpretable model (like linear regression or a decision tree) on this local dataset, with the complex model's predictions as the target.\n",
    "\n",
    "5. The simple model's coefficients or rules then serve as the explanation for the single prediction of the complex model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49fe866",
   "metadata": {},
   "source": [
    "- LIME Advantages:\n",
    "\n",
    "    - Model-Agnostic: \n",
    "        - Like Permutation Importance, it can be applied to any machine learning model.\n",
    "\n",
    "    - Focus on Local Explanations: \n",
    "        - It's designed specifically to answer \"Why did the model make this decision for this particular instance?\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77cee1b",
   "metadata": {},
   "source": [
    "## SHAP (SHapley Additive exPlanations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edfe94b",
   "metadata": {},
   "source": [
    "SHAP is a unified framework for explaining any machine learning model's output. It's based on Shapley values from cooperative game theory, which fairly distributes the \"payout\" (the model's prediction) among the \"players\" (the features). SHAP provides both local and global explanations. It gives us a consistent and detailed way to understand feature importance for models, unlike the built-in feature importance we saw with Lasso regression on Day 18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1142535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import xgboost\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Train a model\n",
    "model = xgboost.XGBClassifier().fit(X_train, y_train)\n",
    "\n",
    "# SHAP values\n",
    "explainer = shap.Explainer(model, X_train)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Plot\n",
    "shap.plots.beeswarm(shap_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d997a23c",
   "metadata": {},
   "source": [
    "- How it works (Simplified): \n",
    "1. For a single prediction, SHAP calculates the contribution of each feature to the final prediction by comparing it to the average prediction for the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b71700",
   "metadata": {},
   "source": [
    "- SHAP Advantages:\n",
    "\n",
    "    - Consistent: \n",
    "        - SHAP values are mathematically guaranteed to be consistent, meaning if a feature's contribution increases, its SHAP value will also increase.\n",
    "\n",
    "    - Local & Global: \n",
    "        - You can use SHAP to explain a single prediction (local) or to summarize feature importance across the entire dataset (global)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27314add",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m venv env\n",
    "! .\\env\\Scripts\\activate\n",
    "! python -m pip install --upgrade pip\n",
    "\n",
    "# Install your packages\n",
    "! pip install numba numpy shap scikit-learn pandas matplotlib seaborn\n",
    "\n",
    "# Test it\n",
    "! python\n",
    "import shap\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load dataset\n",
    "cancer = load_breast_cancer()\n",
    "X = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "y = cancer.target\n",
    "\n",
    "# Split data and train a model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Create a SHAP explainer for the trained model\n",
    "explainer = shap.TreeExplainer(model)\n",
    "# Get SHAP values for a single instance (the first one in our test set)\n",
    "shap_values = explainer.shap_values(X_test.iloc[0, :])\n",
    "\n",
    "# Visualize the local explanation\n",
    "print(\"SHAP Local Explanation for a single test instance:\")\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1], X_test.iloc[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5171141c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python.exe -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93018e29",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
