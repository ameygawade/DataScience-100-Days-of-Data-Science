{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d951f73f",
   "metadata": {},
   "source": [
    "# Day-18: Feature Selection Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0d8e2a",
   "metadata": {},
   "source": [
    " In today's session, we're diving into a crucial part of the machine learning pipeline: **feature selection**. It's all about picking the most relevant features to improve your model's performance and efficiency. Let's get started! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e3ad1b",
   "metadata": {},
   "source": [
    "\n",
    "## Topics Covered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3a37be",
   "metadata": {},
   "source": [
    "- What is Feature Selection?\n",
    "- When to Use It?\n",
    "- Relation with Feature Engineering\n",
    "- Types of Feature Selection Techniques\n",
    "- Usecase + Code Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee52a7e",
   "metadata": {},
   "source": [
    "## What is Feature Selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f28f7f",
   "metadata": {},
   "source": [
    "Feature selection is the process of choosing a subset of **relevant features** (variables, predictors) from a larger set to use in model construction. Think of it like this: if you're baking a cake, you don't use every single ingredient in your pantry; you only pick the ones that will make the cake taste good. Similarly, feature selection helps us find the \"ingredients\" that will make our model perform best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6d2eca",
   "metadata": {},
   "source": [
    "Just remeber that “Just because a feature is in the dataset doesn’t mean it’s helpful.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00b4b10",
   "metadata": {},
   "source": [
    "## When to Use It?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6a9359",
   "metadata": {},
   "source": [
    "\n",
    "- **High-dimensional data:** When you have a massive number of features (e.g., thousands of genes in a bioinformatics dataset).\n",
    "- **Model interpretability:** Fewer features make the model easier to understand and explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2992bac0",
   "metadata": {},
   "source": [
    "## Relation with Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88fb66b",
   "metadata": {},
   "source": [
    "\n",
    "- **Feature Engineering** is about creating **new features** from existing ones. This is a creative process where you might use domain knowledge to extract more information.\n",
    "- **Feature Selection** is about **choosing the best features** from a given set (which might include features you've just engineered). They often go hand-in-hand: you engineer new features and then select the best ones from the combined set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b346212a",
   "metadata": {},
   "source": [
    "## Types of feature selection Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535eeb04",
   "metadata": {},
   "source": [
    "### **Filter Methods:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5f6f13",
   "metadata": {},
   "source": [
    "These methods select features based on their **statistical score** relative to the target variable, **independent of the machine learning algorithm**. They act like a \"pre-filter\" for your data. Examples include **Variance Threshold** and correlation-based methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ed468f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Select top 3 features with highest ANOVA F-value\u001b[39;00m\n\u001b[32m      4\u001b[39m selector = SelectKBest(score_func=f_classif, k=\u001b[32m3\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m X_new = selector.fit_transform(\u001b[43mX\u001b[49m, y)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(X.columns[selector.get_support()])\n",
      "\u001b[31mNameError\u001b[39m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Select top 3 features with highest ANOVA F-value\n",
    "selector = SelectKBest(score_func=f_classif, k=3)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "print(X.columns[selector.get_support()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fe6231",
   "metadata": {},
   "source": [
    "### **Wrapper Methods:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65c09d2",
   "metadata": {},
   "source": [
    "\n",
    "These methods \"wrap\" around a specific machine learning algorithm. They use a **search strategy** to find the best subset of features by repeatedly training and evaluating the model. **Recursive Feature Elimination (RFE)** is a classic example. It's computationally expensive but often yields better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5f5e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model, n_features_to_select=3)\n",
    "X_selected = rfe.fit_transform(X, y)\n",
    "print(X.columns[rfe.support_])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccfe73c",
   "metadata": {},
   "source": [
    "### **Embedded Methods:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a70e794",
   "metadata": {},
   "source": [
    "\n",
    " These methods perform feature selection as an **integral part of the model training process**. The algorithm itself has a built-in mechanism to select the most important features. **Lasso (L1 regularization)** is a prime example, as it can shrink some feature coefficients to exactly zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6062498a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinear_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LassoCV\n\u001b[32m      3\u001b[39m lasso = LassoCV()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m lasso.fit(\u001b[43mX\u001b[49m, y)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Keep features where coefficient is non-zero\u001b[39;00m\n\u001b[32m      7\u001b[39m selected_features = X.columns[lasso.coef_ != \u001b[32m0\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "lasso = LassoCV()\n",
    "lasso.fit(X, y)\n",
    "\n",
    "# Keep features where coefficient is non-zero\n",
    "selected_features = X.columns[lasso.coef_ != 0]\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5248eea4",
   "metadata": {},
   "source": [
    "### Usecase + Code Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a050c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Variance Threshold ---\n",
      "Features selected by Variance Threshold: ['sepal length (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "------------------------------\n",
      "--- Recursive Feature Elimination (RFE) ---\n",
      "Features selected by RFE: ['sepal width (cm)', 'petal width (cm)']\n",
      "Accuracy with RFE-selected features: 0.91\n",
      "------------------------------\n",
      "--- Embedded Method (Lasso) ---\n",
      "Features selected by Lasso: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Accuracy with Lasso-selected features: 1.00\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amey9\\Documents\\GitHub\\DataScience-100-Days-of-Data-Science\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\amey9\\Documents\\GitHub\\DataScience-100-Days-of-Data-Science\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\amey9\\Documents\\GitHub\\DataScience-100-Days-of-Data-Science\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\amey9\\Documents\\GitHub\\DataScience-100-Days-of-Data-Science\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\amey9\\Documents\\GitHub\\DataScience-100-Days-of-Data-Science\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold, RFE\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = iris.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# --- 1. Filter Method: Variance Threshold ---\n",
    "# This method removes features with low variance.\n",
    "# Low variance means a feature's value is nearly constant, thus not very informative.\n",
    "print(\"--- Variance Threshold ---\")\n",
    "selector_vt = VarianceThreshold(threshold=0.5)\n",
    "X_train_vt = selector_vt.fit_transform(X_train)\n",
    "# Let's see which features were kept\n",
    "features_vt = X.columns[selector_vt.get_support()]\n",
    "print(f\"Features selected by Variance Threshold: {list(features_vt)}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- 2. Wrapper Method: Recursive Feature Elimination (RFE) ---\n",
    "# RFE works by recursively considering smaller and smaller sets of features.\n",
    "# It starts with all features and removes the least important ones until the specified number of features is reached.\n",
    "print(\"--- Recursive Feature Elimination (RFE) ---\")\n",
    "model_rfe = LogisticRegression(solver='liblinear')\n",
    "# Let's select the top 2 features\n",
    "selector_rfe = RFE(model_rfe, n_features_to_select=2, step=1)\n",
    "selector_rfe.fit(X_train, y_train)\n",
    "X_train_rfe = selector_rfe.transform(X_train)\n",
    "X_test_rfe = selector_rfe.transform(X_test)\n",
    "features_rfe = X.columns[selector_rfe.get_support()]\n",
    "print(f\"Features selected by RFE: {list(features_rfe)}\")\n",
    "\n",
    "# Train a model on the selected features\n",
    "model_rfe.fit(X_train_rfe, y_train)\n",
    "predictions_rfe = model_rfe.predict(X_test_rfe)\n",
    "accuracy_rfe = accuracy_score(y_test, predictions_rfe)\n",
    "print(f\"Accuracy with RFE-selected features: {accuracy_rfe:.2f}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- 3. Embedded Method: Lasso (L1 regularization) ---\n",
    "# Lasso adds a penalty term that can force some coefficients to become zero.\n",
    "# Features with a non-zero coefficient are considered important.\n",
    "print(\"--- Embedded Method (Lasso) ---\")\n",
    "# Lasso is for regression, but we can use Logistic Regression with L1 penalty for classification\n",
    "model_lasso = LogisticRegression(penalty='l1', solver='liblinear', random_state=42)\n",
    "model_lasso.fit(X_train, y_train)\n",
    "\n",
    "# Get the coefficients\n",
    "coefficients = model_lasso.coef_\n",
    "# Features with non-zero coefficients are the important ones\n",
    "features_lasso = X.columns[np.any(coefficients != 0, axis=0)]\n",
    "print(f\"Features selected by Lasso: {list(features_lasso)}\")\n",
    "\n",
    "# Train a model on the selected features\n",
    "predictions_lasso = model_lasso.predict(X_test)\n",
    "accuracy_lasso = accuracy_score(y_test, predictions_lasso)\n",
    "print(f\"Accuracy with Lasso-selected features: {accuracy_lasso:.2f}\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d553aa70",
   "metadata": {},
   "source": [
    "\n",
    "### Interpreting the Code Example Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239ac357",
   "metadata": {},
   "source": [
    "#### Variance Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57d3dd4",
   "metadata": {},
   "source": [
    "##### Output:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f33e9e",
   "metadata": {},
   "source": [
    "    Features selected by Variance Threshold: ['sepal length (cm)', 'petal length (cm)', 'petal width (cm)']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d96a441",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "\n",
    "- The ***Variance Threshold*** method has evaluated the statistical variance of each feature.\n",
    "\n",
    "- It determined that the feature ***'sepal width (cm)'*** has a ***low variance***, meaning its values are very similar across the dataset. Because of this, it was filtered out.\n",
    "\n",
    "- The remaining features ***('sepal length (cm)', 'petal length (cm)', and 'petal width (cm)')*** were kept because their variance exceeded the threshold we set (0.5 in our code). This suggests they have more variability and are potentially more informative for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d630a864",
   "metadata": {},
   "source": [
    "#### Wrapper Method: Recursive Feature Elimination (RFE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d49c35",
   "metadata": {},
   "source": [
    "##### Output:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d72c18",
   "metadata": {},
   "source": [
    "    Features selected by RFE: ['sepal width (cm)', 'petal width (cm)']\n",
    "    Accuracy with RFE-selected features: 0.91"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd273ac",
   "metadata": {},
   "source": [
    "##### Interpretation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e02a2c6",
   "metadata": {},
   "source": [
    "- **RFE** works by repeatedly training a model (in our case, **LogisticRegression**) and removing the least important feature until it reaches a desired number of features (2 in our code example).\n",
    "\n",
    "- The selected features, **'sepal width (cm)' and 'petal width (cm)'**, are the top two most important features according to the logistic regression model's internal importance ranking.\n",
    "\n",
    "- The **accuracy score** of **0.91** tells us that a model trained using only these two features performs very well, achieving 91% accuracy on the test set. This demonstrates the power of RFE in identifying a small, powerful subset of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f45d3c",
   "metadata": {},
   "source": [
    "\n",
    "#### Embedded Method: Lasso (L1 regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2d0086",
   "metadata": {},
   "source": [
    "##### Variance Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14250e67",
   "metadata": {},
   "source": [
    "##### Output:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f03a21",
   "metadata": {},
   "source": [
    "    Features selected by Lasso: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
    "    Accuracy with Lasso-selected features: 1.00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5c609a",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "\n",
    "- **Lasso (L1 regularization)** is a technique that penalizes large coefficients, effectively pushing the coefficients of less important features to zero.\n",
    "\n",
    "- In this specific run, none of the coefficients were pushed to zero. This means the Lasso model considered all four features to be important and necessary for the classification task.\n",
    "\n",
    "- The model trained with all features selected by Lasso achieved a perfect **1.00 accuracy** on the test set. This is a great result, but it’s important to remember that a perfect score on a simple dataset like Iris can sometimes indicate overfitting. In a real-world scenario, we'd want to be cautious with a perfect score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc70e2f",
   "metadata": {},
   "source": [
    "#### Summary of Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17586391",
   "metadata": {},
   "source": [
    "- The Variance Threshold method was the most aggressive, removing one feature.\n",
    "\n",
    "- RFE showed that we can achieve high accuracy (91%) using only two features, proving that not all features are equally important.\n",
    "\n",
    "- Lasso found all features to be relevant, leading to a perfect accuracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5435de22",
   "metadata": {},
   "source": [
    "By comparing these results, we can see how different methods yield different insights. RFE provides a great balance of simplicity and high performance, while Lasso can be a strong contender when all features are relevant. This comparison helps us decide which features to move forward with for our final model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4ad6ca",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6149afc",
   "metadata": {},
   "source": [
    "\n",
    "Today, we explored the world of feature selection! We learned that it's a critical step for reducing dimensionality, preventing overfitting, and improving model efficiency. We covered three main types of techniques:\n",
    "\n",
    "    - Filter Methods: \n",
    "        - Fast and simple, they use statistical metrics to rank features. We saw an example with VarianceThreshold.\n",
    "\n",
    "    - Wrapper Methods: \n",
    "        - They use a machine learning model to evaluate feature subsets. We used RFE with LogisticRegression to find the best subset.\n",
    "\n",
    "    - Embedded Methods: \n",
    "        - They perform feature selection during the model training process itself, with Lasso being a great example.\n",
    "\n",
    "Each method has its pros and cons, and the best choice often depends on your specific problem and dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb9d334",
   "metadata": {},
   "source": [
    "## What's Next?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ffe16c",
   "metadata": {},
   "source": [
    "\n",
    "On Day 19, we'll shift gears from improving models to understanding them. We'll dive into the fascinating world of model interpretability. We'll learn about powerful tools like SHAP, LIME, and Permutation Importance that help us answer the question, \"Why did my model make that prediction?\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
