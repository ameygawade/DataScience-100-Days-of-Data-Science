{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac042445",
   "metadata": {},
   "source": [
    "# Day 73: Bidirectional RNNs & GRUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00307856",
   "metadata": {},
   "source": [
    "In the last session (Day 72), we explored LSTMs — how they solve the vanishing gradient problem using input, forget, and output gates.\n",
    "\n",
    "But today, we’re going one step further — into GRUs (Gated Recurrent Units) and Bidirectional RNNs — two powerful variations that make sequence models even more efficient and insightful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc96cb7",
   "metadata": {},
   "source": [
    "## Topics Covered:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985f3a24",
   "metadata": {},
   "source": [
    "- What is GRUs (Gated Recurrent Units)\n",
    "\n",
    "- When to use GRU vs LSTM\n",
    "\n",
    "- Bidirectional Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3da2f8",
   "metadata": {},
   "source": [
    "## What is GRUs (Gated Recurrent Units)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209ee981",
   "metadata": {},
   "source": [
    "The Gated Recurrent Unit (GRU) is essentially a simplified version of the LSTM (Long Short-Term Memory) network. Remember how the LSTM had three main gates—Input, Forget, and Output?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6cb207",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "GRU is the streamlined, minimalist version. It reduces the three gates down to just two:\n",
    "\n",
    "- Update Gate ($z_t$): \n",
    " - This gate acts as a combination of the LSTM's input and forget gates. It decides how much of the past information to keep and how much of the new information to let in.\n",
    "\n",
    "- Reset Gate ($r_t$): \n",
    " - This gate decides how much of the previous state is relevant to compute the new state. A strong reset signal basically allows the hidden state to forget the past quickly.\n",
    "\n",
    "`Analogy`:\n",
    "- Think of LSTM as a complex office security system with three separate checkpoints (Input, Forget, Output). \n",
    "- GRU is the modern, open-plan office: it merges two checkpoints into one sleek Update Gate, maintaining efficiency while reducing overhead. \n",
    "- Fewer gates mean fewer parameters and thus faster training!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab78126",
   "metadata": {},
   "source": [
    "## When to use GRU vs LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85c215d",
   "metadata": {},
   "source": [
    "This is a classic interview question, guys, so pay attention! Both are excellent for capturing long-term dependencies. The choice often comes down to practicality:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f95afd5",
   "metadata": {},
   "source": [
    "| **Feature**        | **GRU (Gated Recurrent Unit)**               | **LSTM (Long Short-Term Memory)**                           |\n",
    "| ------------------ | -------------------------------------------- | ----------------------------------------------------------- |\n",
    "| **Complexity**     | Simpler (2 Gates: Update & Reset)            | More Complex (3 Gates + Separate Cell State)                |\n",
    "| **Training Speed** | Generally faster                             | Generally slower (more parameters to update)                |\n",
    "| **Performance**    | Often comparable to LSTM                     | Can perform slightly better on very large or long sequences |\n",
    "| **Resource Use**   | Uses less memory and computation             | Requires more memory and computation                        |\n",
    "| **Best Use Case**  | When you need speed or have smaller datasets | When long-term dependencies or complex patterns matter      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c5256a",
   "metadata": {},
   "source": [
    "### Basic thumb rule of usage:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ee09b7",
   "metadata": {},
   "source": [
    "- Start with GRU: Because it trains faster and often gives comparable results, it’s a great first choice, especially when computational resources (like GPU time) are a concern or when your dataset is smaller.\n",
    "\n",
    "- Switch to LSTM: If the performance from the GRU is not satisfactory, or if you are dealing with extremely long sequences where maximizing the ability to capture dependencies is critical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4702c4a",
   "metadata": {},
   "source": [
    "`Analogy`:\n",
    "Think of LSTM as a full DSLR camera — powerful but heavy.\n",
    "GRU is your smartphone camera — quick, lightweight, and gets the job done beautifully most of the time!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56802b88",
   "metadata": {},
   "source": [
    "## Bidirectional Architecture (Bi-RNN/Bi-LSTM/Bi-GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01c5efd",
   "metadata": {},
   "source": [
    "Standard RNNs (including LSTMs and GRUs) are unidirectional; they process the sequence from t=1 to t=N. They look only at the past to predict the present/future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393ce052",
   "metadata": {},
   "source": [
    "- `The Problem`: In sequence tasks like Named Entity Recognition (NER) or Machine Translation, the meaning of a word can depend on the context that follows it.\n",
    "\n",
    "- `Analogy`:\n",
    " - **Consider the phrase** : \"The movie was bad, but the acting was superb.\"\n",
    "    - A standard RNN reading only up to 'The movie was bad,' might label it as a negative review. But a Bidirectional model gets the full context from the word \"superb\" coming later, allowing for a more nuanced interpretation.\n",
    "\n",
    " - **The Solution**: Bidirectional Architecture\n",
    "    - A Bidirectional layer works by running the input sequence through two separate recurrent layers:\n",
    "\n",
    "       - ***A Forward Layer***: Processes the sequence from t=1 to t=N (Past → Future).\n",
    "\n",
    "       - ***A Backward Layer***: Processes the sequence from t=N to t=1 (Future → Past).\n",
    "\n",
    "    - The final output at any timestep t is the concatenation of the hidden states from both the forward and backward passes. It gives the model complete context of the entire sequence. This is a must-use for applications like machine translation and text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c99b904",
   "metadata": {},
   "source": [
    "## Code Example: Bidirectional GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6eb37d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amey9\\Documents\\GitHub\\DataScience-100-Days-of-Data-Science\\.venv\\Lib\\site-packages\\keras\\src\\layers\\rnn\\bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">138,240</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">123,648</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │       \u001b[38;5;34m138,240\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m123,648\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">262,017</span> (1023.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m262,017\u001b[0m (1023.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">262,017</span> (1023.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m262,017\u001b[0m (1023.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, GRU, Dense\n",
    "\n",
    "# Example setup for a text classification model\n",
    "model = Sequential()\n",
    "\n",
    "# 1. Implement the Bidirectional Architecture\n",
    "# 2. Use the GRU layer for faster training\n",
    "model.add(Bidirectional(GRU(\n",
    "    units=128,          # Number of units/neurons\n",
    "    return_sequences=True # Important for stacking RNN layers\n",
    "), input_shape=(None, 50))) # Input shape: (timesteps, features)\n",
    "\n",
    "# Add another Bidirectional GRU layer\n",
    "model.add(Bidirectional(GRU(units=64)))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17021318",
   "metadata": {},
   "source": [
    "## Summary of Day 73"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3e831e",
   "metadata": {},
   "source": [
    "Today, we've upgraded our sequential modeling toolbox:\n",
    "\n",
    "- GRUs are the efficient, two-gate recurrent units that are generally faster to train and have fewer parameters than LSTMs, often providing comparable performance.\n",
    "\n",
    "- The choice between GRU vs. LSTM depends on resource constraints and dataset size. Start with GRU and use LSTM for maximum performance on complex, long sequences.\n",
    "\n",
    "- Bidirectional Architecture is crucial for sequence-to-sequence tasks. It allows the model to leverage context from both the past and the future, leading to much richer, contextual understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72cf2e7",
   "metadata": {},
   "source": [
    "## What's Next: Day 74"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7408ee41",
   "metadata": {},
   "source": [
    "\n",
    "We've built all the foundational blocks, guys. We have the sequence models—RNN, LSTM, and GRU—ready.\n",
    "\n",
    "In Day 74, we're diving into one of the most commercially valuable applications of these models: Time Series Forecasting.\n",
    "\n",
    "We will take historical data—be it stock prices, weather, or sensor readings—and use our Bidirectional LSTMs and GRUs to predict future values. This is where you start making real money in the industry! See you tomorrow! Keep learning, keep growing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
