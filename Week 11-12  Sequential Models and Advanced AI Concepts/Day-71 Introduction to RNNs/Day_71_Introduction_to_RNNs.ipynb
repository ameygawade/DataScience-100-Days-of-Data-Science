{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c149d9ac",
   "metadata": {},
   "source": [
    "# Day-71: Introduction to RNNs (Recurrent Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164107c3",
   "metadata": {},
   "source": [
    "In the last few sessions, we worked with Convolutional Neural Networks (CNNs) for image-based tasks.\n",
    "But now, we’re stepping into the world of sequences — where order and context matter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce22e11",
   "metadata": {},
   "source": [
    "So today, we’ll explore one of the most powerful architectures for sequence data — Recurrent Neural Networks (RNNs).\n",
    "\n",
    "These are the foundation of models used in language translation, speech recognition, time-series forecasting, and even chatbots like the one you’re watching me through right now!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e5e184",
   "metadata": {},
   "source": [
    "## Topics Covered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b8e188",
   "metadata": {},
   "source": [
    "- What are sequences?\n",
    "\n",
    "- Time-dependent data\n",
    "\n",
    "- The concept of Recurrent Neural Networks\n",
    "\n",
    "- Understanding Vanishing Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5754b8a4",
   "metadata": {},
   "source": [
    "## What are Sequences?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9936c5c",
   "metadata": {},
   "source": [
    "A sequence is simply data where the elements are dependent on each other, and their order is significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2df856c",
   "metadata": {},
   "source": [
    "- `Analogy`:\n",
    " - Let’s start simple — imagine you’re trying to predict the next word in a sentence:\\\n",
    "**The cat sat on the ____**\n",
    "\n",
    " - You can’t make a good guess unless you know what came before — “cat” and “sat on the.”\n",
    " - That’s sequence dependency — each piece of data depends on the previous ones.\n",
    "\n",
    "Examples:\n",
    "\n",
    "- Stock prices (today’s price depends on yesterday’s)\n",
    "\n",
    "- Temperature forecasting\n",
    "\n",
    "- Text, speech, and music\n",
    "\n",
    "Traditional neural networks (like CNNs or FFNNs) treat each input independently —\n",
    "but RNNs remember context, just like your brain recalls what you said a few seconds ago in a conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1787109",
   "metadata": {},
   "source": [
    "## How RNNs Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01654078",
   "metadata": {},
   "source": [
    "Think of an RNN as a human note-taker during a lecture:\n",
    "\n",
    " - Each new sentence (input) adds to their memory.\n",
    "\n",
    " - They use what they remember from before to understand the current point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5f4b33",
   "metadata": {},
   "source": [
    "### Time-Dependent Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf5040d",
   "metadata": {},
   "source": [
    "Time-dependent data is a type of sequence where the dependency is explicitly over time. RNNs handle this by feeding the output of a neuron at time step $t−1$ as an additional input to the same neuron at time step $t$. This hidden state acts as the network's memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6464e5df",
   "metadata": {},
   "source": [
    "- `Analogy`: Imagine reading a novel. As you read sentence $N$, you need to remember the context from sentences $N−1$,$N−2$, and so on, to fully understand the current plot. The RNN's hidden state is like your short-term memory for the text you've already processed.\n",
    "\n",
    "- Mathematical Vibe: The hidden state $h_t$ at time t is calculated as:\n",
    "\n",
    "$ h_t = f(W_hhh_t−1 + W_xhx_t + b_h) $\n",
    "\n",
    "where:\n",
    "- $ x_t $ is the input at time $ t$ . \n",
    "- $ h_t−1 $ is the previous hidden state (the memory). \n",
    "- $ f $ is the activation function (like $tanh$ or $ReLU$).\n",
    "- $ W$'s are the weight matrices (shared across all time steps)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4eb832",
   "metadata": {},
   "source": [
    "## The Vanishing Gradient Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dea1ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
