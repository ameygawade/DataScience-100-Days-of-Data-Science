{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f65e5eb6",
      "metadata": {
        "id": "f65e5eb6"
      },
      "source": [
        "# Day-75: Text Generation using RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f47ad7f7",
      "metadata": {
        "id": "f47ad7f7"
      },
      "source": [
        "In the last few days, we explored RNNs, LSTMs, GRUs, and Bidirectional Networks, learning how these models understand sequential data like text or time series.\n",
        "\n",
        "Today, we’ll take it one step further — and build something creative:\n",
        "A Text Generator using RNNs!\n",
        "\n",
        "We’ll train an RNN on a children’s stories corpus, and then make it generate new story text word-by-word — just like how ChatGPT or any AI writer starts from a word and keeps predicting the next one."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4c321e2",
      "metadata": {
        "id": "c4c321e2"
      },
      "source": [
        "## Topics Covered"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e550d7c",
      "metadata": {
        "id": "1e550d7c"
      },
      "source": [
        "- About the Dataset\n",
        "\n",
        "- Revisiting NLP Concepts We’ll Use\n",
        "\n",
        "- Preparing Data for RNN Text Generation\n",
        "\n",
        "- Building the RNN Model\n",
        "\n",
        "- Generating Text Word-by-Word\n",
        "\n",
        "- Evaluation & Experimentation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e6066bb",
      "metadata": {
        "id": "0e6066bb"
      },
      "source": [
        "## The Dataset: Children Stories Text Corpus (Kaggle)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "228693a9",
      "metadata": {
        "id": "228693a9"
      },
      "source": [
        "Guys, for any project, the first step is always the data! Our dataset is a fantastic collection of children's stories. Why stories? Because they have a sequence and a context."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6f6df8a",
      "metadata": {
        "id": "f6f6df8a"
      },
      "source": [
        "`Analogy`:\n",
        "- Think of this dataset as a massive library of bedtime stories.\n",
        "- When a kid reads a lot of stories, they learn the structure: \"Once upon a time...\" is often followed by a character introduction.\n",
        "- Our model is going to \"read\" this library and learn the grammar, the sentence structure, and the word-to-word dependencies to tell its own story."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73d383ac",
      "metadata": {
        "id": "73d383ac"
      },
      "source": [
        "We’ll be using the Children Stories Text Corpus:https://www.kaggle.com/datasets/edenbd/children-stories-text-corpus/data from Kaggle.\n",
        "It contains hundreds of story texts written for kids — simple grammar, repetitive sentence structures, and rich vocabulary — perfect for language generation tasks!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ef7b84d",
      "metadata": {
        "id": "8ef7b84d"
      },
      "source": [
        "This kind of dataset helps our RNN learn storytelling patterns like:\n",
        "\n",
        "- Sentence structure (subject → verb → object)\n",
        "\n",
        "- Repetitive story motifs (\"Once upon a time\", \"and then\", etc.)\n",
        "\n",
        "- Predictable transitions (\"The next day...\", \"Suddenly...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67128e2d",
      "metadata": {
        "id": "67128e2d"
      },
      "source": [
        "## Concepts from NLP we will use"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b4df8e8",
      "metadata": {
        "id": "5b4df8e8"
      },
      "source": [
        "We aren't starting from scratch! We'll leverage the power of foundational NLP concepts we covered previously."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ad2ed04",
      "metadata": {
        "id": "3ad2ed04"
      },
      "source": [
        "| **NLP Concept**                        | **Day Covered**              | **How We Use It in Text Generation**                                                        | **Analogy**                                                                                                                  |\n",
        "| :------------------------------------- | :--------------------------- | :------------------------------------------------------------------------------------------ | :--------------------------------------------------------------------------------------------------------------------------- |\n",
        "| Tokenization                           | Day 50                       | We break the stories into words or characters, creating a vocabulary.                       | Breaking a long book into individual words to check the frequency of each word.                                              |\n",
        "| Lowercasing & Cleaning                 | Day 50                       | We normalize text to ensure uniformity — “Cat” and “cat” are treated the same.              | Making sure everyone wears the same uniform before a group photo.                                                            |\n",
        "| Stopwords                              | Day 50                       | Usually, we keep stopwords here as they help maintain the flow of sentences.                | Keeping connecting words like “and” or “but” to ensure the story makes sense.                                                |\n",
        "| Word Embeddings (e.g., Word2Vec/GloVe) | Day 52                       | We convert each token into a dense vector representation to capture semantic meaning.       | Giving each word a unique ID card that also contains information about what the word means and what words are similar to it. |\n",
        "| Sequence Data                          | General RNN Concept          | RNNs are designed to handle ordered data, so word order defines meaning in text generation. | A detective trying to solve a crime — the order of events is crucial to understanding the full story.                        |\n",
        "| Padding                                | (General Preprocessing)      | Ensures all input sequences are of the same length before feeding them into RNN.            | Making all sentences the same length by adding blank spaces at the start.                                                    |\n",
        "| One-hot / Categorical Encoding         | (Used before model training) | Converts the target (next word) into categorical vectors for training.                      | Giving each possible next word its own “slot” in the prediction list.                                                        |\n",
        "| Text Generation Loop                   | (Core to RNN Workflow)       | Repeatedly predicts the next word and appends it to the input sequence.                     | Like a storyteller who keeps adding one word at a time until the story ends.                                                 |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0fcaa96",
      "metadata": {
        "id": "d0fcaa96"
      },
      "source": [
        "So yes — we’re reusing everything from Day 50–55!\n",
        "Only this time, we’re not classifying or clustering text — we’re generating new text."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dba8ed19",
      "metadata": {
        "id": "dba8ed19"
      },
      "source": [
        "## Preparing Data for RNN Text Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b32fd327",
      "metadata": {},
      "source": [
        "## Checking GPU tensorflow configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5eca9958",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: TMPDIR=/media/amey/New Volume/tmp\n",
            "env: PIP_CACHE_DIR=/media/amey/New Volume/pip-cache\n",
            "Requirement already satisfied: tensorflow==2.20.0 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from tensorflow[and-cuda]==2.20.0) (2.20.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from tensorflow==2.20.0->tensorflow[and-cuda]==2.20.0) (2.3.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from tensorflow==2.20.0->tensorflow[and-cuda]==2.20.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from tensorflow==2.20.0->tensorflow[and-cuda]==2.20.0) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from tensorflow==2.20.0->tensorflow[and-cuda]==2.20.0) (0.6.0)\n",
            "Requirement already satisfied: google_pasta>=0.1.1 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from tensorflow==2.20.0->tensorflow[and-cuda]==2.20.0) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from tensorflow==2.20.0->tensorflow[and-cuda]==2.20.0) (18.1.1)\n",
            "Requirement already satisfied: opt_einsum>=2.3.2 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from tensorflow==2.20.0->tensorflow[and-cuda]==2.20.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from tensorflow==2.20.0->tensorflow[and-cuda]==2.20.0) (25.0)\n",
            "Requirement already satisfied: protobuf>=5.28.0 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from tensorflow==2.20.0->tensorflow[and-cuda]==2.20.0) (6.33.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from tensorflow==2.20.0->tensorflow[and-cuda]==2.20.0) (2.32.5)\n",
            "Requirement already satisfied: setuptools in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from tensorflow==2.20.0->tensorflow[and-cuda]==2.20.0) (59.6.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from tensorflow==2.20.0->tensorflow[and-cuda]==2.20.0) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from tensorflow==2.20.0->tensorflow[and-cuda]==2.20.0) (3.1.0)\n",
            "Requirement already satisfied: typing_extensions>=3.6.6 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from tensorflow==2.20.0->tensorflow[and-cuda]==2.20.0) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from tensorflow==2.20.0->tensorflow[and-cuda]==2.20.0) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from tensorflow==2.20.0->tensorflow[and-cuda]==2.20.0) (1.75.1)\n",
            "Requirement already satisfied: tensorboard~=2.20.0 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from tensorflow==2.20.0->tensorflow[and-cuda]==2.20.0) (2.20.0)\n",
            "Requirement already satisfied: keras>=3.10.0 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from tensorflow==2.20.0->tensorflow[and-cuda]==2.20.0) (3.11.3)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from tensorflow==2.20.0->tensorflow[and-cuda]==2.20.0) (2.2.6)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from tensorflow==2.20.0->tensorflow[and-cuda]==2.20.0) (3.15.1)\n",
            "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from tensorflow==2.20.0->tensorflow[and-cuda]==2.20.0) (0.5.3)\n",
            "Requirement already satisfied: nvidia-cublas-cu12<13.0,>=12.5.3.2 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from tensorflow[and-cuda]==2.20.0) (12.9.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12<13.0,>=12.5.82 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from tensorflow[and-cuda]==2.20.0) (12.9.79)\n",
            "Requirement already satisfied: nvidia-cuda-nvcc-cu12<13.0,>=12.5.82 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from tensorflow[and-cuda]==2.20.0) (12.9.86)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12<13.0,>=12.5.82 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from tensorflow[and-cuda]==2.20.0) (12.9.86)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12<13.0,>=12.5.82 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from tensorflow[and-cuda]==2.20.0) (12.9.79)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12<10.0,>=9.3.0.75 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from tensorflow[and-cuda]==2.20.0) (9.14.0.64)\n",
            "Requirement already satisfied: nvidia-cufft-cu12<12.0,>=11.2.3.61 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from tensorflow[and-cuda]==2.20.0) (11.4.1.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12<11.0,>=10.3.6.82 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from tensorflow[and-cuda]==2.20.0) (10.3.10.19)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12<12.0,>=11.6.3.83 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from tensorflow[and-cuda]==2.20.0) (11.7.5.82)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12<13.0,>=12.5.1.3 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from tensorflow[and-cuda]==2.20.0) (12.5.10.65)\n",
            "Requirement already satisfied: nvidia-nccl-cu12<3.0,>=2.25.1 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from tensorflow[and-cuda]==2.20.0) (2.28.3)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12<13.0,>=12.5.82 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from tensorflow[and-cuda]==2.20.0) (12.9.86)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow==2.20.0->tensorflow[and-cuda]==2.20.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow==2.20.0->tensorflow[and-cuda]==2.20.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow==2.20.0->tensorflow[and-cuda]==2.20.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow==2.20.0->tensorflow[and-cuda]==2.20.0) (2025.10.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from tensorboard~=2.20.0->tensorflow==2.20.0->tensorflow[and-cuda]==2.20.0) (3.9)\n",
            "Requirement already satisfied: pillow in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from tensorboard~=2.20.0->tensorflow==2.20.0->tensorflow[and-cuda]==2.20.0) (12.0.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from tensorboard~=2.20.0->tensorflow==2.20.0->tensorflow[and-cuda]==2.20.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from tensorboard~=2.20.0->tensorflow==2.20.0->tensorflow[and-cuda]==2.20.0) (3.1.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.20.0->tensorflow[and-cuda]==2.20.0) (0.45.1)\n",
            "Requirement already satisfied: rich in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from keras>=3.10.0->tensorflow==2.20.0->tensorflow[and-cuda]==2.20.0) (14.2.0)\n",
            "Requirement already satisfied: namex in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from keras>=3.10.0->tensorflow==2.20.0->tensorflow[and-cuda]==2.20.0) (0.1.0)\n",
            "Requirement already satisfied: optree in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from keras>=3.10.0->tensorflow==2.20.0->tensorflow[and-cuda]==2.20.0) (0.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow==2.20.0->tensorflow[and-cuda]==2.20.0) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from rich->keras>=3.10.0->tensorflow==2.20.0->tensorflow[and-cuda]==2.20.0) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from rich->keras>=3.10.0->tensorflow==2.20.0->tensorflow[and-cuda]==2.20.0) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow==2.20.0->tensorflow[and-cuda]==2.20.0) (0.1.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%env TMPDIR=/media/amey/New Volume/tmp\n",
        "%env PIP_CACHE_DIR=/media/amey/New Volume/pip-cache\n",
        "\n",
        "# (Re)install in this kernel only, without filling root:\n",
        "%pip install --no-cache-dir \"tensorflow[and-cuda]==2.20.0\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "941a741d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/media/amey/New Volume/tf-venv/bin/python\n",
            "3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(sys.executable) \n",
        "print(sys.version)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b406c1c",
      "metadata": {},
      "source": [
        "### Sanity check that TensorFlow can see your GPU and actually run math on it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d13331d0",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-19 11:35:09.707240: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-10-19 11:35:09.748178: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-10-19 11:35:10.726164: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU devices: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "Matmul: 0.07 s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1760866510.947860   99116 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1524 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf, time\n",
        "print(\"GPU devices:\", tf.config.list_physical_devices('GPU'))\n",
        "with tf.device('/GPU:0'):\n",
        "    a=tf.random.normal([3000,3000]); b=tf.random.normal([3000,3000])\n",
        "    t=time.time(); _=tf.matmul(a,b).numpy(); print(\"Matmul:\", round(time.time()-t,3),\"s\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "b3d5ae87",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: Physical devices cannot be modified after being initialized\n",
            "GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "Mixed precision: <DTypePolicy \"mixed_float16\">\n"
          ]
        }
      ],
      "source": [
        "# --- Run FIRST, before any TF/Keras ops ---\n",
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "# Use env var so it works even if TF was partially initialized by something else\n",
        "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import mixed_precision\n",
        "\n",
        "# Try API-based memory growth (ok if it's already initialized)\n",
        "gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "try:\n",
        "    for gpu in gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "except RuntimeError as e:\n",
        "    print(\"Note:\", e)  # harmless if runtime already initialized\n",
        "\n",
        "# Optional perf tweak; disable if you see graph compilation issues\n",
        "try:\n",
        "    tf.config.optimizer.set_jit(True)  # XLA\n",
        "except Exception as e:\n",
        "    print(\"XLA not enabled:\", e)\n",
        "\n",
        "# Mixed precision on Ampere is a free win\n",
        "mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "print(\"GPUs:\", gpus)\n",
        "print(\"Mixed precision:\", mixed_precision.global_policy())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8083ad1a",
      "metadata": {
        "id": "8083ad1a"
      },
      "source": [
        "### 1. Load the corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "7b0c3fe2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kagglehub in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (0.3.13)\n",
            "Requirement already satisfied: numpy in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (2.2.6)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.10.7-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from kagglehub) (25.0)\n",
            "Requirement already satisfied: pyyaml in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from kagglehub) (6.0.3)\n",
            "Requirement already satisfied: requests in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from kagglehub) (2.32.5)\n",
            "Requirement already satisfied: tqdm in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from kagglehub) (4.67.1)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib)\n",
            "  Downloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib)\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib)\n",
            "  Downloading fonttools-4.60.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (112 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
            "  Downloading kiwisolver-1.4.9-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: pillow>=8 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from matplotlib) (12.0.0)\n",
            "Collecting pyparsing>=3 (from matplotlib)\n",
            "  Downloading pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from requests->kagglehub) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from requests->kagglehub) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from requests->kagglehub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /media/amey/New Volume/tf-venv/lib/python3.10/site-packages (from requests->kagglehub) (2025.10.5)\n",
            "Downloading matplotlib-3.10.7-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
            "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading fonttools-4.60.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kiwisolver-1.4.9-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
            "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [matplotlib]6\u001b[0m [matplotlib]\n",
            "\u001b[1A\u001b[2KSuccessfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.60.1 kiwisolver-1.4.9 matplotlib-3.10.7 pyparsing-3.2.5\n"
          ]
        }
      ],
      "source": [
        "! pip install kagglehub numpy matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d57d47f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d57d47f",
        "outputId": "bf23aa01-7dbe-4e85-c2d5-26865c7dae0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: /home/amey/.cache/kagglehub/datasets/edenbd/children-stories-text-corpus/versions/1\n",
            "/home/amey/.cache/kagglehub/datasets/edenbd/children-stories-text-corpus/versions/1/cleaned_merged_fairy_tales_without_eos.txt\n",
            "Number of characters: 20455694\n",
            "Sample preview (first 500 chars):\n",
            "\n",
            "The Happy Prince.\n",
            "HIGH above the city, on a tall column, stood the statue of the Happy Prince.  He was gilded all over with thin leaves of fine gold, for eyes he had two bright sapphires, and a large red ruby glowed on his sword-hilt.\n",
            "He was very much admired indeed.  “He is as beautiful as a weathercock,” remarked one of the Town Councillors who wished to gain a reputation for having artistic tastes; “only not quite so useful,” he added, fearing lest people should think him unpractical, which h\n",
            "\n",
            "After Cleanup:\n",
            "\n",
            "Number of characters: 22430730\n",
            "Sample preview (first 500 chars):\n",
            "\n",
            "Sentence count (approx): 214397\n",
            "Has double <eos>?: False\n",
            "the happy prince . <eos> high above the city , on a tall column , stood the statue of the happy prince . <eos> he was gilded all over with thin leaves of fine gold , for eyes he had two bright sapphires , and a large red ruby glowed on his sword - hilt . <eos> he was very much admired indeed . <eos> \" he is as beautiful as a weathercock , \" remarked one of the town councillors who wished to gain a reputation for having artistic tastes ; \" only not quite so useful , \" he added , fearing lest peop\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import kagglehub\n",
        "import re\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"edenbd/children-stories-text-corpus\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "DATA_FILE = \"cleaned_merged_fairy_tales_without_eos.txt\"\n",
        "CORPUS_PATH = f\"{path}/{DATA_FILE}\"\n",
        "print(CORPUS_PATH)\n",
        "assert os.path.exists(CORPUS_PATH), f'Could not find {CORPUS_PATH}.'\n",
        "\n",
        "with open(CORPUS_PATH, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "print('Number of characters:', len(raw_text))\n",
        "print('Sample preview (first 500 chars):\\n')\n",
        "print(f'{raw_text[:500]}\\n')\n",
        "\n",
        "# tiny cleanup\n",
        "# collapses all white spaces\n",
        "text = \" \".join(raw_text.split())\n",
        "# lower text\n",
        "text = text.lower()\n",
        "# Normalize smart quotes & dashes to plain ASCII (consistency)\n",
        "text = (text\n",
        "        .replace(\"“\", '\"').replace(\"”\", '\"')\n",
        "        .replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
        "        .replace(\"—\", \"-\"))\n",
        "text = re.sub(r'\\.\\s*\\.\\s*\\.', ' … ', text) \n",
        "\n",
        "# space out other punctuation so they becode tokens\n",
        "text = re.sub(r'([,;:\\-—\"“”‘’()\\[\\]])', r' \\1 ', text)\n",
        "text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "# Avoid <eos> after abbreviations like “mr.”, “dr.”\n",
        "abbrevs = r'(mr|mrs|ms|dr|st|vs|etc|e\\.g|i\\.e)'\n",
        "text = re.sub(fr'\\b{abbrevs}\\.', lambda m: m.group(0).replace('.', '<DOT>'), text)\n",
        "# inser <eos>(end of sentence) after sentence enders\n",
        "text = re.sub(r'([.!?])', r' \\1 <eos>', text)\n",
        "text = text.replace('<DOT>', '.')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('After Cleanup:\\n')\n",
        "print('Number of characters:', len(text))\n",
        "print('Sample preview (first 500 chars):\\n')\n",
        "num_eos = text.count(\"<eos>\")\n",
        "print(\"Sentence count (approx):\", num_eos)\n",
        "print(\"Has double <eos>?:\", \"<eos> <eos>\" in text)\n",
        "print(text[:500])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69fdd695",
      "metadata": {
        "id": "69fdd695"
      },
      "source": [
        "### 2. Tokenize (word-level)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67a49234",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67a49234",
        "outputId": "fec9da03-21c9-4177-9ad2-dc1c9d0a26b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 10001\n",
            "Unique words (vocab):48496\n",
            "Number of tokens: 4756580\n",
            "Vocab used by model (input_dim):10001\n",
            "OOV token rate: 0.00%\n"
          ]
        }
      ],
      "source": [
        "# tonkenising words\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "\n",
        "# Hyperparameter\n",
        "NUM_WORDS = 10000 # Limit vocab\n",
        "OOV_TOKEN = \"<OOV>\" # Out-Vocabluary token\n",
        "FILTER = '' # punctuation remover\n",
        "\n",
        "# init tokenizer\n",
        "tokenizer = Tokenizer(num_words=NUM_WORDS, oov_token=OOV_TOKEN, filters=FILTER)\n",
        "# Learn vocab from corpus\n",
        "tokenizer.fit_on_texts([text])\n",
        "\n",
        "# Extract word index and vocabulary size\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = (min(NUM_WORDS, len(word_index)) + 1)if NUM_WORDS else len(word_index) + 1\n",
        "\n",
        "# Convert word to token id \n",
        "tokens = tokenizer.texts_to_sequences([text])[0]\n",
        "\n",
        "\n",
        "# Sanity check \n",
        "print(f'Vocabulary size: {vocab_size}')\n",
        "print(f\"Unique words (vocab):{len(word_index)}\")\n",
        "print(f'Number of tokens: {len(tokens)}')\n",
        "print(f\"Vocab used by model (input_dim):{vocab_size}\")\n",
        "\n",
        "\n",
        "\n",
        "oov_id = tokenizer.word_index.get(\"<oov>\")\n",
        "tokens_np = np.array(tokens, dtype=np.int32)\n",
        "oov_rate = (tokens_np == oov_id).mean() * 100\n",
        "print(f\"OOV token rate: {oov_rate:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "874067e8",
      "metadata": {},
      "source": [
        "#### hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dafd2a2a",
      "metadata": {},
      "source": [
        "- NUM_WORDS = 10000\n",
        "    - We limit the vocabulary to the 10,000 most frequent words in the corpus.\n",
        "    - Why? It reduces memory and model size, and removes rare words that appear only once or twice.\n",
        "    - If your text has 47k unique words, only the top 10k will get unique IDs — the rest become <OOV>.\n",
        "\n",
        "- OOV_TOKEN = \"<OOV>\"\n",
        "    - OOV = Out-Of-Vocabulary token.\n",
        "    - This placeholder replaces any word not seen during training (rare/new words) — useful for generalization during generation.\n",
        "\n",
        "- FILTER = ''\n",
        "    - By default, Tokenizer removes punctuation (.,!?;: etc.).\n",
        "    - Here we disable filtering so punctuation (like . or ,) stays as separate tokens — important for story text and sentence structure."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90bd47e1",
      "metadata": {},
      "source": [
        "#### Sanity chek for tonkenizer process"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ca23511",
      "metadata": {},
      "source": [
        "*How to decide on Vocab limit(NUM_WORDS)?*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83f31a5a",
      "metadata": {},
      "source": [
        "Measure how many tokens get collapsed to <OOV> with your current NUM_WORDS=10_000. If it’s high, increase the cap.\n",
        "- **Rule of thumb for oov token**\n",
        "    - < 1–2% → 10k is fine.\n",
        "    - 2–5% → consider 20k.\n",
        "    - 5 % → go 20–30k."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BKSx8uBsFr1M",
      "metadata": {
        "id": "BKSx8uBsFr1M"
      },
      "source": [
        "### 3. Building training pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "bQCtzgvzFgWB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQCtzgvzFgWB",
        "outputId": "2d308e94-84a7-4b1c-9668-f5b9e534624a"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "WIN    = 60        # context length\n",
        "BATCH  = 512\n",
        "VAL_FRAC = 0.2\n",
        "GAP      = WIN      # safety gap to avoid leakage\n",
        "\n",
        "toks = tf.constant(tokens, dtype=tf.int32)\n",
        "\n",
        "# split on raw token stream + gap\n",
        "split = int(len(tokens) * (1 - VAL_FRAC))\n",
        "t_train = toks[:split - GAP]\n",
        "t_val   = toks[split + GAP:]\n",
        "\n",
        "ds_train = tf.keras.utils.timeseries_dataset_from_array(\n",
        "    data=t_train[:-1], targets=t_train[WIN:], sequence_length=WIN,\n",
        "    sequence_stride=1, batch_size=BATCH, shuffle=True\n",
        ")\n",
        "ds_val = tf.keras.utils.timeseries_dataset_from_array(\n",
        "    data=t_val[:-1], targets=t_val[WIN:], sequence_length=WIN,\n",
        "    sequence_stride=1, batch_size=BATCH, shuffle=False\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "eceb304b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X batch shape: (512, 60) y batch shape: (512,)\n",
            "tf.Tensor(\n",
            "[   5    4   11  480  201   55    8    3 2502    9 1499    1 3966    6\n",
            " 3506   32    3  515    9   15    5    4   52    3   81  780  189 4937\n",
            "  557  697   68   10  239 1005    5    4   11  225  153   21   51   11\n",
            "   44  396    8 3339   25   21   51   11   59   31   40 2770   22    3\n",
            "  231    2 1803   17], shape=(60,), dtype=int32)\n",
            "tf.Tensor(1179, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "# Inspect one batch\n",
        "for xb, yb in ds_train.take(1):\n",
        "    print(\"X batch shape:\", xb.shape, \"y batch shape:\", yb.shape)\n",
        "    break\n",
        "\n",
        "print(xb[0])\n",
        "print(yb[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GW03d0DIGRI4",
      "metadata": {
        "id": "GW03d0DIGRI4"
      },
      "source": [
        "## Building RNN model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9NOEGeAgGpHl",
      "metadata": {
        "id": "9NOEGeAgGpHl"
      },
      "source": [
        "### 1. Define LSTM model with embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "GdElwUvYGovp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "GdElwUvYGovp",
        "outputId": "242f65d5-1e34-416e-af9b-76fd595cb8d2"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import Sequential, Input\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n",
        "import tensorflow as tf\n",
        "\n",
        "EMBEDDING_DIM = 128\n",
        "LSTM_UNITS = 256\n",
        "VAL_DROPOUT = 0.2\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=(WIN,)),\n",
        "    Embedding(vocab_size, EMBEDDING_DIM, mask_zero=False),   # fixed-length windows -> no mask needed\n",
        "    LSTM(LSTM_UNITS, return_sequences=True),\n",
        "    Dropout(VAL_DROPOUT),\n",
        "    LSTM(LSTM_UNITS, return_sequences=True),\n",
        "    Dropout(VAL_DROPOUT),\n",
        "    LSTM(LSTM_UNITS),\n",
        "    Dense(vocab_size, activation=\"softmax\", dtype=\"float32\") # <-- critical for mixed precision\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4, clipnorm=1.0),\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d66b584f",
      "metadata": {},
      "source": [
        "### 2. Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "a5fceeff",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Targets OK\n",
            "Pred finite: False\n"
          ]
        }
      ],
      "source": [
        "# 1) Check labels are in range\n",
        "for xb, yb in ds_train.take(1):\n",
        "    assert tf.reduce_min(yb).numpy() >= 0\n",
        "    assert tf.reduce_max(yb).numpy() < vocab_size\n",
        "print(\"Targets OK\")\n",
        "\n",
        "# 2) Check model output is finite for one batch\n",
        "preds = model(xb, training=False)\n",
        "import numpy as np\n",
        "print(\"Pred finite:\", np.isfinite(preds.numpy()).all())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89cc3b9a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X min/max: 1 9997\n",
            "y min/max: 1 9744\n"
          ]
        }
      ],
      "source": [
        "# take one batch\n",
        "xb, yb = next(iter(ds_train))\n",
        "\n",
        "# 1) all input ids within embedding range?\n",
        "import tensorflow as tf\n",
        "print(\"X min/max:\", tf.reduce_min(xb).numpy(), tf.reduce_max(xb).numpy())\n",
        "assert tf.reduce_min(xb).numpy() >= 0\n",
        "assert tf.reduce_max(xb).numpy() < vocab_size, \"Some input ids >= vocab_size!\"\n",
        "\n",
        "# 2) targets range (you already checked but keep it here)\n",
        "print(\"y min/max:\", tf.reduce_min(yb).numpy(), tf.reduce_max(yb).numpy())\n",
        "assert tf.reduce_min(yb).numpy() >= 0\n",
        "assert tf.reduce_max(yb).numpy() < vocab_size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "0behxPMKGOrg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0behxPMKGOrg",
        "outputId": "a83baae7-e5c1-4eb1-8fa6-13a193178bfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m7431/7431\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 76ms/step - accuracy: 0.1808 - loss: 5.1825 - val_accuracy: 0.2169 - val_loss: 4.9045 - learning_rate: 3.0000e-04\n",
            "Epoch 2/15\n",
            "\u001b[1m7431/7431\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m587s\u001b[0m 79ms/step - accuracy: 0.2243 - loss: 4.6780 - val_accuracy: 0.2280 - val_loss: 4.7164 - learning_rate: 3.0000e-04\n",
            "Epoch 3/15\n",
            "\u001b[1m7431/7431\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m597s\u001b[0m 80ms/step - accuracy: 0.2354 - loss: 4.5049 - val_accuracy: 0.2369 - val_loss: 4.6134 - learning_rate: 3.0000e-04\n",
            "Epoch 4/15\n",
            "\u001b[1m7431/7431\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m601s\u001b[0m 81ms/step - accuracy: 0.2427 - loss: 4.3924 - val_accuracy: 0.2421 - val_loss: 4.5445 - learning_rate: 3.0000e-04\n",
            "Epoch 5/15\n",
            "\u001b[1m7431/7431\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m604s\u001b[0m 81ms/step - accuracy: 0.2480 - loss: 4.3078 - val_accuracy: 0.2459 - val_loss: 4.4940 - learning_rate: 3.0000e-04\n",
            "Epoch 6/15\n",
            "\u001b[1m7431/7431\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m617s\u001b[0m 83ms/step - accuracy: 0.2522 - loss: 4.2413 - val_accuracy: 0.2482 - val_loss: 4.4554 - learning_rate: 3.0000e-04\n",
            "Epoch 7/15\n",
            "\u001b[1m7431/7431\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m592s\u001b[0m 80ms/step - accuracy: 0.2557 - loss: 4.1854 - val_accuracy: 0.2509 - val_loss: 4.4215 - learning_rate: 3.0000e-04\n",
            "Epoch 8/15\n",
            "\u001b[1m7431/7431\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m589s\u001b[0m 79ms/step - accuracy: 0.2589 - loss: 4.1369 - val_accuracy: 0.2536 - val_loss: 4.3966 - learning_rate: 3.0000e-04\n",
            "Epoch 9/15\n",
            "\u001b[1m7431/7431\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m569s\u001b[0m 77ms/step - accuracy: 0.2619 - loss: 4.0944 - val_accuracy: 0.2559 - val_loss: 4.3695 - learning_rate: 3.0000e-04\n",
            "Epoch 10/15\n",
            "\u001b[1m7431/7431\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m592s\u001b[0m 80ms/step - accuracy: 0.2646 - loss: 4.0567 - val_accuracy: 0.2579 - val_loss: 4.3496 - learning_rate: 3.0000e-04\n",
            "Epoch 11/15\n",
            "\u001b[1m7431/7431\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m590s\u001b[0m 79ms/step - accuracy: 0.2666 - loss: 4.0235 - val_accuracy: 0.2592 - val_loss: 4.3380 - learning_rate: 3.0000e-04\n",
            "Epoch 12/15\n",
            "\u001b[1m7431/7431\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m589s\u001b[0m 79ms/step - accuracy: 0.2689 - loss: 3.9934 - val_accuracy: 0.2604 - val_loss: 4.3267 - learning_rate: 3.0000e-04\n",
            "Epoch 13/15\n",
            "\u001b[1m7431/7431\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m589s\u001b[0m 79ms/step - accuracy: 0.2706 - loss: 3.9659 - val_accuracy: 0.2619 - val_loss: 4.3093 - learning_rate: 3.0000e-04\n",
            "Epoch 14/15\n",
            "\u001b[1m7431/7431\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m605s\u001b[0m 81ms/step - accuracy: 0.2728 - loss: 3.9404 - val_accuracy: 0.2632 - val_loss: 4.3004 - learning_rate: 3.0000e-04\n",
            "Epoch 15/15\n",
            "\u001b[1m7431/7431\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m610s\u001b[0m 82ms/step - accuracy: 0.2744 - loss: 3.9166 - val_accuracy: 0.2644 - val_loss: 4.2938 - learning_rate: 3.0000e-04\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.callbacks import ReduceLROnPlateau,EarlyStopping,ModelCheckpoint\n",
        "from tensorflow.keras import mixed_precision\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "EPOCHS = 15\n",
        "\n",
        "MODEL_PATH = \"day75_lstm_textgen.keras\"\n",
        "\n",
        "callbacks = [\n",
        "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-5, verbose=1),\n",
        "    EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True),\n",
        "    ModelCheckpoint(MODEL_PATH, monitor=\"val_loss\", save_best_only=True)\n",
        "]\n",
        "\n",
        "\n",
        "VAL_FRAC = 0.2\n",
        "split = int(len(tokens) * (1 - VAL_FRAC))\n",
        "\n",
        "n_train = (split - GAP)         # number of tokens in train stream\n",
        "n_val   = (len(tokens) - (split + GAP))\n",
        "\n",
        "train_steps = max((n_train - WIN) // BATCH, 1)\n",
        "val_steps   = max((n_val   - WIN) // BATCH, 1)\n",
        "\n",
        "history = model.fit(\n",
        "    ds_train.repeat(),\n",
        "    epochs=EPOCHS,\n",
        "    steps_per_epoch=train_steps,\n",
        "    validation_data=ds_val.repeat(),\n",
        "    validation_steps=val_steps,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "9pCUAul8PmRL",
      "metadata": {
        "id": "9pCUAul8PmRL"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history[\"loss\"], label=\"train\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"val\")\n",
        "plt.legend(); plt.title(\"Loss\"); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f5b9355",
      "metadata": {},
      "source": [
        "### Saving best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "3986dc6f",
      "metadata": {},
      "outputs": [],
      "source": [
        "best = tf.keras.models.load_model(\"day75_lstm_textgen.keras\")\n",
        "import json\n",
        "with open(\"day75_tokenizer.json\",\"w\",encoding=\"utf-8\") as f:\n",
        "    f.write(tokenizer.to_json())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "e939cffc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Subset Perplexity: 54.43\n"
          ]
        }
      ],
      "source": [
        "def val_perplexity_subset(model, ds, batches=200):\n",
        "    import numpy as np\n",
        "    nll, n, seen = 0.0, 0, 0\n",
        "    for xb, yb in ds:\n",
        "        probs = model.predict(xb, verbose=0).astype(np.float64)\n",
        "        y = yb.numpy().astype(np.int64)\n",
        "        p = probs[np.arange(len(y)), y]\n",
        "        p = np.clip(p, 1e-12, 1.0)\n",
        "        nll += -np.log(p).sum()\n",
        "        n += len(y); seen += 1\n",
        "        if seen >= batches: break\n",
        "    return float(np.exp(nll / max(1, n)))\n",
        "\n",
        "print(\"Subset Perplexity:\", round(val_perplexity_subset(best, ds_val, batches=200), 2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "527779e3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Perplexity: 73.18\n"
          ]
        }
      ],
      "source": [
        "ppl = val_perplexity(best, ds_val)  # full ds_val, not subset\n",
        "print(\"Validation Perplexity:\", round(ppl, 2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e1ad9f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np, re, tensorflow as tf\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def _safe_normalize(p, eps=1e-12):\n",
        "    p = np.asarray(p, dtype=np.float64)\n",
        "    p[~np.isfinite(p)] = 0.0\n",
        "    s = p.sum()\n",
        "    if s <= eps:\n",
        "        nz = np.flatnonzero(p)\n",
        "        if len(nz) == 0:\n",
        "            p[:] = 1.0 / len(p)\n",
        "        else:\n",
        "            p[:] = 0.0\n",
        "            p[nz] = 1.0 / len(nz)\n",
        "    else:\n",
        "        p /= s\n",
        "    return p\n",
        "\n",
        "def sample_with_controls(probs, top_k=50, top_p=0.95, temperature=0.9):\n",
        "    p = np.asarray(probs, dtype=np.float64)\n",
        "    p = np.maximum(p, 1e-20)\n",
        "    p = np.log(p) / max(1e-6, temperature)\n",
        "    p = np.exp(p)\n",
        "    p = _safe_normalize(p)\n",
        "\n",
        "    if top_k and 0 < top_k < len(p):\n",
        "        keep = np.argpartition(-p, top_k)[:top_k]\n",
        "        mask = np.ones_like(p, bool); mask[keep] = False\n",
        "        p[mask] = 0.0; p = _safe_normalize(p)\n",
        "\n",
        "    if top_p and 0 < top_p < 1:\n",
        "        order = np.argsort(-p); cum = np.cumsum(p[order])\n",
        "        cut = order[cum > top_p]\n",
        "        p[cut] = 0.0; p = _safe_normalize(p)\n",
        "    return np.random.choice(len(p), p=p)\n",
        "\n",
        "# ---------- id maps / categories ----------\n",
        "word_to_id = tokenizer.word_index\n",
        "id_to_word = {idx: w for w, idx in word_to_id.items()}\n",
        "pad_idx = 0\n",
        "oov_idx = word_to_id.get(\"<oov>\")\n",
        "eos_idx = word_to_id.get(\"<eos>\")\n",
        "\n",
        "_PUNCTS = {\".\", \",\", \";\", \":\", \"!\", \"?\", \"-\", \"(\", \")\", '\"', \"…\"}\n",
        "PUNCT_IDS = {word_to_id[p] for p in _PUNCTS if p in word_to_id}\n",
        "\n",
        "FUNC_WORDS = {\"and\",\"of\",\"in\",\"to\",\"with\",\"by\",\"on\",\"as\",\"at\",\"from\",\"into\",\"under\",\"for\",\"up\",\"over\"}\n",
        "FUNC_IDS = {word_to_id[w] for w in FUNC_WORDS if w in word_to_id}\n",
        "\n",
        "_ALNUM_RE = re.compile(r\"[a-z0-9]+\")\n",
        "\n",
        "def _too_many_funcs(recent_words, limit=3):\n",
        "    c = 0\n",
        "    for w in reversed(recent_words):\n",
        "        if w in FUNC_WORDS: c += 1\n",
        "        else: break\n",
        "    return c >= limit\n",
        "\n",
        "def _postprocess(text):\n",
        "    text = text.replace(\" <eos>\", \".\")\n",
        "    text = re.sub(r\"\\s+([,;:.!?])\", r\"\\1\", text)\n",
        "    text = re.sub(r\"([(\\\"])\\s+\", r\"\\1\", text)\n",
        "    text = re.sub(r\"\\s+([\\\")])\", r\"\\1\", text)\n",
        "    text = re.sub(r\"([,;:.!?-])\\s*\\1+\", r\"\\1\", text)  # collapse repeated punct\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "# ---------- generator ----------\n",
        "def generate(seed_text,\n",
        "             max_new_tokens=120,\n",
        "             temperature=0.85,\n",
        "             top_k=50,\n",
        "             top_p=0.90,\n",
        "             repeat_penalty=1.07,\n",
        "             min_words_between_punct=2,\n",
        "             min_words_between_eos=8,\n",
        "             ban_oov=True):\n",
        "    out = seed_text.strip().lower()\n",
        "    recent_ids = []\n",
        "    words_since_punct = 999\n",
        "    words_since_eos = 999\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        ids = tokenizer.texts_to_sequences([out])[0]\n",
        "        seq = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "            [ids], maxlen=WIN, padding='post', truncating='pre'\n",
        "        )\n",
        "        probs = best.predict(seq, verbose=0)[0].astype(np.float64)\n",
        "\n",
        "        # hard bans\n",
        "        if pad_idx < len(probs): probs[pad_idx] = 0.0\n",
        "        if ban_oov and oov_idx is not None and oov_idx < len(probs):\n",
        "            probs[oov_idx] = 0.0\n",
        "\n",
        "        # discourage <eos> if sentence too short\n",
        "        if eos_idx is not None and eos_idx < len(probs) and words_since_eos < min_words_between_eos:\n",
        "            probs[eos_idx] *= 0.3\n",
        "\n",
        "        # light repetition penalty\n",
        "        for w in recent_ids[-20:]:\n",
        "            if 0 <= w < len(probs):\n",
        "                probs[w] /= repeat_penalty\n",
        "\n",
        "        # punctuation controls\n",
        "        last_is_punct = (recent_ids and recent_ids[-1] in PUNCT_IDS)\n",
        "        if last_is_punct or words_since_punct < min_words_between_punct:\n",
        "            for pid in PUNCT_IDS:\n",
        "                if pid < len(probs): probs[pid] *= 0.2\n",
        "\n",
        "        # function-word cooldown\n",
        "        last_words = [id_to_word.get(i, \"\") for i in recent_ids[-6:]]\n",
        "        if _too_many_funcs(last_words, limit=3):\n",
        "            for fid in FUNC_IDS:\n",
        "                if fid < len(probs): probs[fid] *= 0.25\n",
        "\n",
        "        probs = _safe_normalize(probs)\n",
        "\n",
        "        # resample until a valid token is chosen (bounded tries)\n",
        "        for _try in range(12):\n",
        "            idx = sample_with_controls(probs, top_k=top_k, top_p=top_p, temperature=temperature)\n",
        "            if (idx in PUNCT_IDS and words_since_punct < min_words_between_punct) \\\n",
        "               or (idx == eos_idx and words_since_eos < min_words_between_eos):\n",
        "                probs[idx] = 0.0; probs = _safe_normalize(probs)\n",
        "                continue\n",
        "            break\n",
        "        else:\n",
        "            # as a last resort, take argmax of remaining mass\n",
        "            idx = int(np.argmax(probs))\n",
        "\n",
        "        w = id_to_word.get(int(idx), \"\")\n",
        "        if not w:\n",
        "            # if unmapped (shouldn't happen), skip\n",
        "            continue\n",
        "\n",
        "        # append token\n",
        "        out += \" \" + w\n",
        "        recent_ids.append(int(idx))\n",
        "\n",
        "        if idx in PUNCT_IDS:\n",
        "            words_since_punct = 0\n",
        "        else:\n",
        "            words_since_punct += 1\n",
        "\n",
        "        if idx == eos_idx:\n",
        "            words_since_eos = 0\n",
        "        else:\n",
        "            words_since_eos += 1\n",
        "\n",
        "    return _postprocess(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "a9486a78",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizer oov_token: <OOV>\n",
            "Resolved OOV tokens: ['<OOV>']\n",
            "Resolved OOV ids   : [1]\n"
          ]
        }
      ],
      "source": [
        "# Build maps\n",
        "word_to_id = tokenizer.word_index\n",
        "id_to_word = {idx: w for w, idx in word_to_id.items()}\n",
        "\n",
        "# Find all plausible OOV spellings and the configured one\n",
        "oov_variants = {\"<OOV>\", \"<oov>\", \"<unk>\", \"<UNK>\"}\n",
        "if getattr(tokenizer, \"oov_token\", None):\n",
        "    oov_variants.add(tokenizer.oov_token)\n",
        "\n",
        "OOV_IDS = {word_to_id[w] for w in oov_variants if w in word_to_id}\n",
        "\n",
        "# (Optional) sanity check\n",
        "print(\"Tokenizer oov_token:\", getattr(tokenizer, \"oov_token\", None))\n",
        "print(\"Resolved OOV tokens:\", sorted(list(oov_variants & set(word_to_id.keys()))))\n",
        "print(\"Resolved OOV ids   :\", sorted(list(OOV_IDS)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "a3af600c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "once upon a time. on <OOV> in and to and at. on in and. around and in. <OOV> <OOV> in on and from on on on - to in <OOV> as in - which which in. as in from from that, in to to into, <OOV> <OOV> in, the the of of the country - <OOV> <OOV> is a <OOV> <OOV>. walks by all the <OOV> of his crew, <OOV> to the pequod's <OOV>, he must be able report to <OOV>. cave in <OOV>, <OOV> that the <OOV> of the sperm whale fishery, the pequod was in a large <OOV>. recognised the ship, and all this <OOV> of <OOV> <OOV> <OOV>; while the ship was inserted in, the ship was a great <OOV> - boat <OOV>\n"
          ]
        }
      ],
      "source": [
        "print(generate(\n",
        "    \"once upon a time\",\n",
        "    140,\n",
        "    temperature=0.8,\n",
        "    top_k=45,\n",
        "    top_p=0.9,\n",
        "    repeat_penalty=1.08,\n",
        "    min_words_between_punct=3\n",
        "))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "74fc8c17",
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "The layer sequential_4 has never been called and thus has no defined input.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[88], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m _ \u001b[38;5;241m=\u001b[39m best\u001b[38;5;241m.\u001b[39mpredict(np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, WIN), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32), verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 1) Create a fp32 softmax head (no retraining)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m base \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mModel(inputs\u001b[38;5;241m=\u001b[39m\u001b[43mbest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m, outputs\u001b[38;5;241m=\u001b[39mbest\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39moutput)\n\u001b[1;32m      8\u001b[0m fp32_head \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(vocab_size, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m                                   dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp32_head\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m best_fp32 \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mModel(base\u001b[38;5;241m.\u001b[39minput, fp32_head(base\u001b[38;5;241m.\u001b[39moutput))\n",
            "File \u001b[0;32m/media/amey/New Volume/tf-venv/lib/python3.10/site-packages/keras/src/ops/operation.py:275\u001b[0m, in \u001b[0;36mOperation.input\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minput\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    267\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Retrieves the input tensor(s) of a symbolic operation.\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03m    Only returns the tensor(s) corresponding to the *first time*\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;124;03m        Input tensor or list of input tensors.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_node_attribute_at_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_tensors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/media/amey/New Volume/tf-venv/lib/python3.10/site-packages/keras/src/ops/operation.py:306\u001b[0m, in \u001b[0;36mOperation._get_node_attribute_at_index\u001b[0;34m(self, node_index, attr, attr_name)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Private utility to retrieves an attribute (e.g. inputs) from a node.\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03mThis is used to implement the properties:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;124;03m    The operation's attribute `attr` at the node of index `node_index`.\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inbound_nodes:\n\u001b[0;32m--> 306\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has never been called \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand thus has no defined \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    309\u001b[0m     )\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inbound_nodes) \u001b[38;5;241m>\u001b[39m node_index:\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to get \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at node \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but the operation has only \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inbound_nodes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m inbound nodes.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    315\u001b[0m     )\n",
            "\u001b[0;31mAttributeError\u001b[0m: The layer sequential_4 has never been called and thus has no defined input."
          ]
        }
      ],
      "source": [
        "import numpy as np, tensorflow as tf\n",
        "\n",
        "# 0) Make sure the model is built\n",
        "_ = best.predict(np.zeros((1, WIN), dtype=np.int32), verbose=0)\n",
        "\n",
        "# 1) Create a fp32 softmax head (no retraining)\n",
        "base = tf.keras.Model(inputs=best.input, outputs=best.layers[-2].output)\n",
        "fp32_head = tf.keras.layers.Dense(vocab_size, activation=\"softmax\",\n",
        "                                  dtype=\"float32\", name=\"fp32_head\")\n",
        "\n",
        "best_fp32 = tf.keras.Model(base.input, fp32_head(base.output))\n",
        "\n",
        "# Alternate: force-build without running predict\n",
        "best.build((None, WIN))\n",
        "_ = best(tf.zeros((1, WIN), dtype=tf.int32), training=False)  # one call to finalize graph\n",
        "\n",
        "\n",
        "# 2) Copy weights from the old head -> new head (cast to float32)\n",
        "old_w = best.layers[-1].get_weights()\n",
        "fp32_head.set_weights([w.astype(\"float32\") for w in old_w])\n",
        "\n",
        "# 3) Sanity check: forward should now be finite\n",
        "xb, _ = next(iter(ds_train))\n",
        "preds = best_fp32(xb[:1], training=False).numpy()\n",
        "import numpy as np\n",
        "print(\"Pred finite:\", np.isfinite(preds).all())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4595f58a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "once upon a time - with and. on up in which, and in and. out for, which as. with a under. under to by which - for on. with to in. and at, of and. in and, over as. as of at <OOV>, upon his, voice of the <OOV> was a strange way, of course the <OOV> of the most <OOV> of the <OOV>; and so the <OOV>, <OOV> of all <OOV>, <OOV> all the <OOV> of <OOV>. jaw to <OOV> it, was as <OOV> as it had a <OOV>. uncle's <OOV> the whole <OOV> of the <OOV> <OOV>, so that it\n",
            "in a small village by the sea. on upon. under and about in in. as on and and at. with a. the on a in, in as and in and and. and and and and. on the. and in. into to, the by away to in and he a great <OOV> and the <OOV>. hast was about the <OOV> of the <OOV>, as if he were about to <OOV> the earth with him and the <OOV> <OOV>. raging <OOV>, that of the other hand, he would find himself a great deal to tell him how it was, and he had been <OOV> and <OOV> in the course reproach, and he <OOV>. walking his heart and he could not get into a certain hole like the <OOV>. left the way of the people of all the boys,\n"
          ]
        }
      ],
      "source": [
        "print(generate(\"once upon a time\", 120))\n",
        "print(generate(\"in a small village by the sea\", 150, temperature=0.8, top_k=60, top_p=0.9))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "a42680e5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "once upon a time; with in. and and <OOV> on and. with up and. to in under. <OOV> at. up a on, which to. under in on. under on. and and. a as into in. into to; of on. and to, into to, it was a very curious\n"
          ]
        }
      ],
      "source": [
        "print(generate(\"once upon a time\", 60, temperature=0.8, top_k=45, top_p=0.9))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "1ee8c954",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "once upon a time. and like <OOV>. <OOV> for... on, on and,,,. on at. upon in. to. from and into, by with which for and. and for, for.,...,.. on to, <OOV> to,, <OOV>, <OOV>. shrine. lasted, when you were going into the place, there was no, we <OOV>, but the <OOV> of the two - heads, and it was a pleasant one. logs, we have not seen, i should have taken a <OOV>, and then i am to be <OOV> the way, and i will stay there in time for us. minerva, \" \" what is you going, and i can, \" said the <OOV>. a', \" that\n",
            "in a small village by the sea. upon -. <OOV>...., in through.. and in... in for on in in from through and through with through down on. and, the. from...,. <OOV> of.., with. <OOV> the, <OOV> <OOV> voice in which a moment, and the <OOV> of a <OOV>, a low, thin - <OOV> of a kind - tree <OOV>; with the <OOV> - and - <OOV> <OOV>, which was like an most <OOV> manner, which was much larger powerful goal than ever. organ, not the white whale of a hundred - four - <OOV> <OOV>, were <OOV>, that <OOV> on <OOV> the surface of the whale. squeers, the <OOV> and the <OOV> were there and <OOV>, like <OOV> of\n"
          ]
        }
      ],
      "source": [
        "print(generate(\"once upon a time\", 150))\n",
        "print(generate(\"in a small village by the sea\", 150, temperature=0.85, top_k=60, top_p=0.92))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "690a6096",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf-venv (3.10.12)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
