{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f65e5eb6",
      "metadata": {
        "id": "f65e5eb6"
      },
      "source": [
        "# Day-75: Text Generation using RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f47ad7f7",
      "metadata": {
        "id": "f47ad7f7"
      },
      "source": [
        "In the last few days, we explored RNNs, LSTMs, GRUs, and Bidirectional Networks, learning how these models understand sequential data like text or time series.\n",
        "\n",
        "Today, we’ll take it one step further — and build something creative:\n",
        "A Text Generator using RNNs!\n",
        "\n",
        "We’ll train an RNN on a children’s stories corpus, and then make it generate new story text word-by-word — just like how ChatGPT or any AI writer starts from a word and keeps predicting the next one."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4c321e2",
      "metadata": {
        "id": "c4c321e2"
      },
      "source": [
        "## Topics Covered"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e550d7c",
      "metadata": {
        "id": "1e550d7c"
      },
      "source": [
        "- About the Dataset\n",
        "\n",
        "- Revisiting NLP Concepts We’ll Use\n",
        "\n",
        "- Preparing Data for RNN Text Generation\n",
        "\n",
        "- Building the RNN Model\n",
        "\n",
        "- Generating Text Word-by-Word\n",
        "\n",
        "- Evaluation & Experimentation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e6066bb",
      "metadata": {
        "id": "0e6066bb"
      },
      "source": [
        "## The Dataset: Children Stories Text Corpus (Kaggle)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "228693a9",
      "metadata": {
        "id": "228693a9"
      },
      "source": [
        "Guys, for any project, the first step is always the data! Our dataset is a fantastic collection of children's stories. Why stories? Because they have a sequence and a context."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6f6df8a",
      "metadata": {
        "id": "f6f6df8a"
      },
      "source": [
        "`Analogy`:\n",
        "- Think of this dataset as a massive library of bedtime stories.\n",
        "- When a kid reads a lot of stories, they learn the structure: \"Once upon a time...\" is often followed by a character introduction.\n",
        "- Our model is going to \"read\" this library and learn the grammar, the sentence structure, and the word-to-word dependencies to tell its own story."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73d383ac",
      "metadata": {
        "id": "73d383ac"
      },
      "source": [
        "We’ll be using the Children Stories Text Corpus:https://www.kaggle.com/datasets/edenbd/children-stories-text-corpus/data from Kaggle.\n",
        "It contains hundreds of story texts written for kids — simple grammar, repetitive sentence structures, and rich vocabulary — perfect for language generation tasks!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ef7b84d",
      "metadata": {
        "id": "8ef7b84d"
      },
      "source": [
        "This kind of dataset helps our RNN learn storytelling patterns like:\n",
        "\n",
        "- Sentence structure (subject → verb → object)\n",
        "\n",
        "- Repetitive story motifs (\"Once upon a time\", \"and then\", etc.)\n",
        "\n",
        "- Predictable transitions (\"The next day...\", \"Suddenly...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67128e2d",
      "metadata": {
        "id": "67128e2d"
      },
      "source": [
        "## Concepts from NLP we will use"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b4df8e8",
      "metadata": {
        "id": "5b4df8e8"
      },
      "source": [
        "We aren't starting from scratch! We'll leverage the power of foundational NLP concepts we covered previously."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ad2ed04",
      "metadata": {
        "id": "3ad2ed04"
      },
      "source": [
        "| **NLP Concept**                        | **Day Covered**              | **How We Use It in Text Generation**                                                        | **Analogy**                                                                                                                  |\n",
        "| :------------------------------------- | :--------------------------- | :------------------------------------------------------------------------------------------ | :--------------------------------------------------------------------------------------------------------------------------- |\n",
        "| Tokenization                           | Day 50                       | We break the stories into words or characters, creating a vocabulary.                       | Breaking a long book into individual words to check the frequency of each word.                                              |\n",
        "| Lowercasing & Cleaning                 | Day 50                       | We normalize text to ensure uniformity — “Cat” and “cat” are treated the same.              | Making sure everyone wears the same uniform before a group photo.                                                            |\n",
        "| Stopwords                              | Day 50                       | Usually, we keep stopwords here as they help maintain the flow of sentences.                | Keeping connecting words like “and” or “but” to ensure the story makes sense.                                                |\n",
        "| Word Embeddings (e.g., Word2Vec/GloVe) | Day 52                       | We convert each token into a dense vector representation to capture semantic meaning.       | Giving each word a unique ID card that also contains information about what the word means and what words are similar to it. |\n",
        "| Sequence Data                          | General RNN Concept          | RNNs are designed to handle ordered data, so word order defines meaning in text generation. | A detective trying to solve a crime — the order of events is crucial to understanding the full story.                        |\n",
        "| Padding                                | (General Preprocessing)      | Ensures all input sequences are of the same length before feeding them into RNN.            | Making all sentences the same length by adding blank spaces at the start.                                                    |\n",
        "| One-hot / Categorical Encoding         | (Used before model training) | Converts the target (next word) into categorical vectors for training.                      | Giving each possible next word its own “slot” in the prediction list.                                                        |\n",
        "| Text Generation Loop                   | (Core to RNN Workflow)       | Repeatedly predicts the next word and appends it to the input sequence.                     | Like a storyteller who keeps adding one word at a time until the story ends.                                                 |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0fcaa96",
      "metadata": {
        "id": "d0fcaa96"
      },
      "source": [
        "So yes — we’re reusing everything from Day 50–55!\n",
        "Only this time, we’re not classifying or clustering text — we’re generating new text."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dba8ed19",
      "metadata": {
        "id": "dba8ed19"
      },
      "source": [
        "## Preparing Data for RNN Text Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8083ad1a",
      "metadata": {
        "id": "8083ad1a"
      },
      "source": [
        "### 1. Load the corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5d57d47f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d57d47f",
        "outputId": "bf23aa01-7dbe-4e85-c2d5-26865c7dae0b"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'kagglehub'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkagglehub\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Download latest version\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'kagglehub'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import kagglehub\n",
        "import re\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"edenbd/children-stories-text-corpus\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "DATA_FILE = \"cleaned_merged_fairy_tales_without_eos.txt\"\n",
        "CORPUS_PATH = f\"{path}/{DATA_FILE}\"\n",
        "print(CORPUS_PATH)\n",
        "assert os.path.exists(CORPUS_PATH), f'Could not find {CORPUS_PATH}.'\n",
        "\n",
        "with open(CORPUS_PATH, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "print('Number of characters:', len(raw_text))\n",
        "print('Sample preview (first 500 chars):\\n')\n",
        "print(f'{raw_text[:500]}\\n')\n",
        "\n",
        "# tiny cleanup\n",
        "text = \" \".join(raw_text.split())\n",
        "text = text.lower()\n",
        "# inser <eos>(end of sentence) after sentence enders\n",
        "text = re.sub(r\"([.!?])\", r\" \\1 <eos>\", text)\n",
        "# space out other punctuation so they becode tokens\n",
        "text = re.sub(r'([,;:\\-—\"“”‘’()\\[\\]])', r' \\1 ', text)\n",
        "text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "print('After Cleanup:\\n')\n",
        "print('Number of characters:', len(text))\n",
        "print('Sample preview (first 500 chars):\\n')\n",
        "print(text[:500])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69fdd695",
      "metadata": {
        "id": "69fdd695"
      },
      "source": [
        "### 2. Tokenize (word-level)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a552ff19",
      "metadata": {
        "collapsed": true,
        "id": "a552ff19"
      },
      "outputs": [],
      "source": [
        "# ! pip install tensorflow keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "67a49234",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67a49234",
        "outputId": "fec9da03-21c9-4177-9ad2-dc1c9d0a26b8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
            "    app.start()\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
            "    self.io_loop.start()\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
            "    self._run_once()\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
            "    handle._run()\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\asyncio\\events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 519, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 508, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 400, in dispatch_shell\n",
            "    await result\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 368, in execute_request\n",
            "    await super().execute_request(stream, ident, parent)\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 455, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 577, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
            "    result = runner(coro)\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
            "    if await self.run_code(code, result, async_=asy):\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"C:\\Users\\amey9\\AppData\\Local\\Temp\\ipykernel_40432\\1048108165.py\", line 2, in <module>\n",
            "    import tensorflow as tf\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\tensorflow\\__init__.py\", line 37, in <module>\n",
            "    from tensorflow.python.tools import module_util as _module_util\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 37, in <module>\n",
            "    from tensorflow.python.eager import context\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 35, in <module>\n",
            "    from tensorflow.python.client import pywrap_tf_session\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\tensorflow\\python\\client\\pywrap_tf_session.py\", line 19, in <module>\n",
            "    from tensorflow.python.client._pywrap_tf_session import *\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "_ARRAY_API not found",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
            "    app.start()\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
            "    self.io_loop.start()\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
            "    self._run_once()\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
            "    handle._run()\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\asyncio\\events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 519, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 508, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 400, in dispatch_shell\n",
            "    await result\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 368, in execute_request\n",
            "    await super().execute_request(stream, ident, parent)\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 455, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 577, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
            "    result = runner(coro)\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
            "    if await self.run_code(code, result, async_=asy):\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"C:\\Users\\amey9\\AppData\\Local\\Temp\\ipykernel_40432\\1048108165.py\", line 2, in <module>\n",
            "    import tensorflow as tf\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\tensorflow\\__init__.py\", line 37, in <module>\n",
            "    from tensorflow.python.tools import module_util as _module_util\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 42, in <module>\n",
            "    from tensorflow.python import data\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\tensorflow\\python\\data\\__init__.py\", line 21, in <module>\n",
            "    from tensorflow.python.data import experimental\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\__init__.py\", line 96, in <module>\n",
            "    from tensorflow.python.data.experimental import service\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\service\\__init__.py\", line 419, in <module>\n",
            "    from tensorflow.python.data.experimental.ops.data_service_ops import distribute\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\data_service_ops.py\", line 24, in <module>\n",
            "    from tensorflow.python.data.experimental.ops import compression_ops\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\compression_ops.py\", line 16, in <module>\n",
            "    from tensorflow.python.data.util import structure\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py\", line 23, in <module>\n",
            "    from tensorflow.python.data.util import nest\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\tensorflow\\python\\data\\util\\nest.py\", line 36, in <module>\n",
            "    from tensorflow.python.framework import sparse_tensor as _sparse_tensor\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\tensorflow\\python\\framework\\sparse_tensor.py\", line 24, in <module>\n",
            "    from tensorflow.python.framework import constant_op\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 25, in <module>\n",
            "    from tensorflow.python.eager import execute\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 23, in <module>\n",
            "    from tensorflow.python.framework import dtypes\n",
            "  File \"c:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py\", line 29, in <module>\n",
            "    from tensorflow.python.lib.core import _pywrap_bfloat16\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "_ARRAY_API not found",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "numpy.core._multiarray_umath failed to import",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[1;31mImportError\u001b[0m: numpy.core._multiarray_umath failed to import"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "numpy.core.umath failed to import",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[1;31mImportError\u001b[0m: numpy.core.umath failed to import"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "Unable to convert function return value to a Python type! The signature was\n\t() -> handle",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# tonkenising words\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[0;32m      4\u001b[0m NUM_WORDS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\tensorflow\\__init__.py:37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\tensorflow\\python\\__init__.py:42\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Bring in subpackages.\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m data\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# from tensorflow.python import keras\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\tensorflow\\python\\data\\__init__.py:21\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"`tf.data.Dataset` API for input pipelines.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mSee [Importing Data](https://tensorflow.org/guide/data) for an overview.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AUTOTUNE\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n",
            "File \u001b[1;32mc:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\__init__.py:96\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Experimental API for building input pipelines.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mThis module contains experimental `Dataset` sources and transformations that can\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;124;03m@@UNKNOWN_CARDINALITY\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m service\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatching\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dense_to_ragged_batch\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatching\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dense_to_sparse_batch\n",
            "File \u001b[1;32mc:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\service\\__init__.py:419\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"API for using the tf.data service.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mThis module contains:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;124;03m  job of ParameterServerStrategy).\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 419\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m from_dataset_id\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m register_dataset\n",
            "File \u001b[1;32mc:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\data_service_ops.py:24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf2\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compression_ops\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservice\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _pywrap_server_lib\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservice\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _pywrap_utils\n",
            "File \u001b[1;32mc:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\compression_ops.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Ops for compressing and uncompressing dataset elements.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m structure\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_experimental_dataset_ops \u001b[38;5;28;01mas\u001b[39;00m ged_ops\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompress\u001b[39m(element):\n",
            "File \u001b[1;32mc:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msix\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwrapt\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nest\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m composite_tensor\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ops\n",
            "File \u001b[1;32mc:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\tensorflow\\python\\data\\util\\nest.py:36\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m\"\"\"## Functions for working with arbitrarily nested sequences of elements.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03mNOTE(mrry): This fork of the `tensorflow.python.util.nest` module\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m   arrays.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m_six\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sparse_tensor \u001b[38;5;28;01mas\u001b[39;00m _sparse_tensor\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _pywrap_utils\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nest\n",
            "File \u001b[1;32mc:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\tensorflow\\python\\framework\\sparse_tensor.py:24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf2\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m composite_tensor\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m constant_op\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dtypes\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ops\n",
            "File \u001b[1;32mc:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m types_pb2\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m execute\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dtypes\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m op_callbacks\n",
            "File \u001b[1;32mc:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pywrap_tfe\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m core\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dtypes\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tensor_shape\n",
            "File \u001b[1;32mc:\\Users\\amey9\\.conda\\envs\\tf-2.10\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:34\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m trace\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunction\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m trace_type\n\u001b[1;32m---> 34\u001b[0m _np_bfloat16 \u001b[38;5;241m=\u001b[39m \u001b[43m_pywrap_bfloat16\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_bfloat16_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mDTypeMeta\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m(_dtypes\u001b[38;5;241m.\u001b[39mDType), abc\u001b[38;5;241m.\u001b[39mABCMeta):\n\u001b[0;32m     38\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
            "\u001b[1;31mTypeError\u001b[0m: Unable to convert function return value to a Python type! The signature was\n\t() -> handle"
          ]
        }
      ],
      "source": [
        "# tonkenising words\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "NUM_WORDS = 10000\n",
        "OOV_TOKEN = \"<OOV>\"\n",
        "FILTER = ''\n",
        "\n",
        "tokenizer = Tokenizer(num_words=NUM_WORDS, oov_token=OOV_TOKEN, filters=FILTER)\n",
        "tokenizer.fit_on_texts([text])\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = (min(NUM_WORDS, len(word_index)) + 1)if NUM_WORDS else len(word_index) + 1\n",
        "tokens = tokenizer.texts_to_sequences([text])[0]\n",
        "\n",
        "print(f'Vocabulary size: {vocab_size}')\n",
        "print(f\"Unique words (vocab):{len(word_index)}\")\n",
        "print(f'Number of tokens: {len(tokens)}')\n",
        "print(f\"Vocab used by model (input_dim):{vocab_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BKSx8uBsFr1M",
      "metadata": {
        "id": "BKSx8uBsFr1M"
      },
      "source": [
        "### 3. Sequebce Building(sliding window -> next word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "bQCtzgvzFgWB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQCtzgvzFgWB",
        "outputId": "2d308e94-84a7-4b1c-9668-f5b9e534624a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X: (4806732, 60)  y: (4806732,)\n",
            "Prview of X:\n",
            "[[   3  312  136    5    4  358  502    3  565    2   34    9  869 4745\n",
            "     2  231    3 3825    8    3  312  136    5    4   11   14 5015   39\n",
            "    98   26 1122  687    8  401  315    2   24  139   11   22  106  505\n",
            "     1    2    6    9  310  283 5173 5383   34   17  833   29 7979    5\n",
            "     4   11   14   67]\n",
            " [ 312  136    5    4  358  502    3  565    2   34    9  869 4745    2\n",
            "   231    3 3825    8    3  312  136    5    4   11   14 5015   39   98\n",
            "    26 1122  687    8  401  315    2   24  139   11   22  106  505    1\n",
            "     2    6    9  310  283 5173 5383   34   17  833   29 7979    5    4\n",
            "    11   14   67  114]\n",
            " [ 136    5    4  358  502    3  565    2   34    9  869 4745    2  231\n",
            "     3 3825    8    3  312  136    5    4   11   14 5015   39   98   26\n",
            "  1122  687    8  401  315    2   24  139   11   22  106  505    1    2\n",
            "     6    9  310  283 5173 5383   34   17  833   29 7979    5    4   11\n",
            "    14   67  114 3073]\n",
            " [   5    4  358  502    3  565    2   34    9  869 4745    2  231    3\n",
            "  3825    8    3  312  136    5    4   11   14 5015   39   98   26 1122\n",
            "   687    8  401  315    2   24  139   11   22  106  505    1    2    6\n",
            "     9  310  283 5173 5383   34   17  833   29 7979    5    4   11   14\n",
            "    67  114 3073  309]\n",
            " [   4  358  502    3  565    2   34    9  869 4745    2  231    3 3825\n",
            "     8    3  312  136    5    4   11   14 5015   39   98   26 1122  687\n",
            "     8  401  315    2   24  139   11   22  106  505    1    2    6    9\n",
            "   310  283 5173 5383   34   17  833   29 7979    5    4   11   14   67\n",
            "   114 3073  309    5]]\n",
            "Prview of y:\n",
            "[ 114 3073  309    5    4]\n"
          ]
        }
      ],
      "source": [
        "WIN = 60\n",
        "inputs, targets = [], []\n",
        "for i in range(WIN, len(tokens)):\n",
        "    inputs.append(tokens[i-WIN:i])\n",
        "    targets.append(tokens[i])\n",
        "\n",
        "X = np.array(inputs, dtype=np.int32)\n",
        "y = np.array(targets, dtype=np.int32)\n",
        "\n",
        "print(\"X:\", X.shape, \" y:\", y.shape)\n",
        "print(f\"Prview of X:\\n{X[:5]}\")\n",
        "print(f\"Prview of y:\\n{y[:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W-Lj8l1hIq9L",
      "metadata": {
        "id": "W-Lj8l1hIq9L"
      },
      "source": [
        "### 4. Safer train/val split (avoid windowa leakage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "qd5EK0AGIqgX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qd5EK0AGIqgX",
        "outputId": "2291f18c-4e80-450c-a90a-d5afe86113a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: (3845325, 60) (3845325,)  Val: (961287, 60) (961287,)\n"
          ]
        }
      ],
      "source": [
        "VAL_FRAC = 0.2\n",
        "split = int(len(X) * (1 - VAL_FRAC))\n",
        "gap = WIN   # leave a gap of one window\n",
        "\n",
        "X_train, y_train = X[:split-gap], y[:split-gap]\n",
        "X_val,   y_val   = X[split+gap:], y[split+gap:]\n",
        "\n",
        "print(\"Train:\", X_train.shape, y_train.shape, \" Val:\", X_val.shape, y_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GW03d0DIGRI4",
      "metadata": {
        "id": "GW03d0DIGRI4"
      },
      "source": [
        "## Building RNN model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9NOEGeAgGpHl",
      "metadata": {
        "id": "9NOEGeAgGpHl"
      },
      "source": [
        "### 1. Define LSTM model with embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "GdElwUvYGovp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "GdElwUvYGovp",
        "outputId": "242f65d5-1e34-416e-af9b-76fd595cb8d2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,128</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10001</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570,257</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │     \u001b[38;5;34m1,280,128\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │       \u001b[38;5;34m394,240\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │       \u001b[38;5;34m525,312\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m525,312\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10001\u001b[0m)          │     \u001b[38;5;34m2,570,257\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,295,249</span> (20.20 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,295,249\u001b[0m (20.20 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,295,249</span> (20.20 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,295,249\u001b[0m (20.20 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential, Input\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "LSTM_UNITS = 256\n",
        "EMBEDDING_DIM = 128\n",
        "VAL_DROPOUT = 0.2\n",
        "ACTIVATION = \"softmax\"\n",
        "LOSS = \"sparse_categorical_crossentropy\"\n",
        "\n",
        "# WIN = context length of your sequences (must match your X shape)\n",
        "model = Sequential([\n",
        "    Input(shape=(WIN,)),\n",
        "    Embedding(vocab_size, EMBEDDING_DIM, mask_zero=True),\n",
        "    LSTM(LSTM_UNITS, return_sequences=True),\n",
        "    Dropout(VAL_DROPOUT),\n",
        "    LSTM(LSTM_UNITS, return_sequences=True),\n",
        "    Dropout(VAL_DROPOUT),\n",
        "    LSTM(LSTM_UNITS),\n",
        "    Dense(vocab_size, activation=ACTIVATION)\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-4, clipnorm=1.0),\n",
        "    loss=LOSS,\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0behxPMKGOrg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0behxPMKGOrg",
        "outputId": "a83baae7-e5c1-4eb1-8fa6-13a193178bfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m   21/15021\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17:48:25\u001b[0m 4s/step - accuracy: 0.0295 - loss: 9.0414"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.callbacks import ReduceLROnPlateau,EarlyStopping,ModelCheckpoint\n",
        "EPOCHS = 15\n",
        "BATCH  = 256\n",
        "MODEL_PATH = \"day75_lstm_textgen.keras\"\n",
        "\n",
        "callbacks = [\n",
        "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-5, verbose=1),\n",
        "    EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True),\n",
        "    ModelCheckpoint(MODEL_PATH, monitor=\"val_loss\", save_best_only=True)\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH,\n",
        "    validation_data=(X_val, y_val),\n",
        "    shuffle=True,\n",
        "    verbose=1,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history[\"loss\"], label=\"train\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"val\")\n",
        "plt.legend(); plt.title(\"Loss\"); plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9pCUAul8PmRL",
      "metadata": {
        "id": "9pCUAul8PmRL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf-2.10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
