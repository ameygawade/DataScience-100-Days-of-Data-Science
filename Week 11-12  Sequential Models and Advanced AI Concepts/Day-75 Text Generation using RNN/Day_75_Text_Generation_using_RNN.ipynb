{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f65e5eb6",
   "metadata": {},
   "source": [
    "# Day-75: Text Generation using RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47ad7f7",
   "metadata": {},
   "source": [
    "In the last few days, we explored RNNs, LSTMs, GRUs, and Bidirectional Networks, learning how these models understand sequential data like text or time series.\n",
    "\n",
    "Today, we’ll take it one step further — and build something creative:\n",
    "A Text Generator using RNNs!\n",
    "\n",
    "We’ll train an RNN on a children’s stories corpus, and then make it generate new story text word-by-word — just like how ChatGPT or any AI writer starts from a word and keeps predicting the next one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c321e2",
   "metadata": {},
   "source": [
    "## Topics Covered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e550d7c",
   "metadata": {},
   "source": [
    "- About the Dataset\n",
    "\n",
    "- Revisiting NLP Concepts We’ll Use\n",
    "\n",
    "- Preparing Data for RNN Text Generation\n",
    "\n",
    "- Building the RNN Model\n",
    "\n",
    "- Generating Text Word-by-Word\n",
    "\n",
    "- Evaluation & Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6066bb",
   "metadata": {},
   "source": [
    "## The Dataset: Children Stories Text Corpus (Kaggle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228693a9",
   "metadata": {},
   "source": [
    "Guys, for any project, the first step is always the data! Our dataset is a fantastic collection of children's stories. Why stories? Because they have a sequence and a context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f6df8a",
   "metadata": {},
   "source": [
    "`Analogy`: \n",
    "- Think of this dataset as a massive library of bedtime stories. \n",
    "- When a kid reads a lot of stories, they learn the structure: \"Once upon a time...\" is often followed by a character introduction. \n",
    "- Our model is going to \"read\" this library and learn the grammar, the sentence structure, and the word-to-word dependencies to tell its own story."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d383ac",
   "metadata": {},
   "source": [
    "We’ll be using the Children Stories Text Corpus:https://www.kaggle.com/datasets/edenbd/children-stories-text-corpus/data from Kaggle.\n",
    "It contains hundreds of story texts written for kids — simple grammar, repetitive sentence structures, and rich vocabulary — perfect for language generation tasks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef7b84d",
   "metadata": {},
   "source": [
    "This kind of dataset helps our RNN learn storytelling patterns like:\n",
    "\n",
    "- Sentence structure (subject → verb → object)\n",
    "\n",
    "- Repetitive story motifs (\"Once upon a time\", \"and then\", etc.)\n",
    "\n",
    "- Predictable transitions (\"The next day...\", \"Suddenly...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67128e2d",
   "metadata": {},
   "source": [
    "## Concepts from NLP we will use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4df8e8",
   "metadata": {},
   "source": [
    "We aren't starting from scratch! We'll leverage the power of foundational NLP concepts we covered previously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad2ed04",
   "metadata": {},
   "source": [
    "| **NLP Concept**                        | **Day Covered**              | **How We Use It in Text Generation**                                                        | **Analogy**                                                                                                                  |\n",
    "| :------------------------------------- | :--------------------------- | :------------------------------------------------------------------------------------------ | :--------------------------------------------------------------------------------------------------------------------------- |\n",
    "| Tokenization                           | Day 50                       | We break the stories into words or characters, creating a vocabulary.                       | Breaking a long book into individual words to check the frequency of each word.                                              |\n",
    "| Lowercasing & Cleaning                 | Day 50                       | We normalize text to ensure uniformity — “Cat” and “cat” are treated the same.              | Making sure everyone wears the same uniform before a group photo.                                                            |\n",
    "| Stopwords                              | Day 50                       | Usually, we keep stopwords here as they help maintain the flow of sentences.                | Keeping connecting words like “and” or “but” to ensure the story makes sense.                                                |\n",
    "| Word Embeddings (e.g., Word2Vec/GloVe) | Day 52                       | We convert each token into a dense vector representation to capture semantic meaning.       | Giving each word a unique ID card that also contains information about what the word means and what words are similar to it. |\n",
    "| Sequence Data                          | General RNN Concept          | RNNs are designed to handle ordered data, so word order defines meaning in text generation. | A detective trying to solve a crime — the order of events is crucial to understanding the full story.                        |\n",
    "| Padding                                | (General Preprocessing)      | Ensures all input sequences are of the same length before feeding them into RNN.            | Making all sentences the same length by adding blank spaces at the start.                                                    |\n",
    "| One-hot / Categorical Encoding         | (Used before model training) | Converts the target (next word) into categorical vectors for training.                      | Giving each possible next word its own “slot” in the prediction list.                                                        |\n",
    "| Text Generation Loop                   | (Core to RNN Workflow)       | Repeatedly predicts the next word and appends it to the input sequence.                     | Like a storyteller who keeps adding one word at a time until the story ends.                                                 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fcaa96",
   "metadata": {},
   "source": [
    "So yes — we’re reusing everything from Day 50–55!\n",
    "Only this time, we’re not classifying or clustering text — we’re generating new text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b8fbe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dba8ed19",
   "metadata": {},
   "source": [
    "## Preparing Data for RNN Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8083ad1a",
   "metadata": {},
   "source": [
    "### 1. Load the corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d57d47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amey9\\Documents\\GitHub\\DataScience-100-Days-of-Data-Science\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\amey9\\.cache\\kagglehub\\datasets\\edenbd\\children-stories-text-corpus\\versions\\1\n",
      "C:\\Users\\amey9\\.cache\\kagglehub\\datasets\\edenbd\\children-stories-text-corpus\\versions\\1\\cleaned_merged_fairy_tales_without_eos.txt\n",
      "Number of characters: 20455694\n",
      "Sample preview (first 500 chars):\n",
      "\n",
      "The Happy Prince.\n",
      "HIGH above the city, on a tall column, stood the statue of the Happy Prince.  He was gilded all over with thin leaves of fine gold, for eyes he had two bright sapphires, and a large red ruby glowed on his sword-hilt.\n",
      "He was very much admired indeed.  “He is as beautiful as a weathercock,” remarked one of the Town Councillors who wished to gain a reputation for having artistic tastes; “only not quite so useful,” he added, fearing lest people should think him unpractical, which h\n",
      "\n",
      "After Cleanup:\n",
      "\n",
      "Number of characters: 20395326\n",
      "Sample preview (first 500 chars):\n",
      "\n",
      "The Happy Prince. HIGH above the city, on a tall column, stood the statue of the Happy Prince. He was gilded all over with thin leaves of fine gold, for eyes he had two bright sapphires, and a large red ruby glowed on his sword-hilt. He was very much admired indeed. “He is as beautiful as a weathercock,” remarked one of the Town Councillors who wished to gain a reputation for having artistic tastes; “only not quite so useful,” he added, fearing lest people should think him unpractical, which he \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"edenbd/children-stories-text-corpus\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "DATA_FILE = \"cleaned_merged_fairy_tales_without_eos.txt\"\n",
    "CORPUS_PATH = f\"{path}\\{DATA_FILE}\"\n",
    "print(CORPUS_PATH)\n",
    "assert os.path.exists(CORPUS_PATH), f'Could not find {CORPUS_PATH}.'\n",
    "\n",
    "with open(CORPUS_PATH, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print('Number of characters:', len(raw_text))\n",
    "print('Sample preview (first 500 chars):\\n')\n",
    "print(f'{raw_text[:500]}\\n')\n",
    "\n",
    "# tiny cleanup\n",
    "text = \" \".join(raw_text.split())\n",
    "print('After Cleanup:\\n')\n",
    "print('Number of characters:', len(text))\n",
    "print('Sample preview (first 500 chars):\\n')\n",
    "print(text[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fdd695",
   "metadata": {},
   "source": [
    "### 2. Tokenize (word-level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a552ff19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: keras in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (3.11.3)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (6.32.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (1.17.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (1.75.1)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (2.3.3)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: rich in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from keras) (14.2.0)\n",
      "Requirement already satisfied: namex in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from keras) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from keras) (0.17.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.9)\n",
      "Requirement already satisfied: pillow in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (11.3.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from rich->keras) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from rich->keras) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\amey9\\documents\\github\\datascience-100-days-of-data-science\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9becdf2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 3853049\n",
      "Preview: [2, 296, 131, 339, 488, 2, 551, 26, 6, 863, 4831, 216, 2, 3928, 5, 2, 296, 131, 7, 9, 5093, 31, 84, 20, 1113, 678, 5, 390, 299, 19, 128, 7, 17, 96, 491, 11696, 3, 6, 294, 268, 5260, 5482, 26, 13, 827, 8127, 7, 9, 57, 101, 3107, 309, 836, 29, 18, 231, 18, 6, 10082, 16]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "## Tokenize (Day 50 concepts)\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "tokens = tokenizer.texts_to_sequences([text])[0]\n",
    "\n",
    "print('Number of tokens:', len(tokens))\n",
    "print('Preview:', tokens[:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabca493",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3) Build n-gram sequences for next-word prediction\n",
    "input_sequences = []\n",
    "for i in range(2, len(tokens)):\n",
    "    n_gram = tokens[:i]      # progressive n-grams\n",
    "    input_sequences.append(n_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777f439c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
