{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Day 12: Decision Trees for Regression and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we will dive into Decision Trees, one of the most interpretable and widely-used models in machine learning. Decision trees are flexible and powerful tools that can handle both regression and classification tasks. They provide a clear structure that resembles a flowchart, making them easy to understand and interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics Covered:\n",
    "- Introduction to Decision Trees\n",
    "- How Decision Trees Work\n",
    "- Key Concepts: Entropy, Gini Impurity, Information Gain\n",
    "- Decision Trees for Classification\n",
    "- Decision Trees for Regression\n",
    "- Advantages and Disadvantages of Decision Trees\n",
    "- Evaluation Metrics for Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Decision Tree is a model that uses a tree-like structure to make decisions based on the input data. At each node in the tree, the model asks a question, and depending on the answer, it moves to a subsequent node. This process continues until a final decision is reached (a leaf node), either for classification or regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key charactertics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nodes: \n",
    "    - Where a decision is made.\n",
    "- Edges: \n",
    "    - The result of a decision, leading to the next node.\n",
    "- Leaves: \n",
    "    - The final decision or prediction at the end of a path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees can be used for:\n",
    "\n",
    "- Classification: Predicting a category or label (e.g., whether an email is spam or not).\n",
    "- Regression: Predicting a continuous value (e.g., predicting house prices).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How Decision Trees Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree builds itself by splitting the data at each node based on a specific feature that results in the highest information gain (or the lowest Gini impurity). \n",
    "The goal is to create branches that effectively divide the data into subsets where the target variable is as homogenous as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Predicting Customer Churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you want to predict whether a customer will churn based on various factors like contract length, monthly charges, and tenure. The decision tree model will first identify the most important feature (e.g., monthly charges) to split the data. Based on the answer (e.g., whether monthly charges are above or below a threshold), the tree moves to the next node, asking another question (e.g., how long the contract is), and so on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Key Concepts: Entropy, Gini Impurity, and Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy is a measure of disorder or impurity in the dataset. The higher the entropy, the more mixed the data is.\n",
    "\n",
    "Formula for Entropy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "- $ P(x_i) $ is the probability of class $ i $.\n",
    "- Entropy is 0 if all the data belongs to one class (perfectly pure), and it increases as the data becomes more mixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Gini Impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gini Impurity is another measure of how often a randomly chosen element from the set would be incorrectly classified. Lower Gini Impurity values indicate better splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "- $ P(x_i) $ is the probability of class $ i $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information Gain tells us how much entropy is reduced after splitting the dataset on a particular feature. It helps the decision tree decide which feature to split on at each node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "- $ H(parent) $ is the entropy of the parent node.\n",
    "- $ H(child_i) $ is the entropy of the child nodes after the split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
