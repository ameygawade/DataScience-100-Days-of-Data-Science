{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 11: Logistic Regression (Binary Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we will explore Logistic Regression, one of the most popular algorithms for binary classification problems. Unlike regression models that predict continuous values, logistic regression predicts the probability of a binary outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics Covered:\n",
    "- Introduction to Binary Classification\n",
    "- Theory of Logistic Regression\n",
    "- Differences Between Logistic and Linear Regression\n",
    "- Key Concepts: Odds Ratios and Probabilities\n",
    "- Evaluating Model Performance: Accuracy, Precision, Recall, F1-Score, ROC-AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary classification is a type of classification where the target variable can take one of two possible outcomes (often referred to as \"classes\"). Logistic Regression is particularly useful for this because it predicts the probability of an outcome falling into a specific class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of Binary Classification:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Spam Detection: Classifying emails as \"spam\" or \"not spam\".\n",
    "- Customer Churn: Predicting whether a customer will leave a service (\"churn\") or stay.\n",
    "- Fraud Detection: Detecting if a transaction is \"fraudulent\" or \"legitimate\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In binary classification, the two classes are typically labeled as 0 and 1. Logistic regression estimates the probability that the target is 1, given the input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Theory Behind Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike linear regression, logistic regression does not predict continuous outcomes but rather the probability of a certain class. \n",
    "\n",
    "It uses the logistic (sigmoid) function to convert the output of the linear model into a probability between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic (Sigmoid) Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P(y=1 \\mid X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n)}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "\n",
    "- $P(y=1 \\mid X)$ is the probability of the positive class (e.g., customer churning).\n",
    "- $\\beta_0, \\beta_1, ..., \\beta_n$ are the coefficients of the model.\n",
    "- $X_1, X_2, ..., X_n$ are the features (input variables)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Odds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In logistic regression, the predicted probabilities are transformed into log-odds, which are then mapped to a probability:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\log\\left(\\frac{P(y=1)}{1 - P(y=1)}\\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes logistic regression a linear classifier at its core, but it outputs probabilities instead of continuous values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Differences Between Logistic and Linear Regressio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Logistic Regression                            | Linear Regression                               |\n",
    "|------------------------------------------------|------------------------------------------------|\n",
    "| Used for binary classification problems (two outcomes: 0 or 1). | Used for regression problems (predicting continuous values). |\n",
    "| Outputs probabilities between 0 and 1.         | Outputs continuous numeric values.              |\n",
    "| Uses the sigmoid function to map the predictions to probabilities. | Uses a straight line to fit the data points.    |\n",
    "| Predictions are non-linear in nature.          | Predictions are linear in nature.               |\n",
    "| Loss function: Log Loss (Cross-Entropy).       | Loss function: Mean Squared Error (MSE).        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Key Concepts: Odds Ratios and Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Odds Ratio: \n",
    "- This measures the odds of an event happening versus not happening. \n",
    "\n",
    "In logistic regression, we model the logarithm of the odds (also called log-odds)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odds: The ratio of the probability of an event happening to the probability of it not happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Odds = \\frac{P}{(1-P)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $ P $ is the probability of the event happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $ P $ = 0.8 (is chance of happening a event), "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then Odds will be \n",
    "\n",
    "$$ Odds = \\frac{0.8}{1-0.8} = 4 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "meaning that the Odds of happeing that event is 4 times compared to not happening "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression outputs a probability between 0 and 1, representing the likelihood of an event happening. If the probability is:\n",
    "\n",
    "    - Greater than 0.5 → Predicts class 1 (event happens).\n",
    "    - Less than 0.5 → Predicts class 0 (event does not happen)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In simpler terms:\n",
    "\n",
    "- Logistic regression transforms odds into a probability.\n",
    "- Odds tell you how much more likely something is to happen compared to it not happening.\n",
    "- The probability (output of the logistic model) is what you use to make the final prediction:\n",
    "    - If it's over 0.5, the event is predicted to occur.\n",
    "    - If it's under 0.5, the event is predicted not to occur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Customer Churn Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s build a logistic regression model to predict whether a customer will churn based on several features like their monthly charges, tenure, and support tickets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data: \n",
    "    - The dataset includes features like Monthly_Charges, Contract_Duration, and Total_Usage_GB to predict whether the customer will churn (Churn column: 1 for churn, 0 for no churn).\n",
    "- Logistic Regression Model: \n",
    "    - We use LogisticRegression to fit the data.\n",
    "- Predictions: \n",
    "    - The model predicts whether the customer will churn (y_pred).\n",
    "- Probabilities: \n",
    "    - The model also outputs the probability of each prediction (y_prob).\n",
    "- Accuracy: \n",
    "    - We evaluate the model's performance using accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted churn outcomes: [0 0]\n",
      "Actual churn outcomes:    [0 0]\n",
      "Accuracy: 1.00\n",
      "Probabilities of predictions: [[9.99999820e-01 1.80351628e-07]\n",
      " [1.00000000e+00 6.50760764e-11]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Example data for customer churn prediction\n",
    "data = {\n",
    "    'Monthly_Charges': [70, 40, 100, 60, 80, 90],\n",
    "    'Contract_Duration': [12, 24, 36, 12, 24, 36],\n",
    "    'Total_Usage_GB': [200, 100, 600, 300, 500, 700],\n",
    "    'Churn': [0, 0, 1, 0, 1, 1]  # 1: Churn, 0: No Churn\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Features (X) and target (y)\n",
    "X = df[['Monthly_Charges', 'Contract_Duration', 'Total_Usage_GB']]\n",
    "y = df['Churn']\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and fit the logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict churn on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Output predictions and model accuracy\n",
    "print(\"Predicted churn outcomes:\", y_pred)\n",
    "print(\"Actual churn outcomes:   \", y_test.values)\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "# Probability of each prediction (class probabilities)\n",
    "y_prob = model.predict_proba(X_test)\n",
    "print(\"Probabilities of predictions:\", y_prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case:\n",
    "\n",
    "- Predicted churn outcomes: The model predicts that neither customer will churn ([0 0]), meaning both are classified as non-churners (class 0).\n",
    "\n",
    "- Actual churn outcomes: The actual data confirms that both customers did not churn ([0 0]), which means the model's prediction matches the real-world outcomes.\n",
    "\n",
    "- Accuracy: The accuracy score is 1.00, meaning the model made perfect predictions on the test set (100% correct).\n",
    "\n",
    "- Probabilities of predictions: The first customer has a probability of approximately 9.99999820e-01 (very close to 1) for not churning (class 0) and 1.80351628e-07 (very close to 0) for churning (class 1). Similarly, the second customer has a probability of 1.00000000e+00 for not churning and a very small probability (6.50760764e-11) for churning. These probabilities show high confidence in predicting no churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is highly confident that neither customer will churn, and it made perfect predictions on the test data. This kind of outcome indicates that the model is likely fitting well for this small dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluating Performance of a Logistic Regression Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we discuss about performance metrics lets discuss about how our prediction can be classified in four outcome:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. True Positive (TP)\n",
    "- Definition: The model correctly predicted the positive class (e.g., predicted churn, and the customer actually churned).\n",
    "- Example: If a model predicts that a customer will churn, and they actually do churn, this is a True Positive.\n",
    "\n",
    "2. True Negative (TN)\n",
    "- Definition: The model correctly predicted the negative class (e.g., predicted no churn, and the customer did not churn).\n",
    "- Example: If a model predicts that a customer will not churn, and they indeed do not churn, this is a True Negative.\n",
    "\n",
    "3. False Positive (FP) (also called Type I Error)\n",
    "- Definition: The model predicted the positive class, but the actual outcome was negative (e.g., predicted churn, but the customer did not churn).\n",
    "- Example: If a model predicts that a customer will churn, but the customer stays, this is a False Positive.\n",
    "\n",
    "4. False Negative (FN) (also called Type II Error)\n",
    "- Definition: The model predicted the negative class, but the actual outcome was positive (e.g., predicted no churn, but the customer did churn).\n",
    "- Example: If a model predicts that a customer will not churn, but they actually churn, this is a False Negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                    | **Predicted Churn**                                     | **Predicted No Churn**                              |\n",
    "|--------------------|---------------------------------------------------------|-----------------------------------------------------|\n",
    "| **Actual Churn**    | **True Positive (TP):** Customer churned and predicted to churn | **False Negative (FN):** Customer churned but predicted not to churn |\n",
    "| **Actual No Churn** | **False Positive (FP):** Customer did not churn but predicted to churn | **True Negative (TN):** Customer did not churn and predicted not to churn |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate a binary classification model like logistic regression, we use several performance metrics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accuracy: \n",
    "    - The proportion of correct predictions out of the total predictions made. \n",
    "    $$ Accuracy = \\frac{TP+TN}{TP+TN+FP+FN} $$\n",
    "    - Example\n",
    "        - If the model made 100 predictions and 90 of them were correct, the accuracy is 90%.\n",
    "\n",
    "- Precision: \n",
    "    - Precision tells us, out of all the positive predictions made, how many were actually correct.\n",
    "    $$ Precision - \\frac{TP}{TP+FP} $$\n",
    "    - Example\n",
    "        - If the model predicted 10 customers would churn, but only 7 actually did, the precision would be 70%.\n",
    "\n",
    "- Recall(Sensitivity) : \n",
    "    - Recall tells us, out of all the actual positive cases, how many were correctly predicted by the model.\n",
    "    $$ Recall =  \\frac{TP}{TP+FN} $$\n",
    "    - Example\n",
    "        - If there were 20 customers who churned, and the model correctly identified 15 of them, the recall is 75%.\n",
    "\n",
    "- F1 Score: \n",
    "    - The F1 score is the balance between precision and recall. It’s useful when you need to balance false positives and false negatives.\n",
    "    $$ F1 score = 2.\\frac{Precision.Recall}{Precision+Recall} $$\n",
    "    - Example\n",
    "        - If precision is 80% and recall is 60%, the F1 score helps balance them, yielding a score around 69%.\n",
    "\n",
    "- ROC-AUC (Receiver Operating Characteristic - Area Under Curve)\n",
    "    - The ROC-AUC score measures how well the model distinguishes between the two classes (0 and 1). A higher value (closer to 1) indicates better performance.\n",
    "    $$$$\n",
    "    - Example\n",
    "        - A score of 0.90 means the model can distinguish between the two classes with 90% accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion of Day 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Day 11, we explored the fundamentals of Logistic Regression for Binary Classification. \n",
    "- We covered how logistic regression works to model the probability of an event occurring, and the key concepts of odds ratios and probabilities in binary classification problems. \n",
    "- Additionally, we learned about various evaluation metrics such as \n",
    "    - Accuracy, \n",
    "    - Precision, \n",
    "    - Recall, F1 Score, \n",
    "    - and ROC-AUC for assessing the performance of our logistic regression model. \n",
    "\n",
    "By the end of this day, you should be comfortable using logistic regression to solve binary classification problems and interpret its outputs effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Day 12, we will dive into Decision Trees for Regression and Classification. Decision trees are powerful, interpretable models that can handle both regression and classification tasks. We will explore how they work, their advantages, and when to use them. Furthermore, we'll introduce key concepts such as information gain, Gini impurity, and entropy. Stay tuned!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
